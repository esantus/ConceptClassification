Semi-Supervised Sequence Modeling with Cross-View Training

Kevin Clark1 Minh-Thang Luong2 Christopher D. Manning1 Quoc V. Le2

1Computer Science Department, Stanford University

2Google Brain

kevclark@cs.stanford.edu, thangluong@google.com, manning@cs.stanford.edu, qvl@google.com

Abstract

Unsupervised representation learning algo-
rithms such as word2vec and ELMo improve
the accuracy of many supervised NLP mod-
els, mainly because they can take advantage
of large amounts of unlabeled text. However,
the supervised models only learn from task-
speciﬁc labeled data during the main train-
ing phase. We therefore propose Cross-View
Training (CVT), a semi-supervised learning
algorithm that improves the representations of
a Bi-LSTM sentence encoder using a mix of
labeled and unlabeled data. On labeled exam-
ples, standard supervised learning is used. On
unlabeled examples, CVT teaches auxiliary
prediction modules that see restricted views
of the input (e.g., only part of a sentence) to
match the predictions of the full model see-
ing the whole input. Since the auxiliary mod-
ules and the full model share intermediate
representations, this in turn improves the full
model. Moreover, we show that CVT is par-
ticularly effective when combined with multi-
task learning. We evaluate CVT on ﬁve se-
quence tagging tasks, machine translation, and
dependency parsing, achieving state-of-the-art
results.1

1

Introduction

Deep learning models work best when trained on
large amounts of labeled data. However, acquir-
ing labels is costly, motivating the need for ef-
fective semi-supervised learning techniques that
leverage unlabeled examples. A widely successful
semi-supervised learning strategy for neural NLP
is pre-training word vectors (Mikolov et al., 2013).
More recent work trains a Bi-LSTM sentence en-
coder to do language modeling and then incorpo-
rates its context-sensitive representations into su-
pervised models (Dai and Le, 2015; Peters et al.,

1Code

is

available

at https://github.com/

tensorflow/models/tree/master/research/
cvt_text

2018). Such pre-training methods perform unsu-
pervised representation learning on a large corpus
of unlabeled data followed by supervised training.
A key disadvantage of pre-training is that the
ﬁrst representation learning phase does not take
advantage of labeled data – the model attempts
to learn generally effective representations rather
than ones that are targeted towards a particular
task. Older semi-supervised learning algorithms
like self-training do not suffer from this prob-
lem because they continually learn about a task
on a mix of labeled and unlabeled data. Self-
training has historically been effective for NLP
(Yarowsky, 1995; McClosky et al., 2006), but is
less commonly used with neural models. This pa-
per presents Cross-View Training (CVT), a new
self-training algorithm that works well for neural
sequence models.

In self-training, the model learns as normal on
labeled examples. On unlabeled examples, the
model acts as both a teacher that makes predictions
about the examples and a student that is trained
on those predictions. Although this process has
shown value for some tasks, it is somewhat tau-
tological: the model already produces the predic-
tions it is being trained on. Recent research on
computer vision addresses this by adding noise to
the student’s input, training the model so it is ro-
bust to input perturbations (Sajjadi et al., 2016;
Wei et al., 2018). However, applying noise is dif-
ﬁcult for discrete inputs like text.

As a solution, we take inspiration from multi-
view learning (Blum and Mitchell, 1998; Xu et al.,
2013) and train the model to produce consistent
predictions across different views of the input. In-
stead of only training the full model as a student,
CVT adds auxiliary prediction modules – neu-
ral networks that transform vector representations
into predictions – to the model and also trains them
as students. The input to each student prediction
module is a subset of the model’s intermediate rep-

8
1
0
2

 

p
e
S
2
2

 

 
 
]
L
C
.
s
c
[
 
 

1
v
0
7
3
8
0

.

9
0
8
1
:
v
i
X
r
a

resentations corresponding to a restricted view of
the input example. For example, one auxiliary pre-
diction module for sequence tagging is attached to
only the “forward” LSTM in the model’s ﬁrst Bi-
LSTM layer, so it makes predictions without see-
ing any tokens to the right of the current one.

CVT works by improving the model’s represen-
tation learning. The auxiliary prediction modules
can learn from the full model’s predictions be-
cause the full model has a better, unrestricted view
of the input. As the auxiliary modules learn to
make accurate predictions despite their restricted
views of the input, they improve the quality of the
representations they are built on top of. This in
turn improves the full model, which uses the same
shared representations. In short, our method com-
bines the idea of representation learning on unla-
beled data with classic self-training.

CVT can be applied to a variety of tasks and
neural architectures, but we focus on sequence
modeling tasks where the prediction modules are
attached to a shared Bi-LSTM encoder. We
propose auxiliary prediction modules that work
well for sequence taggers, graph-based depen-
dency parsers, and sequence-to-sequence mod-
els. We evaluate our approach on English de-
pendency parsing, combinatory categorial gram-
mar supertagging, named entity recognition, part-
of-speech tagging, and text chunking, as well as
English to Vietnamese machine translation. CVT
improves over previously published results on all
these tasks. Furthermore, CVT can easily and ef-
fectively be combined with multi-task learning:
we just add additional prediction modules for the
different tasks on top of the shared Bi-LSTM en-
coder. Training a uniﬁed model to jointly perform
all of the tasks except machine translation im-
proves results (outperforming a multi-task ELMo
model) while decreasing the total training time.

2 Cross-View Training

We ﬁrst present Cross-View Training and describe
how it can be combined effectively with multi-task
learning. See Figure 1 for an overview of the train-
ing method.

2.1 Method
Let Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} repre-
sent a labeled dataset and Dul = {x1, x2, ..., xM}
represent an unlabeled dataset We use pθ(y|xi)
to denote the output distribution over classes pro-

Figure 1: An overview of Cross-View Training. The
model is trained with standard supervised learning on
labeled examples. On unlabeled examples, auxiliary
prediction modules with different views of the input
are trained to agree with the primary prediction mod-
ule. This particular example shows CVT applied to
named entity recognition. From the labeled example,
the model can learn that “Washington” usually refers
to a location. Then, on unlabeled data, auxiliary pre-
diction modules are trained to reach the same predic-
tion without seeing some of the input. In doing so, they
improve the contextual representations produced by the
model, for example, learning that “traveled to” is usu-
ally followed by a location.

duced by the model with parameters θ on input xi.
During CVT, the model alternates learning on a
minibatch of labeled examples and learning on a
minibatch of unlabeled examples. For labeled ex-
amples, CVT uses standard cross-entropy loss:
CE(yi, pθ(y|xi))

Lsup(θ) =

(cid:88)

1
|Dl|

xi,yi∈Dl

CVT adds k auxiliary prediction modules to the
model, which are used when learning on unlabeled
examples. A prediction module is usually a small
neural network (e.g., a hidden layer followed by
a softmax layer). Each one takes as input an in-
termediate representation hj(xi) produced by the
model (e.g., the outputs of one of the LSTMs in a
Bi-LSTM model). It outputs a distribution over la-
θ(y|xi). Each hj is chosen such that it only
bels pj
uses a part of the input xi; the particular choice

Auxiliary 4:    ________________________ by planeLearning on a Labeled Example"She lives inWashington."LOCATION BiLSTMEncoderPrimaryPredictionModulepθ lossLearning on an Unlabeled Example"They traveled toWashington by plane"PrimaryAuxiliary 1Auxiliary 2Auxiliary 3Auxiliary 4Prediction ModuleslossesInputs Seen by Auxiliary Prediction Modules Auxiliary 3:    _____________ Washington by plane Auxiliary 2:    They traveled to Washington  _______BiLSTMEncoderAuxiliary 1:    They traveled to __________________pθ p1θ p2θ p3θ p4θ xi∈Dul

(cid:80)

(cid:80)k
j=1 D(pθ(y|xi), pj

can depend on the task and model architecture. We
propose variants for several tasks in Section 3. The
auxiliary prediction modules are only used during
training; the test-time prediction come from the
primary prediction module that produces pθ.
On an unlabeled example, the model ﬁrst pro-
duces soft targets pθ(y|xi) by performing infer-
ence. CVT trains the auxiliary prediction modules
to match the primary prediction module on the un-
labeled data by minimizing
θ(y|xi))
LCVT(θ) = 1|Dul|
where D is a distance function between probabil-
ity distributions (we use KL divergence). We hold
the primary module’s prediction pθ(y|xi) ﬁxed
during training (i.e., we do not back-propagate
through it) so the auxiliary modules learn to im-
itate the primary one, but not vice versa. CVT
works by enhancing the model’s representation
learning. As the auxiliary modules train, the rep-
resentations they take as input improve so they are
useful for making predictions even when some of
the model’s inputs are not available. This in turn
improves the primary prediction module, which is
built on top of the same shared representations.
We combine the supervised and CVT losses into
the total loss, L = Lsup + LCVT, and minimize it
with stochastic gradient descent. In particular, we
alternate minimizing Lsup over a minibatch of la-
beled examples and minimizing LCVT over a mini-
batch of unlabeled examples.

For most neural networks, adding a few ad-
ditional prediction modules is computationally
cheap compared to the portion of the model build-
ing up representations (such as an RNN or CNN).
Therefore our method contributes little overhead
to training time over other self-training approaches
for most tasks. CVT does not change inference
time or the number of parameters in the fully-
trained model because the auxiliary prediction
modules are only used during training.

2.2 Combining CVT with Multi-Task

Learning

CVT can easily be combined with multi-task
learning by adding additional prediction modules
for the other tasks on top of the shared Bi-LSTM
encoder. During supervised learning, we ran-
domly select a task and then update Lsup using
a minibatch of labeled data for that task. When
learning on the unlabeled data, we optimize LCVT

jointly across all tasks at once, ﬁrst running infer-
ence with all the primary prediction modules and
then learning from the predictions with all the aux-
iliary prediction modules. As before, the model
alternates training on minibatches of labeled and
unlabeled examples.

Examples labeled across many tasks are use-
ful for multi-task systems to learn from, but most
datasets are only labeled with one task. A beneﬁt
of multi-task CVT is that the model creates (ar-
tiﬁcial) all-tasks-labeled examples from unlabeled
data. This signiﬁcantly improves the model’s data
efﬁciency and training time. Since running pre-
diction modules is computationally cheap, com-
puting LCVT is not much slower for many tasks
than it is for a single one. However, we ﬁnd
the all-tasks-labeled examples substantially speed
up model convergence. For example, our model
trained on six tasks takes about three times as long
to converge as the average model trained on one
task, a 50% decrease in total training time.

3 Cross-View Training Models

CVT relies on auxiliary prediction modules that
have restricted views of the input. In this section,
we describe speciﬁc constructions of the auxiliary
prediction modules that are effective for sequence
tagging, dependency parsing, and sequence-to-
sequence learning.

i , x2

i , ..., xT

3.1 Bi-LSTM Sentence Encoder
All of our models use a two-layer CNN-BiLSTM
(Chiu and Nichols, 2016; Ma and Hovy, 2016)
sentence encoder. It takes as input a sequence of
words xi = [x1
i ]. First, each word is
represented as the sum of an embedding vector
and the output of a character-level Convolutional
Neural Network, resulting in a sequence of vectors
v = [v1, v2, ..., vT ]. The encoder applies a two-
layer bidirectional LSTM (Graves and Schmidhu-
ber, 2005) to these representations. The ﬁrst layer
runs a Long Short-Term Memory unit (Hochre-
iter and Schmidhuber, 1997) in the forward di-
rection (taking vt as input at each step t) and the
backward direction (taking vT−t+1 at each step)
−→
1 ] and
to produce vector sequences [
h T
←−
1 ]. The output of the Bi-LSTM is
h 1
−→
[
1,
1 ⊕
the concatenation of these vectors: h1 = [
h 1
←−
1 ]. The second Bi-LSTM layer
h 1
works the same, producing outputs h2, except it
takes h1 as input instead of v.

←−
h T
1 ⊕ ←−

←−
h 2
1, ...
−→
h T

−→
h 1
1,

−→
h 2

1, ...,

1, ...

h T

3.2 CVT for Sequence Tagging
In sequence tagging, each token xt
i has a corre-
sponding label yt
i. The primary prediction module
for sequence tagging produces a probability distri-
bution over classes for the tth label using a one-
hidden-layer neural network applied to the corre-
sponding encoder outputs:
p(yt|xi) = NN(ht

= softmax(U · ReLU(W (ht

1 ⊕ ht
2)) + b)
−→
The auxiliary prediction modules take
←−
h 1(xi)
and
h 1(xi), the outputs of the forward and back-
ward LSTMs in the ﬁrst2 Bi-LSTM layer, as in-
puts. We add the following four auxiliary predic-
tion modules to the model (see Figure 2):

1 ⊕ ht
2)

1(xi))

−→
(yt|xi) = NNfwd(
h t
←−
(yt|xi) = NNbwd(
h t
1(xi))
−→
h t−1
(yt|xi) = NNfuture(
←−
1
(yt|xi) = NNpast(
h t+1

1

pfwd
θ
pbwd
θ
pfuture
θ
ppast
θ

(xi))

(xi))

The “forward” module makes each prediction
without seeing the right context of the current to-
ken. The “future” module makes each predic-
tion without the right context or the current to-
ken itself. Therefore it works like a neural lan-
guage model that, instead of predicting which to-
ken comes next in the sequence, predicts which
class of token comes next. The “backward” and
“past” modules are analogous.

In particular, each word xt
i , ..., xT

3.3 CVT for Dependency Parsing
In a dependency parse, words in a sentence are
treated as nodes in a graph.
Typed directed
edges connect the words, forming a tree struc-
ture describing the syntactic structure of the sen-
i in a sentence
tence.
i receives exactly one in-going edge
xi = x1
i (called the “head”)
(u, t, r) going from word xu
to it (the “dependent”) of type r (the “relation”).
We use a graph-based dependency parser similar
to the one from Dozat and Manning (2017). This
treats dependency parsing as a classiﬁcation task
where the goal is to predict which in-going edge
i.
i = (u, t, r) connects to each word xt
yt
First, the representations produced by the en-
coder for the candidate head and dependent are

2Modules taking inputs from the second Bi-LSTM layer
would not have restricted views because information about
the whole sentence gets propagated through the ﬁrst layer.

Figure 2: Auxiliary prediction modules for sequence
tagging models. Each one sees a restricted view of the
input. For example, the “forward” prediction module
does not see any context to the right of the current token
when predicting that token’s label. For simplicity, we
only show a one layer Bi-LSTM encoder and only show
the model’s predictions for a single time step.

passed through separate hidden layers. A bilin-
ear classiﬁer applied to these representations pro-
duces a score for each candidate edge. Lastly,
these scores are passed through a softmax layer to
produce probabilities. Mathematically, the proba-
bility of an edge is given as:
pθ((u, t, r)|xi) ∝ es(hu
where s is the scoring function:
s(z1, z2, r) = ReLU(Wheadz1 + bhead)(Wr + W )

1 (xi)⊕hu

1(xi)⊕ht

2 (xi),ht

2(xi),r)

ReLU(Wdepz2 + bdep)

The bilinear classiﬁer uses a weight matrix Wr
speciﬁc to the candidate relation as well as a
weight matrix W shared across all relations. Note
that unlike in most prior work, our dependency
parser only takes words as inputs, not words and
part-of-speech tags.

We add four auxiliary prediction modules to our

model for cross-view training:
−→
((u, t, r)|xi) ∝ esfwd-fwd(
h u
1 (xi),
−→
((u, t, r)|xi) ∝ esfwd-bwd(
h u
←−
((u, t, r)|xi) ∝ esbwd-fwd(
h u
1 (xi),
←−
((u, t, r)|xi) ∝ esbwd-bwd(
h u

pfwd-fwd
θ
pfwd-bwd
θ
pbwd-fwd
θ
pbwd-bwd
θ
Each one has some missing context (not seeing ei-
ther the preceding or following words) for the can-
didate head and candidate dependent.

−→
h t
1(xi),r)
←−
h t
−→
h t
1(xi),r)
←−
h t

1(xi),r)

1(xi),r)

1 (xi),

1 (xi),

3.4 CVT for Sequence-to-Sequence Learning
We use an encoder-decoder sequence-to-sequence
model with attention (Sutskever et al., 2014; Bah-
danau et al., 2015). Each example consists of an

LSTMLSTMŷfuture  ŷfwd  ŷ   ŷbwd  ŷpast Backward LSTMForward LSTMPredictLSTMLSTMLSTMLSTMLSTMAuxiliaryPredictionModulesPrimaryPredictionModulex1x2x3EmbedBackward LSTMForward LSTMpθPredictpfwdθpfutureθpbwdθppastθAuxiliaryPredictionModulesPrimaryPredictionModuleLossi

i , ..., xT
i , ..., yK

i and out-
input (source) sequence xi = x1
put (target) sequence yi = y1
i . The en-
coder’s representations are passed into an LSTM
decoder using a bilinear attention mechanism (Lu-
ong et al., 2015).
In particular, at each time
step t the decoder computes an attention distribu-
tion over source sequence hidden states as αj ∝
ehj Wα¯ht where ¯ht is the decoder’s current hid-
den state. The source hidden states weighted by
the attention distribution form a context vector:
j αjhj. Next, the context vector and
current hidden state are combined into an atten-
tion vector at = tanh(Wa[ct, ht]). Lastly, a soft-
max layer predicts the next token in the output se-
quence: p(yt

ct = (cid:80)

, xi) = softmax(Wsat).

i|y<t

We add two auxiliary decoders when applying
CVT. The auxiliary decoders share embedding and
LSTM parameters with the primary decoder, but
have different parameters for the attention mech-
anisms and softmax layers. For the ﬁrst one, we
restrict its view of the input by applying atten-
tion dropout, randomly zeroing out a fraction of
its attention weights. The second one is trained
to predict the next word in the target sequence
rather than the current one: pfuture
, xi) =
softmax(W future
afuture
t−1 ). Since there is no target se-
quence for unlabeled examples, we cannot apply
teacher forcing to get an output distribution over
the vocabulary from the primary decoder at each
time step. Instead, we produce hard targets for the
auxiliary modules by running the primary decoder
with beam search on the input sequence. This
idea has previously been applied to sequence-level
knowledge distillation by Kim and Rush (2016)
and makes the training procedure similar to back-
translation (Sennrich et al., 2016).

i|y<t
(yt

θ

s

i

4 Experiments

We compare Cross-View Training against several
strong baselines on seven tasks:
Combinatory Categorial Grammar (CCG) Su-
pertagging: We use data from CCGBank (Hock-
enmaier and Steedman, 2007).
Text Chunking: We use the CoNLL-2000 data
(Tjong Kim Sang and Buchholz, 2000).
Named Entity Recognition (NER): We use the
CoNLL-2003 data (Tjong Kim Sang and De Meul-
der, 2003).
Fine-Grained NER (FGN): We
OntoNotes (Hovy et al., 2006) dataset.

use

the

Part-of-Speech (POS) Tagging: We use the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al., 1993).
Dependency Parsing: We use the Penn Treebank
converted to Stanford Dependencies version 3.3.0.
Machine Translation: We use the English-
Vietnamese translation dataset from IWSLT 2015
(Cettolo et al., 2015). We report (tokenized)
BLEU scores on the tst2013 test set.

We use the 1 Billion Word Language Model
Benchmark (Chelba et al., 2014) as a pool of un-
labeled sentences for semi-supervised learning.

4.1 Model Details and Baselines
We apply dropout during training, but not when
running the primary prediction module to pro-
duce soft targets on unlabeled examples.
In ad-
dition to the auxiliary prediction modules listed
in Section 3, we ﬁnd it slightly improves re-
sults to add another one that sees the whole in-
put rather than a subset (but unlike the primary
prediction module, does have dropout applied to
its representations). Unless indicated otherwise,
our models have LSTMs with 1024-sized hidden
states and 512-sized projection layers. See the ap-
pendix for full training details and hyperparame-
ters. We compare CVT with the following other
semi-supervised learning algorithms:
Word Dropout.
In this method, we only train
the primary prediction module. When acting as
a teacher it is run as normal, but when acting as
a student, we randomly replace some of the input
words with a REMOVED token. This is similar to
CVT in that it exposes the model to a restricted
view of the input. However, it is less data efﬁ-
cient. By carefully designing the auxiliary pre-
diction modules, it is possible to train the auxil-
iary prediction modules to match the primary one
across many different views of the input a once,
rather than just one view at a time.
Virtual Adversarial Training (VAT). VAT (Miy-
ato et al., 2016) works like word dropout, but
adds noise to the word embeddings of the stu-
dent instead of dropping out words. Notably, the
noise is chosen adversarially so it most changes
the model’s prediction. This method was applied
successfully to semi-supervised text classiﬁcation
by Miyato et al. (2017a).
ELMo. ELMo incorporates the representations

Method

Shortcut LSTM (Wu et al., 2017)
ID-CNN-CRF (Strubell et al., 2017)
JMT† (Hashimoto et al., 2017)
TagLM* (Peters et al., 2017)
ELMo* (Peters et al., 2018)
Biafﬁne (Dozat and Manning, 2017)
Stack Pointer (Ma et al., 2018)
Stanford (Luong and Manning, 2015)
Google (Luong et al., 2017)
Supervised
Virtual Adversarial Training*
Word Dropout*
ELMo (our implementation)*
ELMo + Multi-task*†
CVT*
CVT + Multi-task*†
CVT + Multi-task + Large*†

CCG Chunk NER FGN POS Dep. Parse Translate
Acc. F1
95.1

Acc. UAS LAS BLEU
97.53

F1

F1

90.7 86.8

95.8
96.4

91.9
92.2

97.55 94.7 92.9

95.7 94.1
95.9 94.2

23.3
26.1
91.2 87.5 97.60 95.1 93.3 28.9
91.8 87.9 97.64 95.4 93.7 –
92.1 88.1 97.66 95.6 93.8 29.3
92.2 88.5 97.72 96.2 94.4 29.3
92.3 88.4 97.79 96.4 94.8 –
92.3 88.7 97.70 95.9 94.1 29.6
92.4 88.4 97.76 96.4 94.8 –
92.6 88.8 97.74 96.6 95.0 –

94.9
95.1
95.2
95.8
95.9
95.7
96.0
96.1

95.1
95.1
95.8
96.5
96.8
96.6
96.9
97.0

Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around
0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with
them included. The +Large model has four times as many hidden units as the others, making it similar in size to
the models when ELMo is included. * denotes semi-supervised and † denotes multi-task.

from a large separately-trained language model
into a task-speciﬁc model. Our implementaiton
follows Peters et al. (2018). When combining
ELMo with multi-task learning, we allow each
task to learn its own weights for the ELMo em-
beddings going into each prediction module. We
found applying dropout to the ELMo embeddings
was crucial for achieving good performance.

4.2 Results

Results are shown in Table 1. CVT on its own out-
performs or is comparable to the best previously
published results on all tasks. Figure 3 shows an
example win for CVT over supervised learning. .
Of the prior results listed in Table 1, only
TagLM and ELMo are semi-supervised. These
methods ﬁrst train an enormous language model
on unlabeled data and incorporate the representa-
tions produced by the language model into a su-
pervised classiﬁer. Our base models use 1024 hid-
den units in their LSTMs (compared to 4096 in
ELMo), require fewer training steps (around one
pass over the billion-word benchmark rather than
many passes), and do not require a pipelined train-
ing procedure. Therefore, although they perform

Figure 3: An NER example that CVT classiﬁes cor-
rectly but supervised learning does not. “Warner” only
occurs as a last name in the train set, so the supervised
model classiﬁes “Warner Bros” as a person. The CVT
model also mistakenly classiﬁes “Warner Bros” as a
person to start with, but as it sees more of the unlabeled
data (in which “Warner” occurs thousands of times) it
learns that “Warner Bros” is an organization.

on par with ELMo, they are faster and simpler
to train.
Increasing the size of our CVT+Multi-
task model so it has 4096 units in its LSTMs like
ELMo improves results further so they are signiﬁ-
cantly better than the ELMo+Multi-task ones. We
suspect there could be further gains from combin-
ing our method with language model pre-training,
which we leave for future work.

CVT + Multi-Task. We train a single shared-

04000080000120000Training Steps0.40.60.8p("Warner Bros" = ORG)Dev Set Example: "...statement by Warner Bros."SupervisedCVTencoder CVT model to perform all of the tasks
except machine translation (as it is quite differ-
ent and requires more training time than the other
ones). Multi-task learning improves results on
all of the tasks except ﬁne-grained NER, some-
times by large margins. Prior work on many-task
NLP such as Hashimoto et al. (2017) uses compli-
cated architectures and training algorithms. Our
result shows that simple parameter sharing can be
enough for effective many-task learning when the
model is big and trained on a large amount of data.
Interestingly, multi-task learning works better
in conjunction with CVT than with ELMo. We
hypothesize that the ELMo models quickly ﬁt to
the data primarily using the ELMo vectors, which
perhaps hinders the model from learning effective
representations that transfer across tasks. We also
believe CVT alleviates the danger of the model
“forgetting” one task while training on the other
ones, a well-known problem in many-task learn-
ing (Kirkpatrick et al., 2017). During multi-task
CVT, the model makes predictions about unla-
beled examples across all tasks, creating (artiﬁ-
cial) all-tasks-labeled examples, so the model does
not only see one task at a time. In fact, multi-task
learning plus self training is similar to the Learn-
ing without Forgetting algorithm (Li and Hoiem,
2016), which trains the model to keep its predic-
tions on an old task unchanged when learning a
new task. To test the value of all-tasks-labeled ex-
amples, we trained a multi-task CVT model that
only computes LCVT on one task at a time (chosen
randomly for each unlabeled minibatch) instead of
for all tasks in parallel. The one-at-a-time model
performs substantially worse (see Table 2).

Model
CVT-MT

w/out all-labeled

CCG Chnk NER FGN POS Dep.
96.0 86.7 97.74 94.4
95.7 97.4
95.4 97.1
95.6 86.3 97.71 94.1

Table 2: Dev set performance of multi-task CVT with
and without producing all-tasks-labeled examples.

Model Generalization. In order to evaluate how
our models generalize to the dev set from the train
set, we plot the dev vs. train accuracy for our dif-
ferent methods as they learn (see Figure 4). Both
CVT and multi-task learning improve model gen-
eralization: for the same train accuracy, the mod-
els get better dev accuracy than purely supervised
learning. Interestingly, CVT continues to improve
in dev set accuracy while close to 100% train ac-

Figure 4: Dev set vs. Train set accuracy for various
methods. The “small” model has 1/4 the LSTM hidden
state size of the other ones (256 instead of 1024).

curacy for CCG, Chunking, and NER, perhaps be-
cause the model is still learning from unlabeled
data even when it has completely ﬁt to the train
set. We also show results for a smaller multi-task
+ CVT model. Although it generalizes at least as
well as the larger one, it halts making progress on
the train set earlier. This suggests it is important
to use sufﬁciently large neural networks for multi-
task learning: otherwise the model does not have
the capacity to ﬁt to all the training data.
Auxiliary Prediction Module Ablation. We
brieﬂy explore which auxiliary prediction modules
are more important for the sequence tagging tasks
in Table 3. We ﬁnd that both kinds of auxiliary
prediction modules improve performance, but that
the future and past modules improve results more
than the forward and backward ones, perhaps be-
cause they see a more restricted and challenging
view of the input.

Model
Supervised
CVT

no fwd/bwd
no future/past

CCG Chnk NER FGN POS
97.59
94.8
95.6
97.66
–0.01
–0.1
–0.3
–0.04

95.0
95.9
–0.2
–0.4

86.0
87.3
–0.1
–0.3

95.5
97.0
–0.2
–0.4

Table 3: Ablation study on auxiliary prediction mod-
ules for sequence tagging.

Training Models on Small Datasets. We ex-
plore how CVT scales with dataset size by vary-
ing the amount of training data the model has ac-
cess to. Unsurprisingly, the improvement of CVT

92949698100Train Accuracy93949596Dev AccuracyCCG9698100Train F19496Dev F1Chunking90.092.595.097.5100.0Train LAS909294Dev LASDependency Parsing92949698100Train F1929496Dev F1NERCVTSupervisedCVT + MultitaskCVT + Multitask, SmallFigure 5: Left: Dev set performance vs. percent of the training set provided to the model. Right: Dev set perfor-
mance vs. model size. The x axis shows the number of hidden units in the LSTM layers; the projection layers and
other hidden layers in the network are half that size. Points correspond to the mean of three runs.

over purely supervised learning grows larger as the
amount of labeled data decreases (see Figure 5,
left). Using only 25% of the labeled data, our ap-
proach already performs as well or better than a
fully supervised model using 100% of the training
data, demonstrating that CVT is particularly use-
ful on small datasets.
Training Larger Models. Most sequence taggers
and dependency parsers in prior work use small
LSTMs (hidden state sizes of around 300) because
larger models yield little to no gains in perfor-
mance (Reimers and Gurevych, 2017). We found
our own supervised approaches also do not ben-
eﬁt greatly from increasing the model size.
In
contrast, when using CVT accuracy scales better
with model size (see Figure 5, right). This ﬁnding
suggests the appropriate semi-supervised learning
methods may enable the development of larger,
more sophisticated models for NLP tasks with lim-
ited amounts of labeled data.
Generalizable Representations. Lastly, we ex-
plore training the CVT+multi-task model on ﬁve
tasks, freezing the encoder, and then only training
a prediction module on the sixth task. This tests
whether the encoder’s representations generalize
to a new task not seen during its training. Only
training the prediction module is very fast because
(1) the encoder (which is by far the slowest part of
the model) has to be run over each example only
once and (2) we do not back-propagate into the
encoder. Results are shown in Table 4.

Training only a prediction module on top of
multi-task representations works remarkably well,
outperforming ELMo embeddings and sometimes

Model
Supervised
CVT-MT frozen
ELMo frozen

CCG Chnk NER FGN POS Dep.
95.0 86.0 97.59 92.9
94.8 95.6
94.6 83.2 97.66 92.5
95.1 96.6
94.3 92.2
91.3 80.6 97.50 89.4

Table 4: Comparison of single-task models on the dev
sets. “CVT-MT frozen” means we pretrain a CVT +
multi-task model on ﬁve tasks, and then train only the
prediction module for the sixth. “ELMo frozen” means
we train prediction modules (but no LSTMs) on top of
ELMo embeddings.

even a vanilla supervised model, showing the
multi-task model is building up effective repre-
sentations for language. In particular, the repre-
sentations could be used like skip-thought vectors
(Kiros et al., 2015) to quickly train models on new
tasks without slow representation learning.

5 Related Work

Unsupervised Representation Learning. Early
approaches to deep semi-supervised learning pre-
train neural models on unlabeled data, which has
been successful for applications in computer vi-
sion (Jarrett et al., 2009; LeCun et al., 2010) and
NLP. Particularly noteworthy for NLP are al-
gorithms for learning effective word embeddings
(Collobert et al., 2011; Mikolov et al., 2013; Pen-
nington et al., 2014) and language model pretrain-
ing (Dai and Le, 2015; Ramachandran et al., 2017;
Peters et al., 2018; Howard and Ruder, 2018; Rad-
ford et al., 2018). Pre-training on other tasks
such as machine translation has also been stud-
ied (McCann et al., 2017). Other approaches train
“thought vectors” representing sentences through

10255075100Percent of Training Set Provided92939495AccuracyCCG10255075100Percent of Training Set Provided90929496F1Chunking10255075100Percent of Training Set Provided909294LASDependency Parsing10255075100Percent of Training Set Provided90929496F1NERCVTSupervised1282565127681024Model Size94.595.095.5AccuracyCCG1282565127681024Model Size95.095.596.096.5F1Chunking1282565127681024Model Size91929394LASDependency Parsing1282565127681024Model Size95.095.5F1NERCVTSupervisedunsupervised (Kiros et al., 2015; Hill et al., 2016)
or supervised (Conneau et al., 2017) learning.

Self-Training. One of the earliest approaches
to semi-supervised learning is self-training (Scud-
der, 1965), which has been successfully applied
to NLP tasks such as word-sense disambiguation
(Yarowsky, 1995) and parsing (McClosky et al.,
2006).
In each round of training, the classiﬁer,
acting as a “teacher,” labels some of the unlabeled
data and adds it to the training set. Then, acting as
a “student,” it is retrained on the new training set.
Many recent approaches (including the consisten-
tency regularization methods discussed below and
our own method) train the student with soft tar-
gets from the teacher’s output distribution rather
than a hard label, making the procedure more akin
to knowledge distillation (Hinton et al., 2015). It
is also possible to use multiple models or predic-
tion modules for the teacher, such as in tri-training
(Zhou and Li, 2005; Ruder and Plank, 2018).

Consistency Regularization. Recent works add
noise (e.g., drawn from a Gaussian distribution)
or apply stochastic transformations (e.g., horizon-
tally ﬂipping an image) to the student’s inputs.
This trains the model to give consistent predictions
to nearby data points, encouraging distributional
smoothness in the model. Consistency regular-
ization has been very successful for computer vi-
sion applications (Bachman et al., 2014; Laine and
Aila, 2017; Tarvainen and Valpola, 2017). How-
ever, stochastic input alterations are more difﬁcult
to apply to discrete data like text, making consis-
tency regularization less used for natural language
processing. One solution is to add noise to the
model’s word embeddings (Miyato et al., 2017a);
we compare against this approach in our experi-
ments. CVT is easily applicable to text because it
does not require changing the student’s inputs.

Multi-View Learning. Multi-view learning on
data where features can be separated into distinct
subsets has been well studied (Xu et al., 2013).
Particularly relevant are co-training (Blum and
Mitchell, 1998) and co-regularization (Sindhwani
and Belkin, 2005), which trains two models with
disjoint views of the input. On unlabeled data,
each one acts as a “teacher” for the other model.
In contrast to these methods, our approach trains
a single uniﬁed model where auxiliary prediction
modules see different, but not necessarily indepen-
dent views of the input.

Self Supervision. Self-supervised learning meth-
ods train auxiliary prediction modules on tasks
where performance can be measured without
human-provided labels. Recent work has jointly
trained image classiﬁers with tasks like relative
position and colorization (Doersch and Zisserman,
2017), sequence taggers with language modeling
(Rei, 2017), and reinforcement learning agents
with predicting changes in the environment (Jader-
berg et al., 2017). Unlike these approaches, our
auxiliary losses are based on self-labeling, not la-
bels deterministically constructed from the input.
Multi-Task Learning. There has been extensive
prior work on multi-task learning (Caruana, 1997;
Ruder, 2017). For NLP, most work has focused
on a small number of closely related tasks (Lu-
ong et al., 2016; Zhang and Weiss, 2016; Søgaard
and Goldberg, 2016; Peng et al., 2017). Many-
task systems are less commonly developed. Col-
lobert and Weston (2008) propose a many-task
system sharing word embeddings between the
tasks, Hashimoto et al. (2017) train a many-task
model where the tasks are arranged hierarchically
according to their linguistic level, and Subrama-
nian et al. (2018) train a shared-encoder many-task
model for the purpose of learning better sentence
representations for use in downstream tasks, not
for improving results on the original tasks.

6 Conclusion

We propose Cross-View Training, a new method
for semi-supervised learning. Our approach al-
lows models to effectively leverage their own pre-
dictions on unlabeled data, training them to pro-
duce effective representations that yield accurate
predictions even when some of the input is not
available. We achieve excellent results across
seven NLP tasks, especially when CVT is com-
bined with multi-task learning.

Acknowledgements

We thank Abi See, Christopher Clark, He He,
Peng Qi, Reid Pryzant, Yuaho Zhang, and the
anonymous reviewers for their thoughtful com-
ments and suggestions. We thank Takeru Miyato
for help with his virtual adversarial training code
and Emma Strubell for answering our questions
about OntoNotes NER. Kevin is supported by a
Google PhD Fellowship.

References
Philip Bachman, Ouais Alsharif, and Doina Precup.

2014. Learning with pseudo-ensembles. In NIPS.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
ACM.

Rich Caruana. 1997. Multitask learning. Machine

Learning, 28:41–75.

Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa
Bentivogli, Roldano Cattoni, and Marcello Federico.
2015. The IWSLT 2015 evaluation campaign. In In-
ternational Workshop on Spoken Language Transla-
tion.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2014. One billion word benchmark for mea-
suring progress in statistical language modeling. In
INTERSPEECH.

Jason PC Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional LSTM-CNNs. Trans-
actions of the Association for Computational Lin-
guistics.

Do Kook Choe and Eugene Charniak. 2016. Parsing as

language modeling. In EMNLP.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5):602–610.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple nlp
tasks. In EMNLP.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In HLT-NAACL.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
arXiv preprint
adaptation of feature detectors.
arXiv:1207.0580.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn treebank. Com-
putational Linguistics, 33(3):355–396.

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In HLT-NAACL.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised

sequence learning. In NIPS.

Zihang Dai, Zhilin Yang, Fan Yang, William W Co-
hen, and Ruslan Salakhutdinov. 2017. Good semi-
supervised learning that requires a bad gan. In NIPS.

Carl Doersch and Andrew Zisserman. 2017. Multi-
task self-supervised visual learning. arXiv preprint
arXiv:1708.07860.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In ICLR.

Tommaso Furlanello, Zachary C Lipton, Michael
Tschannen, Laurent Itti, and Anima Anandkumar.
2018. Born again neural networks. In ICML.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
ACL.

Max Jaderberg, Volodymyr Mnih, Wojciech Marian
Czarnecki, Tom Schaul, Joel Z Leibo, David Sil-
ver, and Koray Kavukcuoglu. 2017. Reinforcement
learning with unsupervised auxiliary tasks. In ICLR.

Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al.
2009. What is the best multi-stage architecture for
object recognition? In IEEE Conference on Com-
puter Vision.

Yoon Kim and Alexander M. Rush. 2016. Sequence-

level knowledge distillation. In EMNLP.

James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ra-
malho, Agnieszka Grabska-Barwinska, Demis Has-
sabis, Claudia Clopath, Dharshan Kumaran, and
Raia Hadsell. 2017. Overcoming catastrophic for-
getting in neural networks. Proceedings of the Na-
tional Academy of Sciences of the United States of
America, 114 13:3521–3526.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors.
In
Advances in neural information processing systems,
pages 3294–3302.

Alex Krizhnevsky and Geoffrey Hinton. 2009. Learn-

ing multiple layers of features from tiny images.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
grammars learn about syntax? In EACL.

Samuli Laine and Timo Aila. 2017. Temporal ensem-

bling for semi-supervised learning. In ICLR.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In ACL.

Yann LeCun, Koray Kavukcuoglu, and Cl´ement Fara-
bet. 2010. Convolutional networks and applications
in vision. In ISCAS. IEEE.

Mike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016.

LSTM CCG parsing. In HLT-NAACL.

Zhizhong Li and Derek Hoiem. 2016. Learning with-

out forgetting. In ECCV.

Jiangming Liu and Yue Zhang. 2017.

transition-based constituent parsing. TACL.

In-order

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan
Gui, Jian Peng, and Jiawei Han. 2017. Empower
sequence labeling with task-aware neural language
model. arXiv preprint arXiv:1709.04109.

Minh-Thang Luong, Eugene Brevdo, and Rui Zhao.
2017. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorﬂow/nmt.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In ICLR.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation systems
for spoken language domains. In IWSLT.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP.

Xuezhe Ma and Eduard Hovy. 2016.

End-to-end
sequence labeling via bi-directional LSTM-CNN-
CRF. In ACL.

Xuezhe Ma and Eduard Hovy. 2017. Neural proba-
In

bilistic model for non-projective mst parsing.
IJCNLP.

Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,
Graham Neubig, and Eduard Hovy. 2018. Stack-
pointer networks for dependency parsing. In ACL.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional linguistics, 19(2):313–330.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.

David McClosky, Eugene Charniak, and Mark John-
In

son. 2006. Effective self-training for parsing.
ACL.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In NIPS.

Takeru Miyato, Andrew M Dai, and Ian Goodfel-
low. 2017a. Adversarial training methods for semi-
supervised text classiﬁcation. In ICLR.

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama,
and Shin Ishii. 2017b. Virtual adversarial train-
ing:
supervised
arXiv preprint
and semi-supervised learning.
arXiv:1704.03976.

a regularization method for

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama,
Ken Nakae, and Shin Ishii. 2016. Distributional
smoothing with virtual adversarial
In
ICLR.

training.

Sungrae Park, Jun-Keon Park, Su-Jin Shin, and Il-
Chul Moon. 2017. Adversarial dropout for super-
vised and semi-supervised learning. arXiv preprint
arXiv:1707.03631.

Hao Peng, Sam Thomson, and Noah A. Smith. 2017.
Deep multitask learning for semantic dependency
parsing. In ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular-
izing neural networks by penalizing conﬁdent output
distributions. In ICLR.

Matthew E Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In ACL.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Boris T Polyak. 1964. Some methods of speeding up
the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics,
4(5):1–17.

Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. 2018.
Improving lan-
guage understanding by generative pre-training.
https://blog.openai.com/language-unsupervised.

Prajit Ramachandran, Peter J Liu, and Quoc V Le.
2017. Unsupervised pretraining for sequence to se-
quence learning. In EMNLP.

Marek Rei. 2017. Semi-supervised multitask learning

for sequence labeling. In ACL.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
EMNLP.

Sebastian Ruder. 2017. An overview of multi-task
learning in deep neural networks. arXiv preprint
arXiv:1706.05098.

Sebastian Ruder and Barbara Plank. 2018.

Strong
baselines for neural semi-supervised learning under
domain shift. In ACL.

Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tas-
dizen. 2016.
Regularization with stochastic
transformations and perturbations for deep semi-
supervised learning. In NIPS.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba,
Vicki Cheung, Alec Radford, and Xi Chen. 2016.
Improved techniques for training gans. In NIPS.

H Scudder. 1965. Probability of error of some adap-
IEEE Transac-

tive pattern-recognition machines.
tions on Information Theory, 11(3):363–371.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation models
with monolingual data. In ACL.

Vikas Sindhwani and Mikhail Belkin. 2005. A co-
regularization approach to semi-supervised learning
with multiple views. In ICML Workshop on Learn-
ing with Multiple Views.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In ACL.

Emma Strubell, Patrick Verga, David Belanger, and
Andrew McCallum. 2017. Fast and accurate se-
quence labeling with iterated dilated convolutions.
In EMNLP.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. In ICLR.

Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. On the importance of initial-
ization and momentum in deep learning. In ICML.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision.
In
CVPR.

Antti Tarvainen and Harri Valpola. 2017. Weight-
improve
semi-
In Workshop on

averaged
targets
supervised deep learning results.
Learning with Limited Labeled Data, NIPS.

consistency

Erik F Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In CoNLL.

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
HLT-NAACL.

Vikas Verma, Alex Lamb, Christopher Beckham,
Aaron Courville, Ioannis Mitliagkis, and Yoshua
Bengio. 2018. Manifold mixup: Encouraging mean-
ingful on-manifold interpolation as a regularizer.
arXiv preprint arXiv:1806.05236.

Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing
Improving the improved training of

Gong. 2018.
Wasserstein GANs. In ICLR.

Huijia Wu,

Jiajun Zhang, and Chengqing Zong.
2017. Shortcut sequence tagging. arXiv preprint
arXiv:1701.00576.

Chang Xu, Dacheng Tao, and Chao Xu. 2013. A
arXiv preprint

survey on multi-view learning.
arXiv:1304.5634.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL.

Hongyi Zhang, Moustapha Cisse, Yann N Dauphin,
and David Lopez-Paz. 2018. mixup: Beyond em-
pirical risk minimization. In ICLR.

Yuan Zhang and David Weiss. 2016.

Stack-
propagation: Improved representation learning for
syntax. In ACL.

Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-
ploiting unlabeled data using three classiﬁers. IEEE
Transactions on knowledge and Data Engineering.

A Detailed Results

We provide a more detailed version of the test set
results in the paper, adding two decimals of pre-
cision, standard deviations of the 5 runs for each
model, and more prior work, in Table 5.

B Model Details

Our models use two layer CNN-BiLSTM en-
coders (Chiu and Nichols, 2016; Ma and Hovy,
2016; Lample et al., 2016) and task-speciﬁc pre-
diction modules. See Section 3 of the paper for
details. We provide a few minor details not cov-
ered there below.
Sequence Tagging. For Chunking and Named En-
tity Recognition, we use a BIOES tagging scheme.
We apply label smoothing (Szegedy et al., 2016;
Pereyra et al., 2017) with a rate of 0.1 to the target
labels when training on the labeled data.
Dependency Parsing. We omit punctuation from
evaluation, which is standard practice for the PTB-
SD 3.3.0 dataset. ROOT is represented with a
ﬁxed vector hROOT instead of using a vector from
the encoder, but otherwise dependencies coming
from ROOT are scored the same way as the other
dependencies.
Machine Translation. We apply dropout to the
output of each LSTM layer in the decoder. Our
implementation is heavily based off of the Google
NMT Tutorial3 (Luong et al., 2017). We attribute
our signiﬁcantly better results to using pre-trained
word embeddings, a character-level CNN, a larger
model, stronger regularization, and better hyper-
parameter tuning. Target words occurring 5 or
fewer times in the train set are replaced with a UNK
token (but not during evaluation). We use a beam
size of 10 when performing beam search. We
found it slightly beneﬁcial to apply label smooth-
ing with a rate of 0.1 to the teacher’s predictions
(unlike our other tasks, the teacher only provides
hard targets to the students for translation).
Multi-Task Learning. Several of our datasets
are constructed from the Penn Treebank. How-
ever, we treat them as separate rather than provid-
ing examples labeled across multiple tasks to our
model during supervised training. Furthermore,
the Penn Treebank tasks do not all use the same
train/dev/test splits. We ensure the training split
of one task never overlaps the evaluation split of
another by discarding the overlapping examples
from the train sets.
Other Details. We apply dropout (Hinton et al.,
2012) to the word embeddings and outputs of each
Bi-LSTM. We use an exponential-moving-average
(EMA) of the model weights from training for the

3https://github.com/tensorflow/nmt

ﬁnal model; we found this to slightly improve ac-
curacy and signiﬁcantly reduce the variance in ac-
curacy between models trained with different ran-
dom initializations. The model is trained using
SGD with momentum (Polyak, 1964; Sutskever
et al., 2013). Word embeddings are initialized with
GloVe vectors (Pennington et al., 2014) and ﬁne-
tuned during training. The full set of model hyper-
parameters are listed in Table 6.
Baselines. Baselines were run with the same ar-
chitecture and hyperparameters as the CVT model.
For the “word dropout” model, we randomly re-
place words in the input sentence with a REMOVED
token with probability 0.1 (this value worked well
on the dev sets). For Virtual Adversarial Train-
ing, we set the norm of the perturbation to be 1.5
for CCG, 1.0 for Dependency Parsing, and 0.5 for
the other tasks (these values worked best on the
dev sets). Otherwise, the implementation is as de-
scribed in (Miyato et al., 2017a); we based our im-
plementation off of their code4. We were unable
to successfully apply VAT to machine translation,
perhaps because the student is provided hard tar-
gets for that task. For ELMo, we applied dropout
to the ELMo embeddings before they are incor-
porated into the rest of the model. When training
the multi-task ELMo model, each prediction mod-
ule has its own set of softmax-normalized weights
(stask
in (Peters et al., 2018)) for the ELMo emed-
dings going into the task-speciﬁc prediction mod-
ules. All tasks share the same sj weights for
the ELMo embeddings going into the shared Bi-
LSTM encoder.

j

C CVT for Image Recognition

Although the focus of our work is on NLP, we
also applied CVT to image recognition and found
it performs competitively with existing methods.
Most of the semi-supervised image recognition
approaches we compare against rely on the in-
puts being continuous, so they would be difﬁ-
cult to apply to text. More speciﬁcally, consis-
tency regularization methods (Sajjadi et al., 2016;
Laine and Aila, 2017; Miyato et al., 2017b) rely
on adding continuous noise and applying image-
speciﬁc transformations like cropping to inputs,
GANs (Salimans et al., 2016; Wei et al., 2018) are
very difﬁcult to train on text due to its discrete na-
ture, and mixup (Zhang et al., 2018; Verma et al.,

4https://github.com/tensorflow/models/

tree/master/research/adversarial_text

2018) requires a way of smoothly interpolating be-
tween different inputs.
Approach. Our image recognition models are
based on Convolutional Neural Networks, which
produce a set of features H(xi) ∈ Rn×n×d from
an image xi. The ﬁrst two dimensions of H in-
dex into the spatial coordinates of feature vec-
tors and d is the size of the feature vectors. For
shallower CNNs, a particular feature vector cor-
responds to a region of the input image. For ex-
ample, H0,0 would be a d-dimensional vector of
features extracted from the upper left corner. For
deeper CNNs, a particular feature vector would be
extracted from the whole image, but still only use
a “region” of the representations from an earlier
layer. The CNNs in our experiments are all in the
ﬁrst category.

The primary prediction layers of our CNNs take
as input the mean of H over the ﬁrst two dimen-
sions, which results in a d-dimensional vector that
is fed into a softmax layer:
pθ(y|xi) = softmax(W global average pool(H) + b)
We add n2 auxiliary prediction layers to the top
of the CNN. The jth layer takes a single feature
vector as input:

θ(y|xi) = softmax(W jH(cid:98)j/n(cid:99),j mod n + bj)
pj

Data. We evaluated our models on the CIFAR-
10 (Krizhnevsky and Hinton, 2009) dataset. Fol-
lowing previous work, we make the datasets semi-
supervised by only using the provided labels for a
subset of the examples in the training set; the rest
are treated as unlabeled examples.
Model. We use the convolutional neural network
from Miyato et al. (2017b), adapting their Ten-
sorFlow implementation5. Their model contains
9 convolutional layers and 2 max pooling layers.
See Appendix D of Miyato et al.’s paper for more
details.
We add 36 auxiliary softmax layers to the 6 ×
6 collection of feature vectors produced by the
CNN. Each auxiliary layer sees a patch of the im-
age ranging in size from 21 × 21 pixels (the cor-
ner) to 29 × 29 pixels (the center) of the 32 × 32
pixel images. For some experiments, we combine
CVT with standard consistency regularization by
adding a perturbation (e.g., a small random vec-
tor) to the student’s inputs when computing LCVT.

5https://github.com/takerum/vat_tf

Results. The results are shown in Table 7. Un-
surprisingly, adding continuous noise to the in-
puts works much better with images, where the in-
puts are naturally continuous, than with language.
Therefore we see much better results from VAT
on semi-supervised CIFAR-10 compared to on our
NLP tasks. However, we still ﬁnd incorporat-
ing CVT improves over models without CVT. Our
CVT + VAT models are competitive with current
start-of-the-art approaches. We found the gains
from CVT are larger when no data augmentation
is applied, perhaps because random translations of
the input expose the model to different “views” in
a similar manner as with CVT.

D Negative Results

We brieﬂy describe a few ideas we implemented
that did not seem to be effective in initial experi-
ments. Note these ﬁndings are from early one-off
experiments. We did not pursue them further after
our ﬁrst attempts did not pan out, so it is possible
that some of these approaches could be effective
with the proper adjustments and tuning.

• Hard vs soft targets: Classic self-training
algorithms train the student model with
one-hot “hard” targets corresponding to the
teacher’s highest probability prediction.
In
our experiments, this decreased performance
compared to using soft targets. This ﬁnding is
consistent with research on knowledge distil-
lation (Hinton et al., 2015; Furlanello et al.,
2018) where soft targets also work notably
better than hard targets.

• Conﬁdence thresholding:

Classic self-
training often only trains the student on a
subset of the unlabeled examples on which
the teacher has conﬁdent predictions (i.e., the
output distribution has low entropy). We
tried both “hard” (where the student
ig-
nores low-conﬁdence examples) and “soft”
(where examples are weighted according to
the teacher’s conﬁdence) versions of this for
training our models, but they did not seem to
improve performance.

• Mean Teacher: The Mean Teacher method
(Tarvainen and Valpola, 2017) tracks an ex-
ponential moving average (EMA) of model
weights, which are used to produce targets
for the students. The idea is that these tar-

gets may be better quality due to a self-
ensembling effect. However, we found this
approach to have little to no beneﬁt in our
experiments, although using EMA model
weights at
time did improve results
slightly.

test

• Purely supervised CVT: Lastly, we ex-
plored adding cross-view losses to purely su-
pervised classiﬁers. We hoped that adding
auxiliary softmax layers with different views
of the input would act as a regularizer on the
model. However, we found little to no ben-
eﬁt from this approach. This negative re-
sult suggests that the gains from CVT are
from the improved semi-supervised learning
mechanism, not the additional prediction lay-
ers regularizing the model.

Method

LSTM-CNN-CRF (Ma and Hovy, 2016)
LSTM-CNN (Chiu and Nichols, 2016)
ID-CNN-CRF (Strubell et al., 2017)
Tri-Trained LSTM (Lewis et al., 2016)
Shortcut LSTM (Wu et al., 2017)
JMT* (Hashimoto et al., 2017)
LM-LSTM-CNN-CRF (Liu et al., 2017)
TagLM† (Peters et al., 2017)
ELMo† (Peters et al., 2018)
NPM (Ma and Hovy, 2017)
Deep Biafﬁne (Dozat and Manning, 2017)
Stack Pointer (Ma et al., 2018)
Stanford (Luong and Manning, 2015)
Google (Luong et al., 2017)
Supervised
Virtual Adversarial Training*
Word Dropout*
ELMo*
ELMo + Multi-task*†
CVT*
CVT + Multi-Task*†
CVT + Multi-Task + Large*†

Dependency Parsing

UAS

LAS

Translation
BLEU

Chunking
F1

FGN
F1

NER
F1
91.21
91.62 ± 0.33 86.28 ± 0.26
90.65 ± 0.15 86.84 ± 0.19

POS
Acc.
97.55

CCG
Acc.

94.7
95.08

95.77
95.96 ± 0.08 91.71 ± 0.10
96.37 ± 0.05 91.93 ± 0.19
92.22 ± 0.10

97.53
97.55
97.53 ± 0.03

94.67

92.90

94.9
95.74
95.87

93.0
94.08
94.19

23.3
26.1

94.94 ± 0.02 95.10 ± 0.06 91.16 ± 0.09 87.48 ± 0.08 97.60 ± 0.02 95.08 ± 0.03 93.27 ± 0.03 28.88 ± 0.12
95.07 ± 0.04 95.06 ± 0.06 91.75 ± 0.10 87.91 ± 0.11 97.64 ± 0.03 95.44 ± 0.06 93.72 ± 0.07 –
95.20 ± 0.04 95.79 ± 0.08 92.14 ± 0.11 88.06 ± 0.09 97.66 ± 0.01 95.56 ± 0.05 93.80 ± 0.08 29.33 ± 0.10
95.79 ± 0.04 96.50 ± 0.03 92.24 ± 0.09 88.49 ± 0.12 97.72 ± 0.01 96.22 ± 0.05 94.44 ± 0.06 29.34 ± 0.11
95.91 ± 0.05 96.83 ± 0.03 92.32 ± 0.12 88.37 ± 0.16 97.79 ± 0.03 96.40 ± 0.04 94.79 ± 0.05 –
95.65 ± 0.04 96.58 ± 0.04 92.34 ± 0.06 88.68 ± 0.14 97.70 ± 0.03 95.86 ± 0.03 94.06 ± 0.02 29.58 ± 0.07
95.97 ± 0.04 96.85 ± 0.05 92.42 ± 0.08 88.42 ± 0.13 97.76 ± 0.02 96.44 ± 0.04 94.83 ± 0.06 –
96.05 ± 0.03 96.98 ± 0.05 92.61 ± 0.09 88.81 ± 0.09 97.74 ± 0.02 96.61 ± 0.04 95.02 ± 0.04 –

Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger
model has four times as many hidden units as the others, making it similar in size to the models when ELMo is
included. For dependency parsing, we omit results from Choe and Charniak (2016), Kuncoro et al. (2017), and Liu
and Zhang (2017) because these train constituency parsers and convert the system outputs to dependency parses.
They produce higher scores, but have access to more information during training and do not apply to datasets
without constituency annotations. * denotes semi-supervised and † denotes multi-task.

Parameter
Word Embeddings Initializiation
Character Embedding Size
Character CNN Filter Widths
Character CNN Num Filters
Encoder LSTM sizes
Encoder LSTM sizes, “Large” model
LSTM projection layer size
Hidden layer sizes
Dropout
EMA coefﬁcient
Learning rate
Momentum
Batch size

Value
300d GloVe 6B
50
[2, 3, 4]
300 (100 per ﬁlter width)
1024 for the ﬁrst layer, 512 for the second one
4096 for the ﬁrst layer, 2048 for the second one
512
512
0.5 for labeled examples, 0.8 for unlabeled examples
0.998
0.5/(1 + 0.005t0.5) (t is number of SGD updates so far)
0.9
64 sentences

Table 6: Hyperparameters for the model.

Method

GAN (Salimans et al., 2016)
Stochastic Transformations (Sajjadi et al., 2016)
Π model (Laine and Aila, 2017)
Temporal Ensemble (Laine and Aila, 2017)
Mean Teacher (Tarvainen and Valpola, 2017)
Complement GAN (Dai et al., 2017)
VAT (Miyato et al., 2017b)
VAdD (Park et al., 2017)
VAdD + VAT (Park et al., 2017)
SNGT + Π model (Luong et al., 2017)
SNGT + VAT (Luong et al., 2017)
Consistency + WGAN (Wei et al., 2018)
Manifold Mixup (Verma et al., 2018)
Supervised
VAT (ours)
CVT, no input perturbation
CVT, random input perturbation
CVT, adversarial input perturbation

CIFAR-10

CIFAR-10+

4000 labels

–
–
16.55 ± 0.29
–
–
14.41 ± 0.30
13.15
–
–
13.62 ± 0.17
12.49 ± 0.36
–
–
23.61 ± 0.60
13.29 ± 0.33
14.63 ± 0.20
13.80 ± 0.30
12.01 ± 0.11

18.63 ± 2.32
11.29 ± 0.24
12.36 ± 0.31
12.16 ± 0.24
12.31 ± 0.28
–
10.55
11.68 ± 0.19
10.07 ± 0.11
11.00 ± 0.36
9.89 ± 0.34
9.98 ± 0.21
10.26 ± 0.32
19.61 ± 0.56
10.90 ± 0.31
12.44 ± 0.27
11.10 ± 0.26
10.11 ± 0.15

Table 7: Error rates on semi-supervised CIFAR-10. We report means and standard deviations from 5 runs. CIFAR-
10+ refers to results where data augmentation (random translations of the input image) was applied. For some of
our models we add a random or adversarially chosen perturbation to the student model’s inputs, which is done in
most consistency regularization methods.

