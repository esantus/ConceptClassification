<pages>
	<page id=1>
		<box id=0 pos=105.495,757.199,492.053,775.863>Semi-Supervised Sequence Modeling with Cross-View Training 
		<box id=1 pos=95.605,716.010,504.430,731.564>Kevin Clark ~ 1 Minh-Thang Luong ~ 2 Christopher D. Manning ~ 1 Quoc V. Le ~ 2 
		<box id=2 pos=127.890,701.979,380.178,716.385>1 ~ Computer Science Department, Stanford University 
		<box id=3 pos=403.161,701.979,472.642,716.385>2 ~ Google Brain 
		<box id=4 pos=75.762,689.363,524.772,698.275>kevclark@cs.stanford.edu, thangluong@google.com, manning@cs.stanford.edu, qvl@google.com 
		<box id=5 pos=158.891,623.891,203.376,639.445>Abstract 
		<box id=6 pos=89.008,301.344,273.256,612.228>Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from taskspeciﬁc labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multitask learning. We evaluate CVT on ﬁve sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results. ~ 1 
		<box id=7 pos=72.000,274.989,77.978,290.543>1 
		<box id=8 pos=89.933,274.989,154.814,290.543>Introduction 
		<box id=9 pos=72.000,117.572,290.269,266.209>Deep learning models work best when trained on large amounts of labeled data. However, acquiring labels is costly, motivating the need for effective semi-supervised learning techniques that leverage unlabeled examples. A widely successful semi-supervised learning strategy for neural NLP is pre-training word vectors (Mikolov et al., 2013). More recent work trains a Bi-LSTM sentence encoder to do language modeling and then incorporates its context-sensitive representations into supervised models (Dai and Le, 2015; Peters et al., 
		<box id=10 pos=84.653,96.912,107.067,108.570>1 ~ Code 
		<box id=11 pos=116.213,96.912,122.193,107.717>is 
		<box id=12 pos=131.330,96.912,163.295,107.717>available 
		<box id=13 pos=172.432,96.912,290.268,107.717>at https://github.com/ 
		<box id=14 pos=72.000,77.606,281.814,96.966>tensorflow/models/tree/master/research/ cvt_text 
		<box id=15 pos=307.276,391.865,525.545,637.179>2018). Such pre-training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training. A key disadvantage of pre-training is that the ﬁrst representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task. Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data. Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models. This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models. 
		<box id=16 pos=307.276,213.893,525.545,389.628>In self-training, the model learns as normal on labeled examples. On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions. Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on. Recent research on computer vision addresses this by adding noise to the student’s input, training the model so it is robust to input perturbations (Sajjadi et al., 2016; Wei et al., 2018). However, applying noise is difﬁcult for discrete inputs like text. 
		<box id=17 pos=307.276,76.568,525.545,211.656>As a solution, we take inspiration from multiview learning (Blum and Mitchell, 1998; Xu et al., 2013) and train the model to produce consistent predictions across different views of the input. Instead of only training the full model as a student, CVT adds auxiliary prediction modules – neural networks that transform vector representations into predictions – to the model and also trains them as students. The input to each student prediction module is a subset of the model’s intermediate rep
		<box id=18 pos=14.020,538.640,36.340,578.640>8 1 0 2 
		<box id=19 pos=14.020,533.640,36.340,538.640>  
		<box id=20 pos=14.020,478.640,36.340,533.640>p e S 2 2 
		<box id=21 pos=14.020,498.640,36.340,503.640>  
		<box id=22 pos=14.020,398.100,36.340,478.640>    ] L C . s c [     
		<box id=23 pos=14.020,328.100,36.340,398.100>1 v 0 7 3 8 0 
		<box id=24 pos=14.020,323.100,36.340,328.100>. 
		<box id=25 pos=14.020,232.000,36.340,323.100>9 0 8 1 : v i X r a 
	<page id=2>
		<box id=0 pos=72.000,698.021,290.269,778.912>resentations corresponding to a restricted view of the input example. For example, one auxiliary prediction module for sequence tagging is attached to only the “forward” LSTM in the model’s ﬁrst BiLSTM layer, so it makes predictions without seeing any tokens to the right of the current one. 
		<box id=1 pos=72.000,534.796,290.269,696.982>CVT works by improving the model’s representation learning. The auxiliary prediction modules can learn from the full model’s predictions because the full model has a better, unrestricted view of the input. As the auxiliary modules learn to make accurate predictions despite their restricted views of the input, they improve the quality of the representations they are built on top of. This in turn improves the full model, which uses the same shared representations. In short, our method combines the idea of representation learning on unlabeled data with classic self-training. 
		<box id=2 pos=72.000,249.629,290.269,533.758>CVT can be applied to a variety of tasks and neural architectures, but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi-LSTM encoder. We propose auxiliary prediction modules that work well for sequence taggers, graph-based dependency parsers, and sequence-to-sequence models. We evaluate our approach on English dependency parsing, combinatory categorial grammar supertagging, named entity recognition, partof-speech tagging, and text chunking, as well as English to Vietnamese machine translation. CVT improves over previously published results on all these tasks. Furthermore, CVT can easily and effectively be combined with multi-task learning: we just add additional prediction modules for the different tasks on top of the shared Bi-LSTM encoder. Training a uniﬁed model to jointly perform all of the tasks except machine translation improves results (outperforming a multi-task ELMo model) while decreasing the total training time. 
		<box id=3 pos=72.000,223.523,195.653,239.077>2 Cross-View Training 
		<box id=4 pos=72.000,160.534,290.269,214.327>We ﬁrst present Cross-View Training and describe how it can be combined effectively with multi-task learning. See Figure 1 for an overview of the training method. 
		<box id=5 pos=72.000,76.568,290.269,150.221>2.1 Method Let D ~ l = { ~ (x ~ 1 ~ , y ~ 1 ~ ), (x ~ 2 ~ , y ~ 2 ~ ), ..., (x ~ N , y ~ N ) ~ } represent a labeled dataset and D ~ ul = { ~ x ~ 1 ~ , x ~ 2 ~ , ..., x ~ M ~ } represent an unlabeled dataset We use p ~ θ ~ (y ~ | ~ x ~ i ~ ) to denote the output distribution over classes pro
		<box id=6 pos=307.276,346.456,525.547,513.879>Figure 1: An overview of Cross-View Training. The model is trained with standard supervised learning on labeled examples. On unlabeled examples, auxiliary prediction modules with different views of the input are trained to agree with the primary prediction module. This particular example shows CVT applied to named entity recognition. From the labeled example, the model can learn that “Washington” usually refers to a location. Then, on unlabeled data, auxiliary prediction modules are trained to reach the same prediction without seeing some of the input. In doing so, they improve the contextual representations produced by the model, for example, learning that “traveled to” is usually followed by a location. 
		<box id=7 pos=307.276,231.607,525.545,327.271>duced by the model with parameters θ on input x ~ i ~ . During CVT, the model alternates learning on a minibatch of labeled examples and learning on a minibatch of unlabeled examples. For labeled examples, CVT uses standard cross-entropy loss: CE(y ~ i ~ , p ~ θ ~ (y ~ | ~ x ~ i ~ )) 
		<box id=8 pos=326.280,231.572,370.799,251.742>L ~ sup ~ (θ) = 
		<box id=9 pos=404.115,238.749,419.872,279.461>(cid:88) 
		<box id=10 pos=375.023,224.124,392.620,251.104>1 |D ~ l ~ | 
		<box id=11 pos=395.633,219.100,427.857,233.940>x ~ i ~ ,y ~ i ~ ∈D ~ l 
		<box id=12 pos=307.276,75.742,525.549,211.656>CVT adds k auxiliary prediction modules to the model, which are used when learning on unlabeled examples. A prediction module is usually a small neural network (e.g., a hidden layer followed by a softmax layer). Each one takes as input an intermediate representation h ~ j ~ (x ~ i ~ ) produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model). It outputs a distribution over laθ ~ (y ~ | ~ x ~ i ~ ) ~ . Each h ~ j is chosen such that it only bels p ~ j uses a part of the input x ~ i ~ ; the particular choice 
	<page id=3>
		<box id=0 pos=151.708,615.197,176.467,628.840>x ~ i ~ ∈D ~ ul 
		<box id=1 pos=141.121,622.026,151.708,659.454>(cid:80) 
		<box id=2 pos=179.054,616.135,264.478,659.454>(cid:80) ~ k j=1 D(p ~ θ ~ (y ~ | ~ x ~ i ~ ), p ~ j 
		<box id=3 pos=72.000,348.621,294.240,778.912>can depend on the task and model architecture. We propose variants for several tasks in Section 3. The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces p ~ θ ~ . On an unlabeled example, the model ﬁrst produces soft targets p ~ θ ~ (y ~ | ~ x ~ i ~ ) by performing inference. CVT trains the auxiliary prediction modules to match the primary prediction module on the unlabeled data by minimizing θ ~ (y ~ | ~ x ~ i ~ )) L ~ CVT ~ (θ) = 1 ~ |D ~ ul ~ | where D is a distance function between probability distributions (we use KL divergence). We hold the primary module’s prediction p ~ θ ~ (y ~ | ~ x ~ i ~ ) ﬁxed during training (i.e., we do not back-propagate through it) so the auxiliary modules learn to imitate the primary one, but not vice versa. CVT works by enhancing the model’s representation learning. As the auxiliary modules train, the representations they take as input improve so they are useful for making predictions even when some of the model’s inputs are not available. This in turn improves the primary prediction module, which is built on top of the same shared representations. We combine the supervised and CVT losses into the total loss, L = L ~ sup + L ~ CVT ~ , and minimize it with stochastic gradient descent. In particular, we alternate minimizing L ~ sup over a minibatch of labeled examples and minimizing L ~ CVT over a minibatch of unlabeled examples. 
		<box id=4 pos=72.000,212.842,290.269,347.930>For most neural networks, adding a few additional prediction modules is computationally cheap compared to the portion of the model building up representations (such as an RNN or CNN). Therefore our method contributes little overhead to training time over other self-training approaches for most tasks. CVT does not change inference time or the number of parameters in the fullytrained model because the auxiliary prediction modules are only used during training. 
		<box id=5 pos=72.000,189.536,252.229,203.729>2.2 Combining CVT with Multi-Task 
		<box id=6 pos=96.545,175.987,139.418,190.180>Learning 
		<box id=7 pos=72.000,75.524,290.269,171.008>CVT can easily be combined with multi-task learning by adding additional prediction modules for the other tasks on top of the shared Bi-LSTM encoder. During supervised learning, we randomly select a task and then update L ~ sup using a minibatch of labeled data for that task. When learning on the unlabeled data, we optimize L ~ CVT 
		<box id=8 pos=307.276,698.021,525.545,778.912>jointly across all tasks at once, ﬁrst running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules. As before, the model alternates training on minibatches of labeled and unlabeled examples. 
		<box id=9 pos=307.276,494.681,525.547,697.515>Examples labeled across many tasks are useful for multi-task systems to learn from, but most datasets are only labeled with one task. A beneﬁt of multi-task CVT is that the model creates (artiﬁcial) all-tasks-labeled examples from unlabeled data. This signiﬁcantly improves the model’s data efﬁciency and training time. Since running prediction modules is computationally cheap, computing L ~ CVT is not much slower for many tasks than it is for a single one. However, we ﬁnd the all-tasks-labeled examples substantially speed up model convergence. For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 50% decrease in total training time. 
		<box id=10 pos=307.276,470.419,471.110,485.973>3 Cross-View Training Models 
		<box id=11 pos=307.276,381.651,525.545,462.542>CVT relies on auxiliary prediction modules that have restricted views of the input. In this section, we describe speciﬁc constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning. 
		<box id=12 pos=375.891,298.341,395.942,313.325>i , x ~ 2 
		<box id=13 pos=391.707,298.341,426.392,313.325>i , ..., x ~ T 
		<box id=14 pos=307.276,75.742,525.548,373.182>3.1 Bi-LSTM Sentence Encoder All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder. It takes as input a sequence of words x ~ i = [x ~ 1 i ] ~ . First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v = [v ~ 1 ~ , v ~ 2 ~ , ..., v ~ T ] ~ . The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations. The ﬁrst layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking v ~ t as input at each step t ~ ) and the backward direction (taking v ~ T ~ − ~ t+1 at each step) −→ 1 ] and to produce vector sequences [ h T ←− 1 ] ~ . The output of the Bi-LSTM is h 1 −→ [ 1 ~ , 1 ⊕ the concatenation of these vectors: h ~ 1 = [ h 1 ←− 1 ]. The second Bi-LSTM layer h 1 works the same, producing outputs h ~ 2 ~ , except it takes h ~ 1 as input instead of v ~ . 
		<box id=15 pos=352.614,101.554,385.193,161.404>←− h T 1 ⊕ ←− 
		<box id=16 pos=330.796,103.907,360.382,161.404>←− h 2 1 ~ , ... −→ h T 
		<box id=17 pos=436.389,148.045,455.064,176.901>−→ h 1 1 ~ , 
		<box id=18 pos=456.879,150.398,472.026,176.901>−→ h 2 
		<box id=19 pos=318.185,101.554,339.892,114.816>1 ~ , ..., 
		<box id=20 pos=467.791,148.045,486.467,161.307>1 ~ , ... 
		<box id=21 pos=376.596,103.907,390.123,116.406>h T 
	<page id=4>
		<box id=0 pos=72.000,656.763,290.270,780.036>3.2 CVT for Sequence Tagging In sequence tagging, each token x ~ t i has a corresponding label y ~ t i ~ . The primary prediction module for sequence tagging produces a probability distribution over classes for the t ~ th label using a onehidden-layer neural network applied to the corresponding encoder outputs: p(y ~ t ~ | ~ x ~ i ~ ) = NN ~ (h ~ t 
		<box id=1 pos=110.946,641.052,234.677,660.219>= softmax ~ (U · ReLU ~ (W (h ~ t 
		<box id=2 pos=72.000,559.058,291.445,660.219>1 ⊕ h ~ t 2 ~ )) + b) −→ The auxiliary prediction modules take ←− h 1 ~ (x ~ i ~ ) and h 1 ~ (x ~ i ~ ) ~ , the outputs of the forward and backward LSTMs in the ﬁrst ~ 2 Bi-LSTM layer, as inputs. We add the following four auxiliary prediction modules to the model (see Figure 2): 
		<box id=3 pos=146.080,655.703,179.406,676.757>1 ⊕ h ~ t 2 ~ ) 
		<box id=4 pos=215.017,532.830,242.093,545.866>1 ~ (x ~ i ~ )) 
		<box id=5 pos=128.910,475.007,243.424,561.460>−→ (y ~ t ~ | ~ x ~ i ~ ) = NN ~ fwd ~ ( h t ←− (y ~ t ~ | ~ x ~ i ~ ) = NN ~ bwd ~ ( h t 1 ~ (x ~ i ~ )) −→ h t ~ − ~ 1 (y ~ t ~ | ~ x ~ i ~ ) = NN ~ future ~ ( ←− 1 (y ~ t ~ | ~ x ~ i ~ ) = NN ~ past ~ ( h t+1 
		<box id=6 pos=215.463,473.519,219.698,481.489>1 
		<box id=7 pos=103.891,473.121,128.413,549.459>p ~ fwd θ p ~ bwd θ p ~ future θ p ~ past θ 
		<box id=8 pos=236.033,494.635,258.377,506.610>(x ~ i ~ )) 
		<box id=9 pos=229.840,475.007,252.184,486.982>(x ~ i ~ )) 
		<box id=10 pos=72.000,344.485,290.269,466.024>The “forward” module makes each prediction without seeing the right context of the current token. The “future” module makes each prediction without the right context or the current token itself. Therefore it works like a neural language model that, instead of predicting which token comes next in the sequence, predicts which class of token comes next. The “backward” and “past” modules are analogous. 
		<box id=11 pos=102.396,234.887,226.631,263.826>In particular, each word x ~ t i , ..., x ~ T 
		<box id=12 pos=72.000,115.189,290.273,336.624>3.3 CVT for Dependency Parsing In a dependency parse, words in a sentence are treated as nodes in a graph. Typed directed edges connect the words, forming a tree structure describing the syntactic structure of the seni in a sentence tence. i receives exactly one in-going edge x ~ i = x ~ 1 i (called the “head”) (u, t, r) going from word x ~ u to it (the “dependent”) of type r (the “relation”). We use a graph-based dependency parser similar to the one from Dozat and Manning (2017). This treats dependency parsing as a classiﬁcation task where the goal is to predict which in-going edge i ~ . i = (u, t, r) connects to each word x ~ t y ~ t First, the representations produced by the encoder for the candidate head and dependent are 
		<box id=13 pos=72.000,76.987,290.269,108.570>2 ~ Modules taking inputs from the second Bi-LSTM layer would not have restricted views because information about the whole sentence gets propagated through the ﬁrst layer. 
		<box id=14 pos=307.276,561.417,525.547,645.153>Figure 2: Auxiliary prediction modules for sequence tagging models. Each one sees a restricted view of the input. For example, the “forward” prediction module does not see any context to the right of the current token when predicting that token’s label. For simplicity, we only show a one layer Bi-LSTM encoder and only show the model’s predictions for a single time step. 
		<box id=15 pos=307.276,404.536,525.839,544.713>passed through separate hidden layers. A bilinear classiﬁer applied to these representations produces a score for each candidate edge. Lastly, these scores are passed through a softmax layer to produce probabilities. Mathematically, the probability of an edge is given as: p ~ θ ~ ((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s(h ~ u where s is the scoring function: s(z ~ 1 ~ , z ~ 2 ~ , r) = ReLU ~ (W ~ head ~ z ~ 1 + b ~ head ~ )(W ~ r + W ) 
		<box id=16 pos=406.445,447.634,441.738,463.542>1 (x ~ i ~ ) ~ ⊕ ~ h ~ u 
		<box id=17 pos=463.984,447.634,496.967,463.542>1 ~ (x ~ i ~ ) ~ ⊕ ~ h ~ t 
		<box id=18 pos=437.331,447.634,466.834,458.899>2 (x ~ i ~ ),h ~ t 
		<box id=19 pos=494.117,447.634,522.485,457.692>2 ~ (x ~ i ~ ),r) 
		<box id=20 pos=372.983,387.998,470.267,402.187>ReLU ~ (W ~ dep ~ z ~ 2 + b ~ dep ~ ) 
		<box id=21 pos=307.276,301.882,525.545,382.773>The bilinear classiﬁer uses a weight matrix W ~ r speciﬁc to the candidate relation as well as a weight matrix W shared across all relations. Note that unlike in most prior work, our dependency parser only takes words as inputs, not words and part-of-speech tags. 
		<box id=22 pos=318.185,288.333,525.545,301.478>We add four auxiliary prediction modules to our 
		<box id=23 pos=307.276,188.378,476.604,287.928>model for cross-view training: −→ ((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ fwd-fwd ~ ( h u 1 (x ~ i ~ ), −→ ((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ fwd-bwd ~ ( h u ←− ((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ bwd-fwd ~ ( h u 1 (x ~ i ~ ), ←− ((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ bwd-bwd ~ ( h u 
		<box id=24 pos=307.276,142.692,525.545,267.251>p ~ fwd-fwd θ p ~ fwd-bwd θ p ~ bwd-fwd θ p ~ bwd-bwd θ Each one has some missing context (not seeing either the preceding or following words) for the candidate head and candidate dependent. 
		<box id=25 pos=474.275,194.519,513.441,277.178>−→ h t 1 ~ (x ~ i ~ ),r) ←− h t −→ h t 1 ~ (x ~ i ~ ),r) ←− h t 
		<box id=26 pos=485.073,234.634,513.441,244.692>1 ~ (x ~ i ~ ),r) 
		<box id=27 pos=487.402,192.431,515.770,202.489>1 ~ (x ~ i ~ ),r) 
		<box id=28 pos=457.161,192.431,478.933,202.489>1 (x ~ i ~ ), 
		<box id=29 pos=454.832,234.634,476.604,244.692>1 (x ~ i ~ ), 
		<box id=30 pos=307.276,76.568,525.545,135.412>3.4 CVT for Sequence-to-Sequence Learning We use an encoder-decoder sequence-to-sequence model with attention (Sutskever et al., 2014; Bahdanau et al., 2015). Each example consists of an 
	<page id=5>
		<box id=0 pos=136.715,572.266,139.598,580.236>i 
		<box id=1 pos=205.939,749.972,248.341,778.505>i , ..., x ~ T i , ..., y ~ K 
		<box id=2 pos=72.000,574.713,290.274,778.912>i and outinput (source) sequence x ~ i = x ~ 1 put (target) sequence y ~ i = y ~ 1 i . The encoder’s representations are passed into an LSTM decoder using a bilinear attention mechanism (Luong et al., 2015). In particular, at each time step t the decoder computes an attention distribution over source sequence hidden states as α ~ j ∝ e ~ h ~ j W ~ α ~ ¯h ~ t where ¯h ~ t is the decoder’s current hidden state. The source hidden states weighted by the attention distribution form a context vector: j α ~ j ~ h ~ j ~ . Next, the context vector and current hidden state are combined into an attention vector a ~ t = tanh ~ (W ~ a ~ [c ~ t ~ , h ~ t ~ ]) ~ . Lastly, a softmax layer predicts the next token in the output sequence: p(y ~ t 
		<box id=3 pos=72.000,628.083,115.393,673.614>c ~ t = (cid:80) 
		<box id=4 pos=147.249,573.886,250.901,587.858>, x ~ i ~ ) = softmax ~ (W ~ s ~ a ~ t ~ ) ~ . 
		<box id=5 pos=124.388,572.467,146.751,593.880>i ~ | ~ y ~ <t 
		<box id=6 pos=72.000,289.916,290.272,574.045>We add two auxiliary decoders when applying CVT. The auxiliary decoders share embedding and LSTM parameters with the primary decoder, but have different parameters for the attention mechanisms and softmax layers. For the ﬁrst one, we restrict its view of the input by applying attention dropout, randomly zeroing out a fraction of its attention weights. The second one is trained to predict the next word in the target sequence rather than the current one: p ~ future , x ~ i ~ ) = softmax ~ (W future a ~ future t ~ − ~ 1 ) ~ . Since there is no target sequence for unlabeled examples, we cannot apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step. Instead, we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence. This idea has previously been applied to sequence-level knowledge distillation by Kim and Rush (2016) and makes the training procedure similar to backtranslation (Sennrich et al., 2016). 
		<box id=7 pos=225.460,436.712,257.414,458.124>i ~ | ~ y ~ <t (y ~ t 
		<box id=8 pos=205.929,436.354,209.880,444.324>θ 
		<box id=9 pos=121.694,423.521,125.610,431.491>s 
		<box id=10 pos=247.377,436.511,250.260,444.481>i 
		<box id=11 pos=72.000,265.094,155.017,280.648>4 Experiments 
		<box id=12 pos=72.000,76.568,290.273,256.817>We compare Cross-View Training against several strong baselines on seven tasks: Combinatory Categorial Grammar (CCG) Supertagging: We use data from CCGBank (Hockenmaier and Steedman, 2007). Text Chunking ~ : We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000). Named Entity Recognition (NER) ~ : We use the CoNLL-2003 data (Tjong Kim Sang and De Meulder, 2003). Fine-Grained NER (FGN) ~ : We OntoNotes (Hovy et al., 2006) dataset. 
		<box id=13 pos=251.049,90.117,265.591,103.262>use 
		<box id=14 pos=276.936,90.117,290.267,103.262>the 
		<box id=15 pos=307.276,648.325,525.545,780.036>Part-of-Speech (POS) Tagging ~ : We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). Dependency Parsing: We use the Penn Treebank converted to Stanford Dependencies version 3.3.0. Machine Translation: We use the EnglishVietnamese translation dataset from IWSLT 2015 (Cettolo et al., 2015). We report (tokenized) BLEU scores on the tst2013 test set. 
		<box id=16 pos=307.276,593.857,525.545,634.100>We use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of unlabeled sentences for semi-supervised learning. 
		<box id=17 pos=307.276,76.568,525.545,584.799>4.1 Model Details and Baselines We apply dropout during training, but not when running the primary prediction module to produce soft targets on unlabeled examples. In addition to the auxiliary prediction modules listed in Section 3, we ﬁnd it slightly improves results to add another one that sees the whole input rather than a subset (but unlike the primary prediction module, does have dropout applied to its representations). Unless indicated otherwise, our models have LSTMs with 1024-sized hidden states and 512-sized projection layers. See the appendix for full training details and hyperparameters. We compare CVT with the following other semi-supervised learning algorithms: Word Dropout. In this method, we only train the primary prediction module. When acting as a teacher it is run as normal, but when acting as a student, we randomly replace some of the input words with a REMOVED token. This is similar to CVT in that it exposes the model to a restricted view of the input. However, it is less data efﬁcient. By carefully designing the auxiliary prediction modules, it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once, rather than just one view at a time. Virtual Adversarial Training (VAT). VAT (Miyato et al., 2016) works like word dropout, but adds noise to the word embeddings of the student instead of dropping out words. Notably, the noise is chosen adversarially so it most changes the model’s prediction. This method was applied successfully to semi-supervised text classiﬁcation by Miyato et al. (2017a). ELMo. ELMo incorporates the representations 
	<page id=6>
		<box id=0 pos=75.985,759.470,109.923,772.615>Method 
		<box id=1 pos=75.985,504.235,239.905,747.770>Shortcut LSTM (Wu et al., 2017) ID-CNN-CRF (Strubell et al., 2017) JMT ~ † (Hashimoto et al., 2017) TagLM* (Peters et al., 2017) ELMo* (Peters et al., 2018) Biafﬁne (Dozat and Manning, 2017) Stack Pointer (Ma et al., 2018) Stanford (Luong and Manning, 2015) Google (Luong et al., 2017) Supervised Virtual Adversarial Training* Word Dropout* ELMo (our implementation)* ELMo + Multi-task* ~ † CVT* CVT + Multi-task* ~ † CVT + Multi-task + Large* ~ † 
		<box id=2 pos=263.066,734.625,521.559,779.390>CCG Chunk NER FGN POS Dep. Parse Translate Acc. F1 95.1 
		<box id=3 pos=390.102,734.625,509.832,765.841>Acc. UAS LAS BLEU 97.53 
		<box id=4 pos=330.528,752.696,342.048,765.841>F1 
		<box id=5 pos=360.321,752.696,371.841,765.841>F1 
		<box id=6 pos=330.530,721.075,379.414,734.220>90.7 86.8 
		<box id=7 pos=293.465,693.968,312.556,720.662>95.8 96.4 
		<box id=8 pos=330.530,680.419,349.625,707.113>91.9 92.2 
		<box id=9 pos=390.109,707.517,471.467,720.662>97.55 94.7 92.9 
		<box id=10 pos=423.020,648.798,471.467,675.492>95.7 94.1 95.9 94.2 
		<box id=11 pos=330.528,504.235,500.439,643.872>23.3 26.1 91.2 87.5 97.60 95.1 93.3 28.9 91.8 87.9 97.64 95.4 93.7 – 92.1 88.1 97.66 95.6 93.8 29.3 92.2 88.5 97.72 96.2 94.4 29.3 92.3 88.4 97.79 96.4 94.8 – 92.3 88.7 97.70 95.9 94.1 29.6 92.4 88.4 97.76 96.4 94.8 – 92.6 88.8 97.74 96.6 95.0 – 
		<box id=12 pos=263.066,504.311,282.157,612.252>94.9 95.1 95.2 95.8 95.9 95.7 96.0 96.1 
		<box id=13 pos=293.470,504.311,312.561,612.252>95.1 95.1 95.8 96.5 96.8 96.6 96.9 97.0 
		<box id=14 pos=72.000,442.588,525.547,490.459>Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around 0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with them included. The +Large model has four times as many hidden units as the others, making it similar in size to the models when ELMo is included. * denotes semi-supervised and † denotes multi-task. 
		<box id=15 pos=72.000,317.334,290.269,425.323>from a large separately-trained language model into a task-speciﬁc model. Our implementaiton follows Peters et al. (2018). When combining ELMo with multi-task learning, we allow each task to learn its own weights for the ELMo embeddings going into each prediction module. We found applying dropout to the ELMo embeddings was crucial for achieving good performance. 
		<box id=16 pos=72.000,289.001,130.484,303.194>4.2 Results 
		<box id=17 pos=72.000,76.568,290.269,281.141>Results are shown in Table 1. CVT on its own outperforms or is comparable to the best previously published results on all tasks. Figure 3 shows an example win for CVT over supervised learning. . Of the prior results listed in Table 1, only TagLM and ELMo are semi-supervised. These methods ﬁrst train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classiﬁer. Our base models use 1024 hidden units in their LSTMs (compared to 4096 in ELMo), require fewer training steps (around one pass over the billion-word benchmark rather than many passes), and do not require a pipelined training procedure. Therefore, although they perform 
		<box id=18 pos=307.276,236.216,525.547,331.907>Figure 3: An NER example that CVT classiﬁes correctly but supervised learning does not. “Warner” only occurs as a last name in the train set, so the supervised model classiﬁes “Warner Bros” as a person. The CVT model also mistakenly classiﬁes “Warner Bros” as a person to start with, but as it sees more of the unlabeled data (in which “Warner” occurs thousands of times) it learns that “Warner Bros” is an organization. 
		<box id=19 pos=307.276,99.333,525.545,207.322>on par with ELMo, they are faster and simpler to train. Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are signiﬁcantly better than the ELMo+Multi-task ones. We suspect there could be further gains from combining our method with language model pre-training, which we leave for future work. 
		<box id=20 pos=307.276,76.568,525.548,90.837>CVT + Multi-Task. We train a single shared
	<page id=7>
		<box id=0 pos=72.000,305.094,290.269,778.912>encoder CVT model to perform all of the tasks except machine translation (as it is quite different and requires more training time than the other ones). Multi-task learning improves results on all of the tasks except ﬁne-grained NER, sometimes by large margins. Prior work on many-task NLP such as Hashimoto et al. (2017) uses complicated architectures and training algorithms. Our result shows that simple parameter sharing can be enough for effective many-task learning when the model is big and trained on a large amount of data. Interestingly, multi-task learning works better in conjunction with CVT than with ELMo. We hypothesize that the ELMo models quickly ﬁt to the data primarily using the ELMo vectors, which perhaps hinders the model from learning effective representations that transfer across tasks. We also believe CVT alleviates the danger of the model “forgetting” one task while training on the other ones, a well-known problem in many-task learning (Kirkpatrick et al., 2017). During multi-task CVT, the model makes predictions about unlabeled examples across all tasks, creating (artiﬁcial) all-tasks-labeled examples, so the model does not only see one task at a time. In fact, multi-task learning plus self training is similar to the Learning without Forgetting algorithm (Li and Hoiem, 2016), which trains the model to keep its predictions on an old task unchanged when learning a new task. To test the value of all-tasks-labeled examples, we trained a multi-task CVT model that only computes L ~ CVT on one task at a time (chosen randomly for each unlabeled minibatch) instead of for all tasks in parallel. The one-at-a-time model performs substantially worse (see Table 2). 
		<box id=1 pos=74.989,265.074,108.532,290.803>Model CVT-MT 
		<box id=2 pos=80.658,255.111,141.172,265.916>w/out all-labeled 
		<box id=3 pos=152.028,255.111,293.007,290.803>CCG Chnk NER FGN POS Dep. 96.0 86.7 97.74 94.4 95.7 97.4 95.4 97.1 95.6 86.3 97.71 94.1 
		<box id=4 pos=72.000,214.996,290.271,238.956>Table 2: Dev set performance of multi-task CVT with and without producing all-tasks-labeled examples. 
		<box id=5 pos=72.000,76.568,290.269,199.231>Model Generalization. In order to evaluate how our models generalize to the dev set from the train set, we plot the dev vs. train accuracy for our different methods as they learn (see Figure 4). Both CVT and multi-task learning improve model generalization: for the same train accuracy, the models get better dev accuracy than purely supervised learning. Interestingly, CVT continues to improve in dev set accuracy while close to 100% train ac
		<box id=6 pos=307.276,536.662,525.547,572.577>Figure 4: Dev set vs. Train set accuracy for various methods. The “small” model has 1/4 the LSTM hidden state size of the other ones (256 instead of 1024). 
		<box id=7 pos=307.276,256.476,525.545,518.325>curacy for CCG, Chunking, and NER, perhaps because the model is still learning from unlabeled data even when it has completely ﬁt to the train set. We also show results for a smaller multi-task + CVT model. Although it generalizes at least as well as the larger one, it halts making progress on the train set earlier. This suggests it is important to use sufﬁciently large neural networks for multitask learning: otherwise the model does not have the capacity to ﬁt to all the training data. Auxiliary Prediction Module Ablation. We brieﬂy explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table 3. We ﬁnd that both kinds of auxiliary prediction modules improve performance, but that the future and past modules improve results more than the forward and backward ones, perhaps because they see a more restricted and challenging view of the input. 
		<box id=8 pos=312.257,206.891,352.104,242.583>Model Supervised CVT 
		<box id=9 pos=317.926,186.966,367.483,207.734>no fwd/bwd no future/past 
		<box id=10 pos=393.071,186.966,526.825,242.583>CCG Chnk NER FGN POS 97.59 94.8 95.6 97.66 –0.01 –0.1 –0.3 –0.04 
		<box id=11 pos=450.860,186.966,466.553,227.659>95.0 95.9 –0.2 –0.4 
		<box id=12 pos=478.754,186.966,494.448,227.659>86.0 87.3 –0.1 –0.3 
		<box id=13 pos=421.468,186.966,437.161,227.659>95.5 97.0 –0.2 –0.4 
		<box id=14 pos=307.276,146.851,525.547,170.811>Table 3: Ablation study on auxiliary prediction modules for sequence tagging. 
		<box id=15 pos=307.276,76.568,525.547,131.485>Training Models on Small Datasets. We explore how CVT scales with dataset size by varying the amount of training data the model has access to. Unsurprisingly, the improvement of CVT 
	<page id=8>
		<box id=0 pos=72.000,572.092,525.547,608.007>Figure 5: Left: Dev set performance vs. percent of the training set provided to the model. Right: Dev set performance vs. model size. The x axis shows the number of hidden units in the LSTM layers; the projection layers and other hidden layers in the network are half that size. Points correspond to the mean of three runs. 
		<box id=1 pos=72.000,119.055,290.273,551.992>over purely supervised learning grows larger as the amount of labeled data decreases (see Figure 5, left). Using only 25% of the labeled data, our approach already performs as well or better than a fully supervised model using 100% of the training data, demonstrating that CVT is particularly useful on small datasets. Training Larger Models. Most sequence taggers and dependency parsers in prior work use small LSTMs (hidden state sizes of around 300) because larger models yield little to no gains in performance (Reimers and Gurevych, 2017). We found our own supervised approaches also do not beneﬁt greatly from increasing the model size. In contrast, when using CVT accuracy scales better with model size (see Figure 5, right). This ﬁnding suggests the appropriate semi-supervised learning methods may enable the development of larger, more sophisticated models for NLP tasks with limited amounts of labeled data. Generalizable Representations. Lastly, we explore training the CVT+multi-task model on ﬁve tasks, freezing the encoder, and then only training a prediction module on the sixth task. This tests whether the encoder’s representations generalize to a new task not seen during its training. Only training the prediction module is very fast because (1) the encoder (which is by far the slowest part of the model) has to be run over each example only once and (2) we do not back-propagate into the encoder. Results are shown in Table 4. 
		<box id=2 pos=72.000,76.568,290.269,116.812>Training only a prediction module on top of multi-task representations works remarkably well, outperforming ELMo embeddings and sometimes 
		<box id=3 pos=310.264,503.889,368.949,549.543>Model Supervised CVT-MT frozen ELMo frozen 
		<box id=4 pos=387.303,503.889,528.282,549.543>CCG Chnk NER FGN POS Dep. 95.0 86.0 97.59 92.9 94.8 95.6 94.6 83.2 97.66 92.5 95.1 96.6 94.3 92.2 91.3 80.6 97.50 89.4 
		<box id=5 pos=307.276,415.953,525.547,487.733>Table 4: Comparison of single-task models on the dev sets. “CVT-MT frozen” means we pretrain a CVT + multi-task model on ﬁve tasks, and then train only the prediction module for the sixth. “ELMo frozen” means we train prediction modules (but no LSTMs) on top of ELMo embeddings. 
		<box id=6 pos=307.276,315.493,525.545,396.384>even a vanilla supervised model, showing the multi-task model is building up effective representations for language. In particular, the representations could be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning. 
		<box id=7 pos=307.276,291.715,396.366,307.269>5 Related Work 
		<box id=8 pos=307.276,76.568,525.545,280.526>Unsupervised Representation Learning. Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train “thought vectors” representing sentences through 
	<page id=9>
		<box id=0 pos=72.000,752.217,290.269,778.912>unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning. 
		<box id=1 pos=72.000,499.903,290.269,744.508>Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In each round of training, the classiﬁer, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set. Then, acting as a “student,” it is retrained on the new training set. Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft targets from the teacher’s output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015). It is also possible to use multiple models or prediction modules for the teacher, such as in tri-training (Zhou and Li, 2005; Ruder and Plank, 2018). 
		<box id=2 pos=72.000,247.588,290.271,492.193>Consistency Regularization. Recent works add noise (e.g., drawn from a Gaussian distribution) or apply stochastic transformations (e.g., horizontally ﬂipping an image) to the student’s inputs. This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model. Consistency regularization has been very successful for computer vision applications (Bachman et al., 2014; Laine and Aila, 2017; Tarvainen and Valpola, 2017). However, stochastic input alterations are more difﬁcult to apply to discrete data like text, making consistency regularization less used for natural language processing. One solution is to add noise to the model’s word embeddings (Miyato et al., 2017a); we compare against this approach in our experiments. CVT is easily applicable to text because it does not require changing the student’s inputs. 
		<box id=3 pos=72.000,76.568,290.269,239.879>Multi-View Learning. Multi-view learning on data where features can be separated into distinct subsets has been well studied (Xu et al., 2013). Particularly relevant are co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Belkin, 2005), which trains two models with disjoint views of the input. On unlabeled data, each one acts as a “teacher” for the other model. In contrast to these methods, our approach trains a single uniﬁed model where auxiliary prediction modules see different, but not necessarily independent views of the input. 
		<box id=4 pos=307.276,394.157,525.549,780.036>Self Supervision. Self-supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human-provided labels. Recent work has jointly trained image classiﬁers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input. Multi-Task Learning. There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017). For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017). Manytask systems are less commonly developed. Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks. 
		<box id=5 pos=307.276,366.913,382.343,382.467>6 Conclusion 
		<box id=6 pos=307.276,235.363,525.545,356.901>We propose Cross-View Training, a new method for semi-supervised learning. Our approach allows models to effectively leverage their own predictions on unlabeled data, training them to produce effective representations that yield accurate predictions even when some of the input is not available. We achieve excellent results across seven NLP tasks, especially when CVT is combined with multi-task learning. 
		<box id=7 pos=307.276,208.118,406.110,223.672>Acknowledgements 
		<box id=8 pos=307.276,90.118,525.545,198.107>We thank Abi See, Christopher Clark, He He, Peng Qi, Reid Pryzant, Yuaho Zhang, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Takeru Miyato for help with his virtual adversarial training code and Emma Strubell for answering our questions about OntoNotes NER. Kevin is supported by a Google PhD Fellowship. 
	<page id=10>
		<box id=0 pos=72.000,748.605,290.271,781.178>References Philip Bachman, Ouais Alsharif, and Doina Precup. 
		<box id=1 pos=82.909,737.646,281.005,749.771>2014. Learning with pseudo-ensembles. In NIPS ~ . 
		<box id=2 pos=72.000,696.510,290.271,730.433>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR ~ . 
		<box id=3 pos=72.000,655.374,290.271,689.297>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT ~ . ACM. 
		<box id=4 pos=72.000,636.156,290.268,648.281>Rich Caruana. 1997. Multitask learning. Machine 
		<box id=5 pos=82.909,625.197,164.553,637.322>Learning ~ , 28:41–75. 
		<box id=6 pos=72.000,562.144,290.271,618.034>Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. 2015. The IWSLT 2015 evaluation campaign. In International Workshop on Spoken Language Translation ~ . 
		<box id=7 pos=72.000,499.090,290.271,554.931>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH ~ . 
		<box id=8 pos=72.000,446.995,290.271,491.877>Jason PC Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics ~ . 
		<box id=9 pos=72.000,427.777,290.271,439.782>Do Kook Choe and Eugene Charniak. 2016. Parsing as 
		<box id=10 pos=82.909,416.818,211.018,428.943>language modeling. In EMNLP ~ . 
		<box id=11 pos=307.276,733.094,525.547,778.025>Alex Graves and J¨urgen Schmidhuber. 2005. Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures. Neural Networks ~ , 18(5):602–610. 
		<box id=12 pos=307.276,677.487,525.547,722.369>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In EMNLP ~ . 
		<box id=13 pos=307.276,632.839,525.547,666.762>Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In HLT-NAACL ~ . 
		<box id=14 pos=307.276,588.192,525.547,622.114>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 ~ . 
		<box id=15 pos=307.276,521.626,525.547,577.467>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coarXiv preprint adaptation of feature detectors. arXiv:1207.0580 ~ . 
		<box id=16 pos=307.276,487.937,525.544,510.951>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Neural computation ~ , 
		<box id=17 pos=318.185,476.978,426.837,499.942>Long short-term memory. 9(8):1735–1780. 
		<box id=18 pos=307.276,421.371,525.547,466.253>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn treebank. Computational Linguistics ~ , 33(3):355–396. 
		<box id=19 pos=72.000,375.682,290.271,409.605>Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: deep neural networks with multitask learning. In ICML ~ . 
		<box id=20 pos=307.276,376.724,525.547,410.646>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In HLT-NAACL ~ . 
		<box id=21 pos=72.000,323.587,290.271,368.519>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research ~ . 
		<box id=22 pos=72.000,271.493,290.271,316.424>Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In EMNLP ~ . 
		<box id=23 pos=72.000,252.275,290.271,264.280>Andrew M Dai and Quoc V Le. 2015. Semi-supervised 
		<box id=24 pos=82.909,241.316,194.948,253.441>sequence learning. In NIPS ~ . 
		<box id=25 pos=72.000,200.180,290.271,234.103>Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. 2017. Good semisupervised learning that requires a bad gan. In NIPS ~ . 
		<box id=26 pos=72.000,159.044,290.271,192.967>Carl Doersch and Andrew Zisserman. 2017. Multitask self-supervised visual learning. arXiv preprint arXiv:1708.07860 ~ . 
		<box id=27 pos=72.000,117.908,290.271,151.831>Timothy Dozat and Christopher D. Manning. 2017. Deep biafﬁne attention for neural dependency parsing. In ICLR ~ . 
		<box id=28 pos=72.000,76.772,290.271,110.695>Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. 2018. Born again neural networks. In ICML ~ . 
		<box id=29 pos=307.276,332.076,525.547,365.999>Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In ACL ~ . 
		<box id=30 pos=307.276,276.469,525.547,321.351>Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. 2017. Reinforcement learning with unsupervised auxiliary tasks. In ICLR ~ . 
		<box id=31 pos=307.276,220.862,525.547,265.744>Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. 2009. What is the best multi-stage architecture for object recognition? In IEEE Conference on Computer Vision ~ . 
		<box id=32 pos=307.276,198.132,525.547,210.137>Yoon Kim and Alexander M. Rush. 2016. Sequence
		<box id=33 pos=318.185,187.173,480.246,199.298>level knowledge distillation. In EMNLP ~ . 
		<box id=34 pos=307.276,76.772,525.547,176.448>James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America ~ , 114 13:3521–3526. 
	<page id=11>
		<box id=0 pos=72.000,722.135,290.271,777.976>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems ~ , pages 3294–3302. 
		<box id=1 pos=72.000,702.348,290.271,714.353>Alex Krizhnevsky and Geoffrey Hinton. 2009. Learn
		<box id=2 pos=82.909,691.389,275.347,703.394>ing multiple layers of features from tiny images. 
		<box id=3 pos=72.000,638.725,290.271,683.607>Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, and Noah A. Smith. 2017. What do recurrent neural network grammars learn about syntax? In EACL ~ . 
		<box id=4 pos=72.000,618.938,290.271,630.943>Samuli Laine and Timo Aila. 2017. Temporal ensem
		<box id=5 pos=82.909,607.979,260.820,620.104>bling for semi-supervised learning. In ICLR ~ . 
		<box id=6 pos=72.000,555.315,290.271,600.197>Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In ACL ~ . 
		<box id=7 pos=72.000,513.610,290.271,547.583>Yann LeCun, Koray Kavukcuoglu, and Cl´ement Farabet. 2010. Convolutional networks and applications in vision. In ISCAS ~ . IEEE. 
		<box id=8 pos=72.000,493.823,290.271,505.828>Mike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016. 
		<box id=9 pos=82.909,482.864,234.719,494.989>LSTM CCG parsing. In HLT-NAACL ~ . 
		<box id=10 pos=72.000,463.077,290.271,475.082>Zhizhong Li and Derek Hoiem. 2016. Learning with
		<box id=11 pos=82.909,452.118,182.067,464.243>out forgetting. In ECCV ~ . 
		<box id=12 pos=72.000,432.331,240.776,444.336>Jiangming Liu and Yue Zhang. 2017. 
		<box id=13 pos=82.909,421.372,256.168,433.497>transition-based constituent parsing. TACL ~ . 
		<box id=14 pos=257.633,432.331,290.271,444.336>In-order 
		<box id=15 pos=72.000,368.708,290.271,413.589>Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan Gui, Jian Peng, and Jiawei Han. 2017. Empower sequence labeling with task-aware neural language model. arXiv preprint arXiv:1709.04109 ~ . 
		<box id=16 pos=72.000,327.003,290.271,360.925>Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. 2017. Neural machine translation (seq2seq) tutorial. https://github.com/tensorﬂow/nmt ~ . 
		<box id=17 pos=72.000,285.298,290.271,319.220>Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task sequence to sequence learning. In ICLR ~ . 
		<box id=18 pos=72.000,243.593,290.271,277.515>Minh-Thang Luong and Christopher D. Manning. 2015. Stanford neural machine translation systems for spoken language domains. In IWSLT ~ . 
		<box id=19 pos=72.000,201.888,290.271,235.810>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In EMNLP ~ . 
		<box id=20 pos=72.000,182.100,232.378,194.105>Xuezhe Ma and Eduard Hovy. 2016. 
		<box id=21 pos=82.909,160.182,290.271,194.105>End-to-end sequence labeling via bi-directional LSTM-CNNCRF. In ACL ~ . 
		<box id=22 pos=72.000,129.436,290.271,152.400>Xuezhe Ma and Eduard Hovy. 2017. Neural probaIn 
		<box id=23 pos=82.909,118.477,272.348,141.441>bilistic model for non-projective mst parsing. IJCNLP ~ . 
		<box id=24 pos=72.000,76.772,290.271,110.695>Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng, Graham Neubig, and Eduard Hovy. 2018. Stackpointer networks for dependency parsing. In ACL ~ . 
		<box id=25 pos=307.276,733.094,525.547,777.976>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The Penn treebank. Computational linguistics ~ , 19(2):313–330. 
		<box id=26 pos=307.276,690.195,525.547,724.118>Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS ~ . 
		<box id=27 pos=307.276,658.254,525.547,681.218>David McClosky, Eugene Charniak, and Mark JohnIn 
		<box id=28 pos=318.185,647.295,508.989,670.259>son. 2006. Effective self-training for parsing. ACL ~ . 
		<box id=29 pos=307.276,593.437,525.547,638.319>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS ~ . 
		<box id=30 pos=307.276,550.538,525.547,584.461>Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2017a. Adversarial training methods for semisupervised text classiﬁcation. In ICLR ~ . 
		<box id=31 pos=307.276,485.721,525.547,541.561>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2017b. Virtual adversarial training: supervised arXiv preprint and semi-supervised learning. arXiv:1704.03976 ~ . 
		<box id=32 pos=348.471,507.638,474.598,519.643>a regularization method for 
		<box id=33 pos=307.276,431.862,525.547,476.744>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. 2016. Distributional smoothing with virtual adversarial In ICLR ~ . 
		<box id=34 pos=470.095,442.821,503.579,454.826>training. 
		<box id=35 pos=307.276,378.004,525.547,422.886>Sungrae Park, Jun-Keon Park, Su-Jin Shin, and IlChul Moon. 2017. Adversarial dropout for supervised and semi-supervised learning. arXiv preprint arXiv:1707.03631 ~ . 
		<box id=36 pos=307.276,335.105,525.547,369.028>Hao Peng, Sam Thomson, and Noah A. Smith. 2017. Deep multitask learning for semantic dependency parsing. In ACL ~ . 
		<box id=37 pos=307.276,292.205,525.547,326.128>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In EMNLP ~ . 
		<box id=38 pos=307.276,271.224,451.076,283.229>Gabriel Pereyra, George Tucker, 
		<box id=39 pos=318.185,238.347,525.547,283.229>Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. 2017. Regularizing neural networks by penalizing conﬁdent output distributions. In ICLR ~ . 
		<box id=40 pos=307.276,184.489,525.547,229.371>Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL ~ . 
		<box id=41 pos=307.276,130.631,525.547,175.512>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365 ~ . 
		<box id=42 pos=307.276,76.772,525.547,121.654>Boris T Polyak. 1964. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics ~ , 4(5):1–17. 
	<page id=12>
		<box id=0 pos=72.000,733.094,290.271,777.976>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. https://blog.openai.com/language-unsupervised ~ . 
		<box id=1 pos=72.000,689.345,290.271,723.268>Prajit Ramachandran, Peter J Liu, and Quoc V Le. 2017. Unsupervised pretraining for sequence to sequence learning. In EMNLP ~ . 
		<box id=2 pos=72.000,667.514,290.271,679.519>Marek Rei. 2017. Semi-supervised multitask learning 
		<box id=3 pos=82.909,656.555,205.449,668.680>for sequence labeling. In ACL ~ . 
		<box id=4 pos=72.000,601.847,290.271,646.729>Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging. In EMNLP ~ . 
		<box id=5 pos=72.000,558.098,290.271,592.021>Sebastian Ruder. 2017. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098 ~ . 
		<box id=6 pos=72.000,536.267,252.463,548.272>Sebastian Ruder and Barbara Plank. 2018. 
		<box id=7 pos=82.909,514.349,290.271,548.272>Strong baselines for neural semi-supervised learning under domain shift. In ACL ~ . 
		<box id=8 pos=72.000,459.641,290.271,504.523>Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. 2016. Regularization with stochastic transformations and perturbations for deep semisupervised learning. In NIPS ~ . 
		<box id=9 pos=72.000,415.892,290.271,449.815>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. In NIPS ~ . 
		<box id=10 pos=72.000,383.331,290.271,406.066>H Scudder. 1965. Probability of error of some adapIEEE Transac
		<box id=11 pos=82.909,372.143,262.236,395.107>tive pattern-recognition machines. tions on Information Theory ~ , 11(3):363–371. 
		<box id=12 pos=72.000,328.394,290.271,362.317>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In ACL ~ . 
		<box id=13 pos=72.000,273.686,290.271,318.568>Vikas Sindhwani and Mikhail Belkin. 2005. A coregularization approach to semi-supervised learning with multiple views. In ICML Workshop on Learning with Multiple Views ~ . 
		<box id=14 pos=72.000,229.937,290.271,263.860>Anders Søgaard and Yoav Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In ACL ~ . 
		<box id=15 pos=72.000,175.229,290.271,220.111>Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. 2017. Fast and accurate sequence labeling with iterated dilated convolutions. In EMNLP ~ . 
		<box id=16 pos=72.000,120.521,290.271,165.403>Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. 2018. Learning general purpose distributed sentence representations via large scale multi-task learning. In ICLR ~ . 
		<box id=17 pos=72.000,76.772,290.271,110.695>Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In ICML ~ . 
		<box id=18 pos=307.276,744.053,525.547,777.976>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS ~ . 
		<box id=19 pos=307.276,688.642,525.547,733.524>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In CVPR ~ . 
		<box id=20 pos=307.276,644.190,525.547,678.113>Antti Tarvainen and Harri Valpola. 2017. Weightimprove semiIn Workshop on 
		<box id=21 pos=318.185,633.231,492.818,667.154>averaged targets supervised deep learning results. Learning with Limited Labeled Data, NIPS ~ . 
		<box id=22 pos=364.790,655.149,411.126,667.154>consistency 
		<box id=23 pos=307.276,588.779,525.547,622.702>Erik F Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In CoNLL ~ . 
		<box id=24 pos=307.276,533.369,525.547,578.250>Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In HLT-NAACL ~ . 
		<box id=25 pos=307.276,466.999,525.547,522.839>Vikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkis, and Yoshua Bengio. 2018. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. arXiv preprint arXiv:1806.05236 ~ . 
		<box id=26 pos=307.276,433.506,525.547,456.470>Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing Improving the improved training of 
		<box id=27 pos=318.185,422.547,434.418,445.511>Gong. 2018. Wasserstein GANs. In ICLR ~ . 
		<box id=28 pos=307.276,400.013,355.465,412.018>Huijia Wu, 
		<box id=29 pos=318.185,378.095,525.547,412.018>Jiajun Zhang, and Chengqing Zong. 2017. Shortcut sequence tagging. arXiv preprint arXiv:1701.00576 ~ . 
		<box id=30 pos=307.276,344.831,525.547,367.566>Chang Xu, Dacheng Tao, and Chao Xu. 2013. A arXiv preprint 
		<box id=31 pos=318.185,333.643,450.717,356.607>survey on multi-view learning. arXiv:1304.5634 ~ . 
		<box id=32 pos=307.276,300.150,525.547,323.114>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL ~ . 
		<box id=33 pos=307.276,255.698,525.547,289.621>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In ICLR ~ . 
		<box id=34 pos=307.276,233.164,479.390,245.169>Yuan Zhang and David Weiss. 2016. 
		<box id=35 pos=318.185,211.246,525.547,245.169>Stackpropagation: Improved representation learning for syntax. In ACL ~ . 
		<box id=36 pos=307.276,166.794,525.547,200.717>Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classiﬁers. IEEE Transactions on knowledge and Data Engineering ~ . 
		<box id=37 pos=307.276,139.602,410.545,155.156>A Detailed Results 
		<box id=38 pos=307.276,76.568,525.545,130.361>We provide a more detailed version of the test set results in the paper, adding two decimals of precision, standard deviations of the 5 runs for each model, and more prior work, in Table 5. 
	<page id=13>
		<box id=0 pos=72.000,765.624,162.656,781.178>B Model Details 
		<box id=1 pos=72.000,96.380,290.272,757.629>Our models use two layer CNN-BiLSTM encoders (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lample et al., 2016) and task-speciﬁc prediction modules. See Section 3 of the paper for details. We provide a few minor details not covered there below. Sequence Tagging. For Chunking and Named Entity Recognition, we use a BIOES tagging scheme. We apply label smoothing (Szegedy et al., 2016; Pereyra et al., 2017) with a rate of 0.1 to the target labels when training on the labeled data. Dependency Parsing. We omit punctuation from evaluation, which is standard practice for the PTBSD 3.3.0 dataset. ROOT is represented with a ﬁxed vector h ~ ROOT instead of using a vector from the encoder, but otherwise dependencies coming from ROOT are scored the same way as the other dependencies. Machine Translation. We apply dropout to the output of each LSTM layer in the decoder. Our implementation is heavily based off of the Google NMT Tutorial ~ 3 (Luong et al., 2017). We attribute our signiﬁcantly better results to using pre-trained word embeddings, a character-level CNN, a larger model, stronger regularization, and better hyperparameter tuning. Target words occurring 5 or fewer times in the train set are replaced with a UNK token (but not during evaluation). We use a beam size of 10 when performing beam search. We found it slightly beneﬁcial to apply label smoothing with a rate of 0.1 to the teacher’s predictions (unlike our other tasks, the teacher only provides hard targets to the students for translation). Multi-Task Learning. Several of our datasets are constructed from the Penn Treebank. However, we treat them as separate rather than providing examples labeled across multiple tasks to our model during supervised training. Furthermore, the Penn Treebank tasks do not all use the same train/dev/test splits. We ensure the training split of one task never overlaps the evaluation split of another by discarding the overlapping examples from the train sets. Other Details. We apply dropout (Hinton et al., 2012) to the word embeddings and outputs of each Bi-LSTM. We use an exponential-moving-average (EMA) of the model weights from training for the 
		<box id=2 pos=84.653,77.606,265.674,88.645>3 ~ https://github.com/tensorflow/nmt 
		<box id=3 pos=307.276,327.300,525.549,778.912>ﬁnal model; we found this to slightly improve accuracy and signiﬁcantly reduce the variance in accuracy between models trained with different random initializations. The model is trained using SGD with momentum (Polyak, 1964; Sutskever et al., 2013). Word embeddings are initialized with GloVe vectors (Pennington et al., 2014) and ﬁnetuned during training. The full set of model hyperparameters are listed in Table 6. Baselines. Baselines were run with the same architecture and hyperparameters as the CVT model. For the “word dropout” model, we randomly replace words in the input sentence with a REMOVED token with probability 0.1 (this value worked well on the dev sets). For Virtual Adversarial Training, we set the norm of the perturbation to be 1.5 for CCG, 1.0 for Dependency Parsing, and 0.5 for the other tasks (these values worked best on the dev sets). Otherwise, the implementation is as described in (Miyato et al., 2017a); we based our implementation off of their code ~ 4 ~ . We were unable to successfully apply VAT to machine translation, perhaps because the student is provided hard targets for that task. For ELMo, we applied dropout to the ELMo embeddings before they are incorporated into the rest of the model. When training the multi-task ELMo model, each prediction module has its own set of softmax-normalized weights ( ~ s ~ task in (Peters et al., 2018)) for the ELMo emeddings going into the task-speciﬁc prediction modules. All tasks share the same s ~ j weights for the ELMo embeddings going into the shared BiLSTM encoder. 
		<box id=4 pos=316.022,379.251,319.447,387.221>j 
		<box id=5 pos=307.276,303.134,470.010,318.688>C CVT for Image Recognition 
		<box id=6 pos=307.276,106.041,525.545,295.326>Although the focus of our work is on NLP, we also applied CVT to image recognition and found it performs competitively with existing methods. Most of the semi-supervised image recognition approaches we compare against rely on the inputs being continuous, so they would be difﬁcult to apply to text. More speciﬁcally, consistency regularization methods (Sajjadi et al., 2016; Laine and Aila, 2017; Miyato et al., 2017b) rely on adding continuous noise and applying imagespeciﬁc transformations like cropping to inputs, GANs (Salimans et al., 2016; Wei et al., 2018) are very difﬁcult to train on text due to its discrete nature, and mixup (Zhang et al., 2018; Verma et al., 
		<box id=7 pos=319.928,87.569,522.469,98.608>4 ~ https://github.com/tensorflow/models/ 
		<box id=8 pos=307.276,77.606,506.330,87.003>tree/master/research/adversarial_text 
	<page id=14>
		<box id=0 pos=72.000,544.161,290.273,778.912>2018) requires a way of smoothly interpolating between different inputs. Approach. Our image recognition models are based on Convolutional Neural Networks, which produce a set of features H(x ~ i ~ ) ∈ R ~ n ~ × ~ n ~ × ~ d from an image x ~ i ~ . The ﬁrst two dimensions of H index into the spatial coordinates of feature vectors and d is the size of the feature vectors. For shallower CNNs, a particular feature vector corresponds to a region of the input image. For example, H ~ 0,0 would be a d ~ -dimensional vector of features extracted from the upper left corner. For deeper CNNs, a particular feature vector would be extracted from the whole image, but still only use a “region” of the representations from an earlier layer. The CNNs in our experiments are all in the ﬁrst category. 
		<box id=1 pos=72.000,419.190,291.977,543.756>The primary prediction layers of our CNNs take as input the mean of H over the ﬁrst two dimensions, which results in a d ~ -dimensional vector that is fed into a softmax layer: p ~ θ ~ (y ~ | ~ x ~ i ~ ) = softmax ~ (W global average pool ~ (H) + b) We add n ~ 2 auxiliary prediction layers to the top of the CNN. The j ~ th layer takes a single feature vector as input: 
		<box id=2 pos=84.283,394.639,277.985,416.519>θ ~ (y ~ | ~ x ~ i ~ ) = softmax ~ (W j ~ H ~ (cid:98) ~ j/n ~ (cid:99) ~ ,j mod n + b ~ j ~ ) p ~ j 
		<box id=3 pos=72.000,93.848,290.269,384.964>Data. We evaluated our models on the CIFAR10 (Krizhnevsky and Hinton, 2009) dataset. Following previous work, we make the datasets semisupervised by only using the provided labels for a subset of the examples in the training set; the rest are treated as unlabeled examples. Model. We use the convolutional neural network from Miyato et al. (2017b), adapting their TensorFlow implementation ~ 5 ~ . Their model contains 9 convolutional layers and 2 max pooling layers. See Appendix D of Miyato et al.’s paper for more details. We add 36 auxiliary softmax layers to the 6 × 6 collection of feature vectors produced by the CNN. Each auxiliary layer sees a patch of the image ranging in size from 21 × 21 pixels (the corner) to 29 × 29 pixels (the center) of the 32 × 32 pixel images. For some experiments, we combine CVT with standard consistency regularization by adding a perturbation (e.g., a small random vector) to the student’s inputs when computing L ~ CVT ~ . 
		<box id=4 pos=84.653,77.606,265.674,88.645>5 ~ https://github.com/takerum/vat_tf 
		<box id=5 pos=307.276,589.627,525.545,780.036>Results. The results are shown in Table 7. Unsurprisingly, adding continuous noise to the inputs works much better with images, where the inputs are naturally continuous, than with language. Therefore we see much better results from VAT on semi-supervised CIFAR-10 compared to on our NLP tasks. However, we still ﬁnd incorporating CVT improves over models without CVT. Our CVT + VAT models are competitive with current start-of-the-art approaches. We found the gains from CVT are larger when no data augmentation is applied, perhaps because random translations of the input expose the model to different “views” in a similar manner as with CVT. 
		<box id=6 pos=307.276,564.192,412.290,579.746>D Negative Results 
		<box id=7 pos=307.276,461.035,525.545,555.475>We brieﬂy describe a few ideas we implemented that did not seem to be effective in initial experiments. Note these ﬁndings are from early one-off experiments. We did not pursue them further after our ﬁrst attempts did not pan out, so it is possible that some of these approaches could be effective with the proper adjustments and tuning. 
		<box id=8 pos=318.658,314.814,525.545,455.923>• Hard vs soft targets: Classic self-training algorithms train the student model with one-hot “hard” targets corresponding to the teacher’s highest probability prediction. In our experiments, this decreased performance compared to using soft targets. This ﬁnding is consistent with research on knowledge distillation (Hinton et al., 2015; Furlanello et al., 2018) where soft targets also work notably better than hard targets. 
		<box id=9 pos=318.658,290.611,451.767,309.702>• Conﬁdence thresholding: 
		<box id=10 pos=329.094,155.043,525.545,303.680>Classic selftraining often only trains the student on a subset of the unlabeled examples on which the teacher has conﬁdent predictions (i.e., the output distribution has low entropy). We tried both “hard” (where the student ignores low-conﬁdence examples) and “soft” (where examples are weighted according to the teacher’s conﬁdence) versions of this for training our models, but they did not seem to improve performance. 
		<box id=11 pos=318.658,76.568,525.545,149.932>• Mean Teacher: The Mean Teacher method (Tarvainen and Valpola, 2017) tracks an exponential moving average (EMA) of model weights, which are used to produce targets for the students. The idea is that these tar
	<page id=15>
		<box id=0 pos=93.818,698.021,290.269,778.912>gets may be better quality due to a selfensembling effect. However, we found this approach to have little to no beneﬁt in our experiments, although using EMA model weights at time did improve results slightly. 
		<box id=1 pos=149.509,711.570,164.662,724.715>test 
		<box id=2 pos=83.382,540.013,290.269,694.672>• Purely supervised CVT: Lastly, we explored adding cross-view losses to purely supervised classiﬁers. We hoped that adding auxiliary softmax layers with different views of the input would act as a regularizer on the model. However, we found little to no beneﬁt from this approach. This negative result suggests that the gains from CVT are from the improved semi-supervised learning mechanism, not the additional prediction layers regularizing the model. 
	<page id=16>
		<box id=0 pos=76.698,725.772,97.382,733.784>Method 
		<box id=1 pos=76.698,528.926,189.165,718.641>LSTM-CNN-CRF (Ma and Hovy, 2016) LSTM-CNN (Chiu and Nichols, 2016) ID-CNN-CRF (Strubell et al., 2017) Tri-Trained LSTM (Lewis et al., 2016) Shortcut LSTM (Wu et al., 2017) JMT* (Hashimoto et al., 2017) LM-LSTM-CNN-CRF (Liu et al., 2017) TagLM ~ † (Peters et al., 2017) ELMo ~ † (Peters et al., 2018) NPM (Ma and Hovy, 2017) Deep Biafﬁne (Dozat and Manning, 2017) Stack Pointer (Ma et al., 2018) Stanford (Luong and Manning, 2015) Google (Luong et al., 2017) Supervised Virtual Adversarial Training* Word Dropout* ELMo* ELMo + Multi-task* ~ † CVT* CVT + Multi-Task* ~ † CVT + Multi-Task + Large* ~ † 
		<box id=2 pos=412.334,729.901,466.972,737.913>Dependency Parsing 
		<box id=3 pos=401.527,721.644,414.558,729.655>UAS 
		<box id=4 pos=442.688,721.644,455.247,729.655>LAS 
		<box id=5 pos=484.093,721.644,514.144,737.913>Translation BLEU 
		<box id=6 pos=236.639,721.644,262.868,737.913>Chunking F1 
		<box id=7 pos=318.962,721.644,332.259,737.913>FGN F1 
		<box id=8 pos=277.801,694.115,354.052,737.913>NER F1 91.21 91.62 ± 0.33 86.28 ± 0.26 90.65 ± 0.15 86.84 ± 0.19 
		<box id=9 pos=360.123,710.630,375.083,737.913>POS Acc. 97.55 
		<box id=10 pos=195.478,721.644,209.148,737.913>CCG Acc. 
		<box id=11 pos=195.478,677.600,210.437,693.869>94.7 95.08 
		<box id=12 pos=236.640,644.559,312.891,677.354>95.77 95.96 ± 0.08 91.71 ± 0.10 96.37 ± 0.05 91.93 ± 0.19 92.22 ± 0.10 
		<box id=13 pos=360.122,661.085,395.213,685.612>97.53 97.55 97.53 ± 0.03 
		<box id=14 pos=401.527,669.343,416.486,677.354>94.67 
		<box id=15 pos=442.688,669.343,457.648,677.354>92.90 
		<box id=16 pos=401.527,617.030,416.486,641.556>94.9 95.74 95.87 
		<box id=17 pos=442.688,617.030,457.648,641.556>93.0 94.08 94.19 
		<box id=18 pos=484.093,597.759,495.728,614.028>23.3 26.1 
		<box id=19 pos=195.478,528.926,519.182,598.427>94.94 ± 0.02 95.10 ± 0.06 91.16 ± 0.09 87.48 ± 0.08 97.60 ± 0.02 95.08 ± 0.03 93.27 ± 0.03 28.88 ± 0.12 95.07 ± 0.04 95.06 ± 0.06 91.75 ± 0.10 87.91 ± 0.11 97.64 ± 0.03 95.44 ± 0.06 93.72 ± 0.07 – 95.20 ± 0.04 95.79 ± 0.08 92.14 ± 0.11 88.06 ± 0.09 97.66 ± 0.01 95.56 ± 0.05 93.80 ± 0.08 29.33 ± 0.10 95.79 ± 0.04 96.50 ± 0.03 92.24 ± 0.09 88.49 ± 0.12 97.72 ± 0.01 96.22 ± 0.05 94.44 ± 0.06 29.34 ± 0.11 95.91 ± 0.05 96.83 ± 0.03 92.32 ± 0.12 88.37 ± 0.16 97.79 ± 0.03 96.40 ± 0.04 94.79 ± 0.05 – 95.65 ± 0.04 96.58 ± 0.04 92.34 ± 0.06 88.68 ± 0.14 97.70 ± 0.03 95.86 ± 0.03 94.06 ± 0.02 29.58 ± 0.07 95.97 ± 0.04 96.85 ± 0.05 92.42 ± 0.08 88.42 ± 0.13 97.76 ± 0.02 96.44 ± 0.04 94.83 ± 0.06 – 96.05 ± 0.03 96.98 ± 0.05 92.61 ± 0.09 88.81 ± 0.09 97.74 ± 0.02 96.61 ± 0.04 95.02 ± 0.04 – 
		<box id=20 pos=72.000,428.371,525.547,500.152>Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger model has four times as many hidden units as the others, making it similar in size to the models when ELMo is included. For dependency parsing, we omit results from Choe and Charniak (2016), Kuncoro et al. (2017), and Liu and Zhang (2017) because these train constituency parsers and convert the system outputs to dependency parses. They produce higher scores, but have access to more information during training and do not apply to datasets without constituency annotations. * denotes semi-supervised and † denotes multi-task. 
		<box id=21 pos=77.978,143.740,240.469,337.547>Parameter Word Embeddings Initializiation Character Embedding Size Character CNN Filter Widths Character CNN Num Filters Encoder LSTM sizes Encoder LSTM sizes, “Large” model LSTM projection layer size Hidden layer sizes Dropout EMA coefﬁcient Learning rate Momentum Batch size 
		<box id=22 pos=252.422,143.740,498.753,337.547>Value 300d GloVe 6B 50 [2, 3, 4] 300 (100 per ﬁlter width) 1024 for the ﬁrst layer, 512 for the second one 4096 for the ﬁrst layer, 2048 for the second one 512 512 0.5 for labeled examples, 0.8 for unlabeled examples 0.998 0.5/(1 + 0.005t ~ 0.5 ~ ) ( ~ t is number of SGD updates so far) 0.9 64 sentences 
		<box id=23 pos=217.532,120.286,380.012,132.291>Table 6: Hyperparameters for the model. 
	<page id=17>
		<box id=0 pos=92.922,581.070,126.860,594.215>Method 
		<box id=1 pos=92.922,321.366,303.729,569.369>GAN (Salimans et al., 2016) Stochastic Transformations (Sajjadi et al., 2016) Π model (Laine and Aila, 2017) Temporal Ensemble (Laine and Aila, 2017) Mean Teacher (Tarvainen and Valpola, 2017) Complement GAN (Dai et al., 2017) VAT (Miyato et al., 2017b) VAdD (Park et al., 2017) VAdD + VAT (Park et al., 2017) SNGT + Π model (Luong et al., 2017) SNGT + VAT (Luong et al., 2017) Consistency + WGAN (Wei et al., 2018) Manifold Mixup (Verma et al., 2018) Supervised VAT (ours) CVT, no input perturbation CVT, random input perturbation CVT, adversarial input perturbation 
		<box id=2 pos=345.569,587.844,391.431,600.989>CIFAR-10 
		<box id=3 pos=444.984,587.844,496.998,600.989>CIFAR-10+ 
		<box id=4 pos=400.432,574.295,450.428,587.440>4000 labels 
		<box id=5 pos=345.565,321.442,403.145,569.369>– – 16.55 ± 0.29 – – 14.41 ± 0.30 13.15 – – 13.62 ± 0.17 12.49 ± 0.36 – – 23.61 ± 0.60 13.29 ± 0.33 14.63 ± 0.20 13.80 ± 0.30 12.01 ± 0.11 
		<box id=6 pos=444.988,321.442,502.564,575.391>18.63 ± 2.32 11.29 ± 0.24 12.36 ± 0.31 12.16 ± 0.24 12.31 ± 0.28 – 10.55 11.68 ± 0.19 10.07 ± 0.11 11.00 ± 0.36 9.89 ± 0.34 9.98 ± 0.21 10.26 ± 0.32 19.61 ± 0.56 10.90 ± 0.31 12.44 ± 0.27 11.10 ± 0.26 10.11 ± 0.15 
		<box id=7 pos=72.000,254.050,525.547,301.921>Table 7: Error rates on semi-supervised CIFAR-10. We report means and standard deviations from 5 runs. CIFAR10+ refers to results where data augmentation (random translations of the input image) was applied. For some of our models we add a random or adversarially chosen perturbation to the student model’s inputs, which is done in most consistency regularization methods. 