<pages>
	<page id=1>
		<box id=0 pos=105.495,757.199,492.053,775.863>
			<line pos=105.495,757.199,492.053,775.863>Semi-Supervised Sequence Modeling with Cross-View Training 
		<box id=1 pos=95.605,716.010,504.430,731.564>
			<line pos=95.605,716.010,504.430,731.564>Kevin Clark ~ 1 Minh-Thang Luong ~ 2 Christopher D. Manning ~ 1 Quoc V. Le ~ 2 
		<box id=2 pos=127.890,701.979,380.178,716.385>
			<line pos=127.890,701.979,380.178,716.385>1 ~ Computer Science Department, Stanford University 
		<box id=3 pos=403.161,701.979,472.642,716.385>
			<line pos=403.161,701.979,472.642,716.385>2 ~ Google Brain 
		<box id=4 pos=75.762,689.363,524.772,698.275>
			<line pos=75.762,689.363,524.772,698.275>kevclark@cs.stanford.edu, thangluong@google.com, manning@cs.stanford.edu, qvl@google.com 
		<box id=5 pos=158.891,623.891,203.376,639.445>
			<line pos=158.891,623.891,203.376,639.445>Abstract 
		<box id=6 pos=89.008,301.344,273.256,612.228>
			<line pos=89.008,600.223,273.256,612.228>Unsupervised representation learning algo- 
			<line pos=89.008,588.268,273.256,600.273>rithms such as word2vec and ELMo improve 
			<line pos=89.008,576.313,273.256,588.318>the accuracy of many supervised NLP mod- 
			<line pos=89.008,564.357,273.256,576.362>els, mainly because they can take advantage 
			<line pos=89.008,552.402,273.256,564.407>of large amounts of unlabeled text. However, 
			<line pos=89.008,540.447,273.256,552.452>the supervised models only learn from task- 
			<line pos=89.008,528.492,273.256,540.497>speciﬁc labeled data during the main train- 
			<line pos=89.008,516.537,273.256,528.542>ing phase. We therefore propose Cross-View 
			<line pos=89.008,504.582,273.256,516.587>Training (CVT), a semi-supervised learning 
			<line pos=89.008,492.626,273.256,504.631>algorithm that improves the representations of 
			<line pos=89.008,480.671,273.256,492.676>a Bi-LSTM sentence encoder using a mix of 
			<line pos=89.008,468.716,273.256,480.721>labeled and unlabeled data. On labeled exam- 
			<line pos=89.008,456.761,273.256,468.766>ples, standard supervised learning is used. On 
			<line pos=89.008,444.806,273.256,456.811>unlabeled examples, CVT teaches auxiliary 
			<line pos=89.008,432.851,273.256,444.856>prediction modules that see restricted views 
			<line pos=89.008,420.895,273.256,432.900>of the input (e.g., only part of a sentence) to 
			<line pos=89.008,408.940,273.256,420.945>match the predictions of the full model see- 
			<line pos=89.008,396.985,273.256,408.990>ing the whole input. Since the auxiliary mod- 
			<line pos=89.008,385.030,273.256,397.035>ules and the full model share intermediate 
			<line pos=89.008,373.075,273.256,385.080>representations, this in turn improves the full 
			<line pos=89.008,361.119,273.256,373.124>model. Moreover, we show that CVT is par- 
			<line pos=89.008,349.164,273.256,361.169>ticularly effective when combined with multi- 
			<line pos=89.008,337.209,273.256,349.214>task learning. We evaluate CVT on ﬁve se- 
			<line pos=89.008,325.254,273.256,337.259>quence tagging tasks, machine translation, and 
			<line pos=89.008,313.299,273.256,325.304>dependency parsing, achieving state-of-the-art 
			<line pos=89.008,301.344,120.998,314.008>results. ~ 1 
		<box id=7 pos=72.000,274.989,77.978,290.543>
			<line pos=72.000,274.989,77.978,290.543>1 
		<box id=8 pos=89.933,274.989,154.814,290.543>
			<line pos=89.933,274.989,154.814,290.543>Introduction 
		<box id=9 pos=72.000,117.572,290.269,266.209>
			<line pos=72.000,253.064,290.269,266.209>Deep learning models work best when trained on 
			<line pos=72.000,239.514,290.269,252.659>large amounts of labeled data. However, acquir- 
			<line pos=72.000,225.965,290.269,239.110>ing labels is costly, motivating the need for ef- 
			<line pos=72.000,212.416,290.269,225.561>fective semi-supervised learning techniques that 
			<line pos=72.000,198.867,290.269,212.012>leverage unlabeled examples. A widely successful 
			<line pos=72.000,185.318,290.269,198.463>semi-supervised learning strategy for neural NLP 
			<line pos=72.000,171.768,290.269,184.913>is pre-training word vectors (Mikolov et al., 2013). 
			<line pos=72.000,158.219,290.269,171.364>More recent work trains a Bi-LSTM sentence en- 
			<line pos=72.000,144.670,290.269,157.815>coder to do language modeling and then incorpo- 
			<line pos=72.000,131.121,290.269,144.266>rates its context-sensitive representations into su- 
			<line pos=72.000,117.572,290.269,130.717>pervised models (Dai and Le, 2015; Peters et al., 
		<box id=10 pos=84.653,96.912,107.067,108.570>
			<line pos=84.653,96.912,107.067,108.570>1 ~ Code 
		<box id=11 pos=116.213,96.912,122.193,107.717>
			<line pos=116.213,96.912,122.193,107.717>is 
		<box id=12 pos=131.330,96.912,163.295,107.717>
			<line pos=131.330,96.912,163.295,107.717>available 
		<box id=13 pos=172.432,96.912,290.268,107.717>
			<line pos=172.432,96.912,290.268,107.717>at https://github.com/ 
		<box id=14 pos=72.000,77.606,281.814,96.966>
			<line pos=72.000,87.569,281.814,96.966>tensorflow/models/tree/master/research/ 
			<line pos=72.000,77.606,115.039,87.003>cvt_text 
		<box id=15 pos=307.276,391.865,525.545,637.179>
			<line pos=307.276,624.034,525.545,637.179>2018). Such pre-training methods perform unsu- 
			<line pos=307.276,610.485,525.545,623.630>pervised representation learning on a large corpus 
			<line pos=307.276,596.936,525.545,610.081>of unlabeled data followed by supervised training. 
			<line pos=318.185,581.554,525.545,594.699>A key disadvantage of pre-training is that the 
			<line pos=307.276,568.005,525.545,581.150>ﬁrst representation learning phase does not take 
			<line pos=307.276,554.456,525.545,567.601>advantage of labeled data – the model attempts 
			<line pos=307.276,540.906,525.545,554.051>to learn generally effective representations rather 
			<line pos=307.276,527.357,525.545,540.502>than ones that are targeted towards a particular 
			<line pos=307.276,513.808,525.545,526.953>task. Older semi-supervised learning algorithms 
			<line pos=307.276,500.259,525.545,513.404>like self-training do not suffer from this prob- 
			<line pos=307.276,486.710,525.545,499.855>lem because they continually learn about a task 
			<line pos=307.276,473.160,525.545,486.305>on a mix of labeled and unlabeled data. Self- 
			<line pos=307.276,459.611,525.545,472.756>training has historically been effective for NLP 
			<line pos=307.276,446.062,525.545,459.207>(Yarowsky, 1995; McClosky et al., 2006), but is 
			<line pos=307.276,432.513,525.545,445.658>less commonly used with neural models. This pa- 
			<line pos=307.276,418.964,525.545,432.109>per presents Cross-View Training (CVT), a new 
			<line pos=307.276,405.414,525.545,418.559>self-training algorithm that works well for neural 
			<line pos=307.276,391.865,384.229,405.010>sequence models. 
		<box id=16 pos=307.276,213.893,525.545,389.628>
			<line pos=318.185,376.483,525.545,389.628>In self-training, the model learns as normal on 
			<line pos=307.276,362.934,525.545,376.079>labeled examples. On unlabeled examples, the 
			<line pos=307.276,349.385,525.545,362.530>model acts as both a teacher that makes predictions 
			<line pos=307.276,335.836,525.545,348.981>about the examples and a student that is trained 
			<line pos=307.276,322.287,525.545,335.432>on those predictions. Although this process has 
			<line pos=307.276,308.737,525.545,321.882>shown value for some tasks, it is somewhat tau- 
			<line pos=307.276,295.188,525.545,308.333>tological: the model already produces the predic- 
			<line pos=307.276,281.639,525.545,294.784>tions it is being trained on. Recent research on 
			<line pos=307.276,268.090,525.545,281.235>computer vision addresses this by adding noise to 
			<line pos=307.276,254.541,525.545,267.686>the student’s input, training the model so it is ro- 
			<line pos=307.276,240.991,525.545,254.136>bust to input perturbations (Sajjadi et al., 2016; 
			<line pos=307.276,227.442,525.545,240.587>Wei et al., 2018). However, applying noise is dif- 
			<line pos=307.276,213.893,451.843,227.038>ﬁcult for discrete inputs like text. 
		<box id=17 pos=307.276,76.568,525.545,211.656>
			<line pos=318.185,198.511,525.545,211.656>As a solution, we take inspiration from multi- 
			<line pos=307.276,184.962,525.545,198.107>view learning (Blum and Mitchell, 1998; Xu et al., 
			<line pos=307.276,171.413,525.545,184.558>2013) and train the model to produce consistent 
			<line pos=307.276,157.863,525.541,171.139>predictions across different views of the input. In- 
			<line pos=307.276,144.314,525.545,157.459>stead of only training the full model as a student, 
			<line pos=307.276,130.765,525.545,143.910>CVT adds auxiliary prediction modules – neu- 
			<line pos=307.276,117.216,525.545,130.361>ral networks that transform vector representations 
			<line pos=307.276,103.667,525.545,116.812>into predictions – to the model and also trains them 
			<line pos=307.276,90.117,525.545,103.262>as students. The input to each student prediction 
			<line pos=307.276,76.568,525.545,89.713>module is a subset of the model’s intermediate rep- 
		<box id=18 pos=14.020,538.640,36.340,578.640>
			<line pos=14.020,568.640,36.340,578.640>8 
			<line pos=14.020,558.640,36.340,568.640>1 
			<line pos=14.020,548.640,36.340,558.640>0 
			<line pos=14.020,538.640,36.340,548.640>2 
		<box id=19 pos=14.020,533.640,36.340,538.640>
			<line pos=14.020,533.640,36.340,538.640>  
		<box id=20 pos=14.020,478.640,36.340,533.640>
			<line pos=14.020,523.640,36.340,533.640>p 
			<line pos=14.020,514.760,36.340,523.640>e 
			<line pos=14.020,503.640,36.340,514.760>S 
			<line pos=14.020,488.640,36.340,498.640>2 
			<line pos=14.020,478.640,36.340,488.640>2 
		<box id=21 pos=14.020,498.640,36.340,503.640>
			<line pos=14.020,498.640,36.340,503.640>  
		<box id=22 pos=14.020,398.100,36.340,478.640>
			<line pos=14.020,473.640,36.340,478.640>  
			<line pos=14.020,468.640,36.340,473.640>  
			<line pos=14.020,461.980,36.340,468.640>] 
			<line pos=14.020,449.760,36.340,461.980>L 
			<line pos=14.020,436.420,36.340,449.760>C 
			<line pos=14.020,431.420,36.340,436.420>. 
			<line pos=14.020,423.640,36.340,431.420>s 
			<line pos=14.020,414.760,36.340,423.640>c 
			<line pos=14.020,408.100,36.340,414.760>[ 
			<line pos=14.020,403.100,36.340,408.100>  
			<line pos=14.020,398.100,36.340,403.100>  
		<box id=23 pos=14.020,328.100,36.340,398.100>
			<line pos=14.020,388.100,36.340,398.100>1 
			<line pos=14.020,378.100,36.340,388.100>v 
			<line pos=14.020,368.100,36.340,378.100>0 
			<line pos=14.020,358.100,36.340,368.100>7 
			<line pos=14.020,348.100,36.340,358.100>3 
			<line pos=14.020,338.100,36.340,348.100>8 
			<line pos=14.020,328.100,36.340,338.100>0 
		<box id=24 pos=14.020,323.100,36.340,328.100>
			<line pos=14.020,323.100,36.340,328.100>. 
		<box id=25 pos=14.020,232.000,36.340,323.100>
			<line pos=14.020,313.100,36.340,323.100>9 
			<line pos=14.020,303.100,36.340,313.100>0 
			<line pos=14.020,293.100,36.340,303.100>8 
			<line pos=14.020,283.100,36.340,293.100>1 
			<line pos=14.020,277.540,36.340,283.100>: 
			<line pos=14.020,267.540,36.340,277.540>v 
			<line pos=14.020,261.980,36.340,267.540>i 
			<line pos=14.020,247.540,36.340,261.980>X 
			<line pos=14.020,240.880,36.340,247.540>r 
			<line pos=14.020,232.000,36.340,240.880>a 
	<page id=2>
		<box id=0 pos=72.000,698.021,290.269,778.912>
			<line pos=72.000,765.767,290.269,778.912>resentations corresponding to a restricted view of 
			<line pos=72.000,752.217,290.269,765.362>the input example. For example, one auxiliary pre- 
			<line pos=72.000,738.668,290.269,751.813>diction module for sequence tagging is attached to 
			<line pos=72.000,725.119,290.269,738.264>only the “forward” LSTM in the model’s ﬁrst Bi- 
			<line pos=72.000,711.570,290.269,724.715>LSTM layer, so it makes predictions without see- 
			<line pos=72.000,698.021,268.669,711.166>ing any tokens to the right of the current one. 
		<box id=1 pos=72.000,534.796,290.269,696.982>
			<line pos=82.909,683.837,290.269,696.982>CVT works by improving the model’s represen- 
			<line pos=72.000,670.288,290.269,683.433>tation learning. The auxiliary prediction modules 
			<line pos=72.000,656.739,290.269,669.884>can learn from the full model’s predictions be- 
			<line pos=72.000,643.190,290.269,656.335>cause the full model has a better, unrestricted view 
			<line pos=72.000,629.641,290.269,642.786>of the input. As the auxiliary modules learn to 
			<line pos=72.000,616.091,290.269,629.236>make accurate predictions despite their restricted 
			<line pos=72.000,602.542,290.269,615.687>views of the input, they improve the quality of the 
			<line pos=72.000,588.993,290.269,602.138>representations they are built on top of. This in 
			<line pos=72.000,575.444,290.269,588.589>turn improves the full model, which uses the same 
			<line pos=72.000,561.895,290.269,575.040>shared representations. In short, our method com- 
			<line pos=72.000,548.345,290.269,561.490>bines the idea of representation learning on unla- 
			<line pos=72.000,534.796,229.244,547.941>beled data with classic self-training. 
		<box id=2 pos=72.000,249.629,290.269,533.758>
			<line pos=82.909,520.613,290.269,533.758>CVT can be applied to a variety of tasks and 
			<line pos=72.000,507.064,290.269,520.209>neural architectures, but we focus on sequence 
			<line pos=72.000,493.514,290.269,506.659>modeling tasks where the prediction modules are 
			<line pos=72.000,479.965,290.269,493.110>attached to a shared Bi-LSTM encoder. We 
			<line pos=72.000,466.416,290.269,479.561>propose auxiliary prediction modules that work 
			<line pos=72.000,452.867,290.269,466.012>well for sequence taggers, graph-based depen- 
			<line pos=72.000,439.318,290.269,452.463>dency parsers, and sequence-to-sequence mod- 
			<line pos=72.000,425.768,290.269,438.913>els. We evaluate our approach on English de- 
			<line pos=72.000,412.219,290.269,425.364>pendency parsing, combinatory categorial gram- 
			<line pos=72.000,398.670,290.269,411.815>mar supertagging, named entity recognition, part- 
			<line pos=72.000,385.121,290.269,398.266>of-speech tagging, and text chunking, as well as 
			<line pos=72.000,371.572,290.269,384.717>English to Vietnamese machine translation. CVT 
			<line pos=72.000,358.022,290.269,371.167>improves over previously published results on all 
			<line pos=72.000,344.473,290.269,357.618>these tasks. Furthermore, CVT can easily and ef- 
			<line pos=72.000,330.924,290.269,344.069>fectively be combined with multi-task learning: 
			<line pos=72.000,317.375,290.269,330.520>we just add additional prediction modules for the 
			<line pos=72.000,303.826,290.269,316.971>different tasks on top of the shared Bi-LSTM en- 
			<line pos=72.000,290.276,290.269,303.421>coder. Training a uniﬁed model to jointly perform 
			<line pos=72.000,276.727,290.269,289.872>all of the tasks except machine translation im- 
			<line pos=72.000,263.178,290.269,276.323>proves results (outperforming a multi-task ELMo 
			<line pos=72.000,249.629,278.946,262.774>model) while decreasing the total training time. 
		<box id=3 pos=72.000,223.523,195.653,239.077>
			<line pos=72.000,223.523,195.653,239.077>2 Cross-View Training 
		<box id=4 pos=72.000,160.534,290.269,214.327>
			<line pos=72.000,201.182,290.269,214.327>We ﬁrst present Cross-View Training and describe 
			<line pos=72.000,187.632,290.269,200.777>how it can be combined effectively with multi-task 
			<line pos=72.000,174.083,290.269,187.228>learning. See Figure 1 for an overview of the train- 
			<line pos=72.000,160.534,124.124,173.679>ing method. 
		<box id=5 pos=72.000,76.568,290.269,150.221>
			<line pos=72.000,136.028,132.906,150.221>2.1 Method 
			<line pos=72.000,116.249,290.268,136.383>Let D ~ l = { ~ (x ~ 1 ~ , y ~ 1 ~ ), (x ~ 2 ~ , y ~ 2 ~ ), ..., (x ~ N , y ~ N ) ~ } repre- 
			<line pos=72.000,102.700,290.268,122.834>sent a labeled dataset and D ~ ul = { ~ x ~ 1 ~ , x ~ 2 ~ , ..., x ~ M ~ } 
			<line pos=72.000,89.150,290.268,109.285>represent an unlabeled dataset We use p ~ θ ~ (y ~ | ~ x ~ i ~ ) 
			<line pos=72.000,76.568,290.269,89.713>to denote the output distribution over classes pro- 
		<box id=6 pos=307.276,346.456,525.547,513.879>
			<line pos=307.276,501.874,525.547,513.879>Figure 1: An overview of Cross-View Training. The 
			<line pos=307.276,489.918,525.547,501.923>model is trained with standard supervised learning on 
			<line pos=307.276,477.963,525.547,489.968>labeled examples. On unlabeled examples, auxiliary 
			<line pos=307.276,466.008,525.547,478.013>prediction modules with different views of the input 
			<line pos=307.276,454.053,525.547,466.058>are trained to agree with the primary prediction mod- 
			<line pos=307.276,442.098,525.547,454.103>ule. This particular example shows CVT applied to 
			<line pos=307.276,430.143,525.547,442.148>named entity recognition. From the labeled example, 
			<line pos=307.276,418.187,525.547,430.192>the model can learn that “Washington” usually refers 
			<line pos=307.276,406.232,525.547,418.237>to a location. Then, on unlabeled data, auxiliary pre- 
			<line pos=307.276,394.277,525.547,406.282>diction modules are trained to reach the same predic- 
			<line pos=307.276,382.322,525.547,394.327>tion without seeing some of the input. In doing so, they 
			<line pos=307.276,370.367,525.547,382.372>improve the contextual representations produced by the 
			<line pos=307.276,358.411,525.547,370.416>model, for example, learning that “traveled to” is usu- 
			<line pos=307.276,346.456,416.327,358.461>ally followed by a location. 
		<box id=7 pos=307.276,231.607,525.545,327.271>
			<line pos=307.276,313.300,525.543,327.271>duced by the model with parameters θ on input x ~ i ~ . 
			<line pos=307.276,300.577,525.545,313.722>During CVT, the model alternates learning on a 
			<line pos=307.276,287.028,525.545,300.173>minibatch of labeled examples and learning on a 
			<line pos=307.276,273.479,525.545,286.624>minibatch of unlabeled examples. For labeled ex- 
			<line pos=307.276,259.929,511.385,273.074>amples, CVT uses standard cross-entropy loss: 
			<line pos=430.173,231.607,506.539,251.742>CE(y ~ i ~ , p ~ θ ~ (y ~ | ~ x ~ i ~ )) 
		<box id=8 pos=326.280,231.572,370.799,251.742>
			<line pos=326.280,231.572,370.799,251.742>L ~ sup ~ (θ) = 
		<box id=9 pos=404.115,238.749,419.872,279.461>
			<line pos=404.115,238.749,419.872,279.461>(cid:88) 
		<box id=10 pos=375.023,224.124,392.620,251.104>
			<line pos=381.094,240.195,386.549,251.104>1 
			<line pos=375.023,224.124,392.620,244.259>|D ~ l ~ | 
		<box id=11 pos=395.633,219.100,427.857,233.940>
			<line pos=395.633,219.100,427.857,233.940>x ~ i ~ ,y ~ i ~ ∈D ~ l 
		<box id=12 pos=307.276,75.742,525.549,211.656>
			<line pos=318.185,198.511,525.545,211.656>CVT adds k auxiliary prediction modules to the 
			<line pos=307.276,184.962,525.545,198.107>model, which are used when learning on unlabeled 
			<line pos=307.276,171.413,525.545,184.558>examples. A prediction module is usually a small 
			<line pos=307.276,157.863,525.545,171.008>neural network (e.g., a hidden layer followed by 
			<line pos=307.276,144.314,525.545,157.459>a softmax layer). Each one takes as input an in- 
			<line pos=307.276,129.939,525.547,143.910>termediate representation h ~ j ~ (x ~ i ~ ) produced by the 
			<line pos=307.276,117.216,525.545,130.361>model (e.g., the outputs of one of the LSTMs in a 
			<line pos=307.276,103.667,525.545,116.812>Bi-LSTM model). It outputs a distribution over la- 
			<line pos=333.416,87.405,525.549,109.284>θ ~ (y ~ | ~ x ~ i ~ ) ~ . Each h ~ j is chosen such that it only 
			<line pos=307.276,90.117,336.841,104.205>bels p ~ j 
			<line pos=307.276,75.742,525.542,89.713>uses a part of the input x ~ i ~ ; the particular choice 
	<page id=3>
		<box id=0 pos=151.708,615.197,176.467,628.840>
			<line pos=151.708,615.197,176.467,628.840>x ~ i ~ ∈D ~ ul 
		<box id=1 pos=141.121,622.026,151.708,659.454>
			<line pos=141.121,622.026,151.708,659.454>(cid:80) 
		<box id=2 pos=179.054,616.135,264.478,659.454>
			<line pos=179.054,622.026,193.694,659.454>(cid:80) ~ k 
			<line pos=189.641,616.135,264.478,635.976>j=1 D(p ~ θ ~ (y ~ | ~ x ~ i ~ ), p ~ j 
		<box id=3 pos=72.000,348.621,294.240,778.912>
			<line pos=72.000,765.767,290.269,778.912>can depend on the task and model architecture. We 
			<line pos=72.000,752.217,290.269,765.362>propose variants for several tasks in Section 3. The 
			<line pos=72.000,738.668,290.269,751.813>auxiliary prediction modules are only used during 
			<line pos=72.000,725.119,290.269,738.264>training; the test-time prediction come from the 
			<line pos=72.000,710.603,265.781,724.715>primary prediction module that produces p ~ θ ~ . 
			<line pos=82.909,697.733,290.269,710.878>On an unlabeled example, the model ﬁrst pro- 
			<line pos=72.000,683.217,290.267,703.351>duces soft targets p ~ θ ~ (y ~ | ~ x ~ i ~ ) by performing infer- 
			<line pos=72.000,670.635,290.269,683.780>ence. CVT trains the auxiliary prediction modules 
			<line pos=72.000,657.086,290.269,670.231>to match the primary prediction module on the un- 
			<line pos=72.000,643.536,191.084,656.681>labeled data by minimizing 
			<line pos=261.330,615.862,294.240,635.976>θ ~ (y ~ | ~ x ~ i ~ )) 
			<line pos=72.000,614.573,138.351,635.976>L ~ CVT ~ (θ) = 1 ~ |D ~ ul ~ | 
			<line pos=72.000,592.794,290.273,605.939>where D is a distance function between probabil- 
			<line pos=72.000,579.245,290.269,592.390>ity distributions (we use KL divergence). We hold 
			<line pos=72.000,564.729,290.268,584.863>the primary module’s prediction p ~ θ ~ (y ~ | ~ x ~ i ~ ) ﬁxed 
			<line pos=72.000,552.147,290.269,565.292>during training (i.e., we do not back-propagate 
			<line pos=72.000,538.598,290.269,551.743>through it) so the auxiliary modules learn to im- 
			<line pos=72.000,525.049,290.269,538.194>itate the primary one, but not vice versa. CVT 
			<line pos=72.000,511.499,290.269,524.644>works by enhancing the model’s representation 
			<line pos=72.000,497.950,290.269,511.095>learning. As the auxiliary modules train, the rep- 
			<line pos=72.000,484.401,290.269,497.546>resentations they take as input improve so they are 
			<line pos=72.000,470.852,290.269,483.997>useful for making predictions even when some of 
			<line pos=72.000,457.303,290.269,470.448>the model’s inputs are not available. This in turn 
			<line pos=72.000,443.753,290.269,456.898>improves the primary prediction module, which is 
			<line pos=72.000,430.204,277.811,443.349>built on top of the same shared representations. 
			<line pos=82.909,416.367,290.269,429.512>We combine the supervised and CVT losses into 
			<line pos=72.000,401.774,290.268,421.985>the total loss, L = L ~ sup + L ~ CVT ~ , and minimize it 
			<line pos=72.000,389.269,290.269,402.414>with stochastic gradient descent. In particular, we 
			<line pos=72.000,374.717,290.272,394.887>alternate minimizing L ~ sup over a minibatch of la- 
			<line pos=72.000,361.127,290.270,381.338>beled examples and minimizing L ~ CVT over a mini- 
			<line pos=72.000,348.621,199.080,361.766>batch of unlabeled examples. 
		<box id=4 pos=72.000,212.842,290.269,347.930>
			<line pos=82.909,334.785,290.269,347.930>For most neural networks, adding a few ad- 
			<line pos=72.000,321.235,290.269,334.380>ditional prediction modules is computationally 
			<line pos=72.000,307.686,290.269,320.831>cheap compared to the portion of the model build- 
			<line pos=72.000,294.137,290.269,307.282>ing up representations (such as an RNN or CNN). 
			<line pos=72.000,280.588,290.269,293.733>Therefore our method contributes little overhead 
			<line pos=72.000,267.039,290.269,280.184>to training time over other self-training approaches 
			<line pos=72.000,253.489,290.269,266.634>for most tasks. CVT does not change inference 
			<line pos=72.000,239.940,290.269,253.085>time or the number of parameters in the fully- 
			<line pos=72.000,226.391,290.269,239.536>trained model because the auxiliary prediction 
			<line pos=72.000,212.842,240.469,225.987>modules are only used during training. 
		<box id=5 pos=72.000,189.536,252.229,203.729>
			<line pos=72.000,189.536,252.229,203.729>2.2 Combining CVT with Multi-Task 
		<box id=6 pos=96.545,175.987,139.418,190.180>
			<line pos=96.545,175.987,139.418,190.180>Learning 
		<box id=7 pos=72.000,75.524,290.269,171.008>
			<line pos=72.000,157.863,290.269,171.008>CVT can easily be combined with multi-task 
			<line pos=72.000,144.314,290.269,157.459>learning by adding additional prediction modules 
			<line pos=72.000,130.765,290.269,143.910>for the other tasks on top of the shared Bi-LSTM 
			<line pos=72.000,117.216,290.269,130.361>encoder. During supervised learning, we ran- 
			<line pos=72.000,102.664,290.268,122.834>domly select a task and then update L ~ sup using 
			<line pos=72.000,90.118,290.269,103.263>a minibatch of labeled data for that task. When 
			<line pos=72.000,75.524,289.769,95.735>learning on the unlabeled data, we optimize L ~ CVT 
		<box id=8 pos=307.276,698.021,525.545,778.912>
			<line pos=307.276,765.767,525.545,778.912>jointly across all tasks at once, ﬁrst running infer- 
			<line pos=307.276,752.218,525.545,765.363>ence with all the primary prediction modules and 
			<line pos=307.276,738.668,525.545,751.813>then learning from the predictions with all the aux- 
			<line pos=307.276,725.119,525.545,738.264>iliary prediction modules. As before, the model 
			<line pos=307.276,711.570,525.545,724.715>alternates training on minibatches of labeled and 
			<line pos=307.276,698.021,396.185,711.166>unlabeled examples. 
		<box id=9 pos=307.276,494.681,525.547,697.515>
			<line pos=318.185,684.370,525.545,697.515>Examples labeled across many tasks are use- 
			<line pos=307.276,670.821,525.545,683.966>ful for multi-task systems to learn from, but most 
			<line pos=307.276,657.271,525.545,670.416>datasets are only labeled with one task. A beneﬁt 
			<line pos=307.276,643.722,525.545,656.867>of multi-task CVT is that the model creates (ar- 
			<line pos=307.276,630.173,525.545,643.318>tiﬁcial) all-tasks-labeled examples from unlabeled 
			<line pos=307.276,616.624,525.545,629.769>data. This signiﬁcantly improves the model’s data 
			<line pos=307.276,603.075,525.545,616.220>efﬁciency and training time. Since running pre- 
			<line pos=307.276,589.525,525.545,602.670>diction modules is computationally cheap, com- 
			<line pos=307.276,574.932,525.547,595.143>puting L ~ CVT is not much slower for many tasks 
			<line pos=307.276,562.427,525.545,575.572>than it is for a single one. However, we ﬁnd 
			<line pos=307.276,548.878,525.545,562.023>the all-tasks-labeled examples substantially speed 
			<line pos=307.276,535.329,525.545,548.474>up model convergence. For example, our model 
			<line pos=307.276,521.779,525.545,534.924>trained on six tasks takes about three times as long 
			<line pos=307.276,508.230,525.545,521.375>to converge as the average model trained on one 
			<line pos=307.276,494.681,493.003,507.826>task, a 50% decrease in total training time. 
		<box id=10 pos=307.276,470.419,471.110,485.973>
			<line pos=307.276,470.419,471.110,485.973>3 Cross-View Training Models 
		<box id=11 pos=307.276,381.651,525.545,462.542>
			<line pos=307.276,449.397,525.545,462.542>CVT relies on auxiliary prediction modules that 
			<line pos=307.276,435.848,525.545,448.993>have restricted views of the input. In this section, 
			<line pos=307.276,422.299,525.545,435.444>we describe speciﬁc constructions of the auxiliary 
			<line pos=307.276,408.749,525.545,421.894>prediction modules that are effective for sequence 
			<line pos=307.276,395.200,525.545,408.345>tagging, dependency parsing, and sequence-to- 
			<line pos=307.276,381.651,388.462,394.796>sequence learning. 
		<box id=12 pos=375.891,298.341,395.942,313.325>
			<line pos=375.891,298.341,395.942,313.325>i , x ~ 2 
		<box id=13 pos=391.707,298.341,426.392,313.325>
			<line pos=391.707,298.341,426.392,313.325>i , ..., x ~ T 
		<box id=14 pos=307.276,75.742,525.548,373.182>
			<line pos=307.276,358.989,462.731,373.182>3.1 Bi-LSTM Sentence Encoder 
			<line pos=307.276,341.234,525.545,354.379>All of our models use a two-layer CNN-BiLSTM 
			<line pos=307.276,327.685,525.545,340.830>(Chiu and Nichols, 2016; Ma and Hovy, 2016) 
			<line pos=307.276,314.135,525.545,327.280>sentence encoder. It takes as input a sequence of 
			<line pos=307.276,299.760,380.126,313.731>words x ~ i = [x ~ 1 
			<line pos=421.462,298.341,525.548,313.731>i ] ~ . First, each word is 
			<line pos=307.276,287.037,525.545,300.182>represented as the sum of an embedding vector 
			<line pos=307.276,273.488,525.545,286.633>and the output of a character-level Convolutional 
			<line pos=307.276,259.939,525.545,273.084>Neural Network, resulting in a sequence of vectors 
			<line pos=307.276,246.389,525.543,259.534>v = [v ~ 1 ~ , v ~ 2 ~ , ..., v ~ T ] ~ . The encoder applies a two- 
			<line pos=307.276,232.840,525.545,245.985>layer bidirectional LSTM (Graves and Schmidhu- 
			<line pos=307.276,219.291,525.545,232.436>ber, 2005) to these representations. The ﬁrst layer 
			<line pos=307.276,205.742,525.545,218.887>runs a Long Short-Term Memory unit (Hochre- 
			<line pos=307.276,192.193,525.545,205.338>iter and Schmidhuber, 1997) in the forward di- 
			<line pos=307.276,178.643,525.538,191.788>rection (taking v ~ t as input at each step t ~ ) and the 
			<line pos=307.276,165.094,525.547,183.683>backward direction (taking v ~ T ~ − ~ t+1 at each step) 
			<line pos=486.460,157.974,497.369,176.901>−→ 
			<line pos=497.372,148.045,525.544,163.303>1 ] and 
			<line pos=307.276,150.158,436.392,163.303>to produce vector sequences [ 
			<line pos=488.774,150.398,502.302,162.897>h T 
			<line pos=310.306,142.477,321.215,161.404>←− 
			<line pos=371.286,132.548,525.543,147.806>1 ] ~ . The output of the Bi-LSTM is 
			<line pos=312.618,134.901,325.450,147.400>h 1 
			<line pos=498.350,126.980,509.259,145.907>−→ 
			<line pos=307.276,134.901,310.307,145.810>[ 
			<line pos=321.215,132.548,328.979,145.810>1 ~ , 
			<line pos=509.261,117.051,525.543,138.331>1 ⊕ 
			<line pos=307.276,118.337,498.357,132.309>the concatenation of these vectors: h ~ 1 = [ 
			<line pos=500.664,119.404,513.496,131.903>h 1 
			<line pos=307.276,111.482,318.185,130.409>←− 
			<line pos=385.193,101.554,525.541,116.812>1 ]. The second Bi-LSTM layer 
			<line pos=309.588,103.907,322.420,116.406>h 1 
			<line pos=307.276,89.291,525.545,103.263>works the same, producing outputs h ~ 2 ~ , except it 
			<line pos=307.276,75.742,436.880,89.713>takes h ~ 1 as input instead of v ~ . 
		<box id=15 pos=352.614,101.554,385.193,161.404>
			<line pos=360.377,142.477,371.286,161.404>←− 
			<line pos=362.689,134.901,376.216,147.400>h T 
			<line pos=352.614,101.554,385.193,130.410>1 ⊕ ←− 
		<box id=16 pos=330.796,103.907,360.382,161.404>
			<line pos=330.796,142.477,341.705,161.404>←− 
			<line pos=333.108,134.901,345.940,147.400>h 2 
			<line pos=341.705,132.548,360.382,145.810>1 ~ , ... 
			<line pos=341.703,111.482,352.612,130.409>−→ 
			<line pos=344.017,103.907,357.544,116.406>h T 
		<box id=17 pos=436.389,148.045,455.064,176.901>
			<line pos=436.389,157.974,447.298,176.901>−→ 
			<line pos=438.703,150.398,451.535,162.897>h 1 
			<line pos=447.300,148.045,455.064,161.307>1 ~ , 
		<box id=18 pos=456.879,150.398,472.026,176.901>
			<line pos=456.879,157.974,467.788,176.901>−→ 
			<line pos=459.193,150.398,472.026,162.897>h 2 
		<box id=19 pos=318.185,101.554,339.892,114.816>
			<line pos=318.185,101.554,339.892,114.816>1 ~ , ..., 
		<box id=20 pos=467.791,148.045,486.467,161.307>
			<line pos=467.791,148.045,486.467,161.307>1 ~ , ... 
		<box id=21 pos=376.596,103.907,390.123,116.406>
			<line pos=376.596,103.907,390.123,116.406>h T 
	<page id=4>
		<box id=0 pos=72.000,656.763,290.270,780.036>
			<line pos=72.000,765.843,222.240,780.036>3.2 CVT for Sequence Tagging 
			<line pos=72.000,748.290,231.583,761.435>In sequence tagging, each token x ~ t 
			<line pos=228.525,746.045,290.270,761.435>i has a corre- 
			<line pos=72.000,734.741,147.295,747.886>sponding label y ~ t 
			<line pos=143.845,732.495,290.266,747.886>i ~ . The primary prediction module 
			<line pos=72.000,721.192,290.269,734.337>for sequence tagging produces a probability distri- 
			<line pos=72.000,707.643,290.269,720.854>bution over classes for the t ~ th label using a one- 
			<line pos=72.000,694.093,290.269,707.238>hidden-layer neural network applied to the corre- 
			<line pos=72.000,680.544,187.146,693.689>sponding encoder outputs: 
			<line pos=72.000,656.763,149.138,676.757>p(y ~ t ~ | ~ x ~ i ~ ) = NN ~ (h ~ t 
		<box id=1 pos=110.946,641.052,234.677,660.219>
			<line pos=110.946,641.052,234.677,660.219>= softmax ~ (U · ReLU ~ (W (h ~ t 
		<box id=2 pos=72.000,559.058,291.445,660.219>
			<line pos=231.619,639.165,259.028,660.219>1 ⊕ h ~ t 
			<line pos=255.970,639.165,291.445,652.201>2 ~ )) + b) 
			<line pos=256.523,622.823,267.432,641.750>−→ 
			<line pos=82.909,615.007,252.807,628.152>The auxiliary prediction modules take 
			<line pos=90.716,607.521,101.625,626.448>←− 
			<line pos=258.837,614.181,290.268,626.156>h 1 ~ (x ~ i ~ ) 
			<line pos=72.000,599.706,87.753,612.851>and 
			<line pos=93.028,598.879,290.266,612.851>h 1 ~ (x ~ i ~ ) ~ , the outputs of the forward and back- 
			<line pos=72.000,586.157,290.272,600.353>ward LSTMs in the ﬁrst ~ 2 Bi-LSTM layer, as in- 
			<line pos=72.000,572.607,290.269,585.752>puts. We add the following four auxiliary predic- 
			<line pos=72.000,559.058,252.295,572.203>tion modules to the model (see Figure 2): 
		<box id=3 pos=146.080,655.703,179.406,676.757>
			<line pos=146.080,655.703,173.489,676.757>1 ⊕ h ~ t 
			<line pos=170.431,655.703,179.406,668.739>2 ~ ) 
		<box id=4 pos=215.017,532.830,242.093,545.866>
			<line pos=215.017,532.830,242.093,545.866>1 ~ (x ~ i ~ )) 
		<box id=5 pos=128.910,475.007,243.424,561.460>
			<line pos=204.106,542.533,215.015,561.460>−→ 
			<line pos=128.910,533.890,204.108,553.884>(y ~ t ~ | ~ x ~ i ~ ) = NN ~ fwd ~ ( 
			<line pos=206.420,534.957,218.075,548.001>h t 
			<line pos=205.439,522.905,216.348,541.832>←− 
			<line pos=128.910,514.263,205.439,534.256>(y ~ t ~ | ~ x ~ i ~ ) = NN ~ bwd ~ ( 
			<line pos=207.751,515.329,219.406,528.373>h t 
			<line pos=216.348,513.202,243.424,526.238>1 ~ (x ~ i ~ )) 
			<line pos=210.745,503.277,221.654,522.204>−→ 
			<line pos=213.059,495.701,235.536,514.679>h t ~ − ~ 1 
			<line pos=128.910,494.635,210.747,514.628>(y ~ t ~ | ~ x ~ i ~ ) = NN ~ future ~ ( 
			<line pos=204.554,483.649,215.463,502.576>←− 
			<line pos=221.656,493.147,225.891,501.117>1 
			<line pos=128.910,475.007,204.555,495.000>(y ~ t ~ | ~ x ~ i ~ ) = NN ~ past ~ ( 
			<line pos=206.866,476.073,229.342,489.201>h t+1 
		<box id=6 pos=215.463,473.519,219.698,481.489>
			<line pos=215.463,473.519,219.698,481.489>1 
		<box id=7 pos=103.891,473.121,128.413,549.459>
			<line pos=110.530,534.957,128.413,549.459>p ~ fwd 
			<line pos=116.019,532.639,119.970,540.609>θ 
			<line pos=109.199,515.329,128.413,529.831>p ~ bwd 
			<line pos=114.688,513.011,118.639,520.981>θ 
			<line pos=103.891,495.701,128.413,510.203>p ~ future 
			<line pos=109.380,493.404,113.331,501.374>θ 
			<line pos=110.084,476.073,128.412,491.554>p ~ past 
			<line pos=115.572,473.121,119.523,481.091>θ 
		<box id=8 pos=236.033,494.635,258.377,506.610>
			<line pos=236.033,494.635,258.377,506.610>(x ~ i ~ )) 
		<box id=9 pos=229.840,475.007,252.184,486.982>
			<line pos=229.840,475.007,252.184,486.982>(x ~ i ~ )) 
		<box id=10 pos=72.000,344.485,290.269,466.024>
			<line pos=72.000,452.879,290.269,466.024>The “forward” module makes each prediction 
			<line pos=72.000,439.329,290.269,452.474>without seeing the right context of the current to- 
			<line pos=72.000,425.780,290.269,438.925>ken. The “future” module makes each predic- 
			<line pos=72.000,412.231,290.269,425.376>tion without the right context or the current to- 
			<line pos=72.000,398.682,290.269,411.827>ken itself. Therefore it works like a neural lan- 
			<line pos=72.000,385.133,290.269,398.278>guage model that, instead of predicting which to- 
			<line pos=72.000,371.583,290.269,384.728>ken comes next in the sequence, predicts which 
			<line pos=72.000,358.034,290.269,371.179>class of token comes next. The “backward” and 
			<line pos=72.000,344.485,204.698,357.630>“past” modules are analogous. 
		<box id=11 pos=102.396,234.887,226.631,263.826>
			<line pos=105.196,250.681,226.631,263.826>In particular, each word x ~ t 
			<line pos=102.396,234.887,137.081,249.871>i , ..., x ~ T 
		<box id=12 pos=72.000,115.189,290.273,336.624>
			<line pos=72.000,322.431,234.033,336.624>3.3 CVT for Dependency Parsing 
			<line pos=72.000,304.878,290.269,318.023>In a dependency parse, words in a sentence are 
			<line pos=72.000,291.329,208.124,304.474>treated as nodes in a graph. 
			<line pos=221.924,291.329,290.269,304.474>Typed directed 
			<line pos=72.000,277.780,290.269,290.925>edges connect the words, forming a tree struc- 
			<line pos=72.000,264.231,290.269,277.376>ture describing the syntactic structure of the sen- 
			<line pos=223.573,248.436,290.263,263.826>i in a sentence 
			<line pos=72.000,250.681,97.745,263.826>tence. 
			<line pos=132.151,234.887,290.270,250.277>i receives exactly one in-going edge 
			<line pos=72.000,236.306,106.631,249.871>x ~ i = x ~ 1 
			<line pos=196.046,221.337,290.273,236.728>i (called the “head”) 
			<line pos=72.000,223.583,200.949,236.728>(u, t, r) going from word x ~ u 
			<line pos=72.000,210.034,290.267,223.179>to it (the “dependent”) of type r (the “relation”). 
			<line pos=72.000,196.485,290.269,209.630>We use a graph-based dependency parser similar 
			<line pos=72.000,182.935,290.269,196.080>to the one from Dozat and Manning (2017). This 
			<line pos=72.000,169.386,290.269,182.531>treats dependency parsing as a classiﬁcation task 
			<line pos=72.000,155.837,290.269,168.982>where the goal is to predict which in-going edge 
			<line pos=238.255,140.042,244.538,155.433>i ~ . 
			<line pos=77.348,140.042,241.313,155.433>i = (u, t, r) connects to each word x ~ t 
			<line pos=72.000,142.528,80.798,155.027>y ~ t 
			<line pos=82.909,128.739,290.269,141.884>First, the representations produced by the en- 
			<line pos=72.000,115.189,290.269,128.334>coder for the candidate head and dependent are 
		<box id=13 pos=72.000,76.987,290.269,108.570>
			<line pos=84.653,96.912,290.269,108.570>2 ~ Modules taking inputs from the second Bi-LSTM layer 
			<line pos=72.000,86.950,290.269,97.755>would not have restricted views because information about 
			<line pos=72.000,76.987,280.388,87.792>the whole sentence gets propagated through the ﬁrst layer. 
		<box id=14 pos=307.276,561.417,525.547,645.153>
			<line pos=307.276,633.148,525.547,645.153>Figure 2: Auxiliary prediction modules for sequence 
			<line pos=307.276,621.193,525.547,633.198>tagging models. Each one sees a restricted view of the 
			<line pos=307.276,609.237,525.547,621.242>input. For example, the “forward” prediction module 
			<line pos=307.276,597.282,525.547,609.287>does not see any context to the right of the current token 
			<line pos=307.276,585.327,525.547,597.332>when predicting that token’s label. For simplicity, we 
			<line pos=307.276,573.372,525.547,585.377>only show a one layer Bi-LSTM encoder and only show 
			<line pos=307.276,561.417,488.805,573.422>the model’s predictions for a single time step. 
		<box id=15 pos=307.276,404.536,525.839,544.713>
			<line pos=307.276,531.568,525.545,544.713>passed through separate hidden layers. A bilin- 
			<line pos=307.276,518.019,525.545,531.164>ear classiﬁer applied to these representations pro- 
			<line pos=307.276,504.469,525.545,517.614>duces a score for each candidate edge. Lastly, 
			<line pos=307.276,490.920,525.545,504.065>these scores are passed through a softmax layer to 
			<line pos=307.276,477.371,525.545,490.516>produce probabilities. Mathematically, the proba- 
			<line pos=307.276,463.822,429.862,476.967>bility of an edge is given as: 
			<line pos=309.836,443.441,410.852,463.575>p ~ θ ~ ((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s(h ~ u 
			<line pos=307.276,424.994,444.793,438.139>where s is the scoring function: 
			<line pos=307.276,404.536,525.839,418.725>s(z ~ 1 ~ , z ~ 2 ~ , r) = ReLU ~ (W ~ head ~ z ~ 1 + b ~ head ~ )(W ~ r + W ) 
		<box id=16 pos=406.445,447.634,441.738,463.542>
			<line pos=406.445,447.634,441.738,463.542>1 (x ~ i ~ ) ~ ⊕ ~ h ~ u 
		<box id=17 pos=463.984,447.634,496.967,463.542>
			<line pos=463.984,447.634,496.967,463.542>1 ~ (x ~ i ~ ) ~ ⊕ ~ h ~ t 
		<box id=18 pos=437.331,447.634,466.834,458.899>
			<line pos=437.331,447.634,466.834,458.899>2 (x ~ i ~ ),h ~ t 
		<box id=19 pos=494.117,447.634,522.485,457.692>
			<line pos=494.117,447.634,522.485,457.692>2 ~ (x ~ i ~ ),r) 
		<box id=20 pos=372.983,387.998,470.267,402.187>
			<line pos=372.983,387.998,470.267,402.187>ReLU ~ (W ~ dep ~ z ~ 2 + b ~ dep ~ ) 
		<box id=21 pos=307.276,301.882,525.545,382.773>
			<line pos=307.276,368.801,524.811,382.773>The bilinear classiﬁer uses a weight matrix W ~ r 
			<line pos=307.276,356.079,525.545,369.224>speciﬁc to the candidate relation as well as a 
			<line pos=307.276,342.529,525.544,355.674>weight matrix W shared across all relations. Note 
			<line pos=307.276,328.980,525.545,342.125>that unlike in most prior work, our dependency 
			<line pos=307.276,315.431,525.545,328.576>parser only takes words as inputs, not words and 
			<line pos=307.276,301.882,393.305,315.027>part-of-speech tags. 
		<box id=22 pos=318.185,288.333,525.545,301.478>
			<line pos=318.185,288.333,525.545,301.478>We add four auxiliary prediction modules to our 
		<box id=23 pos=307.276,188.378,476.604,287.928>
			<line pos=307.276,274.783,439.407,287.928>model for cross-view training: 
			<line pos=444.034,263.358,452.505,277.178>−→ 
			<line pos=341.399,251.683,444.034,271.676>((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ fwd-fwd ~ ( 
			<line pos=445.828,257.823,456.909,267.000>h u 
			<line pos=452.502,255.735,474.275,265.793>1 (x ~ i ~ ), 
			<line pos=446.363,242.256,454.834,256.076>−→ 
			<line pos=342.730,230.581,446.363,250.574>((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ fwd-bwd ~ ( 
			<line pos=448.157,236.722,459.239,245.898>h u 
			<line pos=446.363,221.155,454.834,234.975>←− 
			<line pos=342.730,209.480,446.363,229.473>((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ bwd-fwd ~ ( 
			<line pos=448.157,215.620,459.239,224.797>h u 
			<line pos=454.832,213.532,476.604,223.590>1 (x ~ i ~ ), 
			<line pos=448.693,200.053,457.164,213.873>←− 
			<line pos=344.061,188.378,448.692,208.371>((u, t, r) ~ | ~ x ~ i ~ ) ∝ e ~ s ~ bwd-bwd ~ ( 
			<line pos=450.487,194.519,461.568,203.695>h u 
		<box id=24 pos=307.276,142.692,525.545,267.251>
			<line pos=307.971,252.749,340.901,267.251>p ~ fwd-fwd 
			<line pos=313.460,250.432,317.411,258.402>θ 
			<line pos=307.971,231.647,342.232,246.150>p ~ fwd-bwd 
			<line pos=313.460,229.330,317.411,237.300>θ 
			<line pos=307.971,210.546,342.232,225.048>p ~ bwd-fwd 
			<line pos=313.460,208.228,317.411,216.198>θ 
			<line pos=307.971,189.444,343.563,203.947>p ~ bwd-bwd 
			<line pos=313.460,187.127,317.411,195.097>θ 
			<line pos=307.276,169.790,525.545,182.935>Each one has some missing context (not seeing ei- 
			<line pos=307.276,156.241,525.545,169.386>ther the preceding or following words) for the can- 
			<line pos=307.276,142.692,470.563,155.837>didate head and candidate dependent. 
		<box id=25 pos=474.275,194.519,513.441,277.178>
			<line pos=474.275,263.358,482.746,277.178>−→ 
			<line pos=476.069,257.823,485.593,267.000>h t 
			<line pos=482.743,255.735,511.111,265.793>1 ~ (x ~ i ~ ),r) 
			<line pos=476.604,242.256,485.075,256.076>←− 
			<line pos=478.399,236.722,487.923,245.898>h t 
			<line pos=476.604,221.155,485.075,234.975>−→ 
			<line pos=478.399,215.620,487.923,224.797>h t 
			<line pos=485.073,213.532,513.441,223.590>1 ~ (x ~ i ~ ),r) 
			<line pos=478.934,200.053,487.405,213.873>←− 
			<line pos=480.728,194.519,490.252,203.695>h t 
		<box id=26 pos=485.073,234.634,513.441,244.692>
			<line pos=485.073,234.634,513.441,244.692>1 ~ (x ~ i ~ ),r) 
		<box id=27 pos=487.402,192.431,515.770,202.489>
			<line pos=487.402,192.431,515.770,202.489>1 ~ (x ~ i ~ ),r) 
		<box id=28 pos=457.161,192.431,478.933,202.489>
			<line pos=457.161,192.431,478.933,202.489>1 (x ~ i ~ ), 
		<box id=29 pos=454.832,234.634,476.604,244.692>
			<line pos=454.832,234.634,476.604,244.692>1 (x ~ i ~ ), 
		<box id=30 pos=307.276,76.568,525.545,135.412>
			<line pos=307.276,121.219,523.189,135.412>3.4 CVT for Sequence-to-Sequence Learning 
			<line pos=307.276,103.667,525.545,116.812>We use an encoder-decoder sequence-to-sequence 
			<line pos=307.276,90.117,525.545,103.262>model with attention (Sutskever et al., 2014; Bah- 
			<line pos=307.276,76.568,525.545,89.713>danau et al., 2015). Each example consists of an 
	<page id=5>
		<box id=0 pos=136.715,572.266,139.598,580.236>
			<line pos=136.715,572.266,139.598,580.236>i 
		<box id=1 pos=205.939,749.972,248.341,778.505>
			<line pos=213.656,763.521,248.341,778.505>i , ..., x ~ T 
			<line pos=205.939,749.972,242.732,764.956>i , ..., y ~ K 
		<box id=2 pos=72.000,574.713,290.274,778.912>
			<line pos=243.411,763.521,290.263,778.912>i and out- 
			<line pos=72.000,764.940,217.891,778.912>input (source) sequence x ~ i = x ~ 1 
			<line pos=72.000,751.391,210.566,765.362>put (target) sequence y ~ i = y ~ 1 
			<line pos=235.199,749.972,290.266,765.362>i . The en- 
			<line pos=72.000,738.668,290.269,751.813>coder’s representations are passed into an LSTM 
			<line pos=72.000,725.119,290.269,738.264>decoder using a bilinear attention mechanism (Lu- 
			<line pos=72.000,711.570,154.593,724.715>ong et al., 2015). 
			<line pos=166.647,711.570,290.269,724.715>In particular, at each time 
			<line pos=72.000,698.021,290.274,711.166>step t the decoder computes an attention distribu- 
			<line pos=72.000,683.645,290.268,703.638>tion over source sequence hidden states as α ~ j ∝ 
			<line pos=72.000,669.557,290.270,684.399>e ~ h ~ j W ~ α ~ ¯h ~ t where ¯h ~ t is the decoder’s current hid- 
			<line pos=72.000,656.008,290.269,669.153>den state. The source hidden states weighted by 
			<line pos=72.000,642.459,290.269,655.604>the attention distribution form a context vector: 
			<line pos=115.392,626.494,290.272,642.055>j α ~ j ~ h ~ j ~ . Next, the context vector and 
			<line pos=72.000,615.360,290.269,628.505>current hidden state are combined into an atten- 
			<line pos=72.000,600.985,290.267,614.956>tion vector a ~ t = tanh ~ (W ~ a ~ [c ~ t ~ , h ~ t ~ ]) ~ . Lastly, a soft- 
			<line pos=72.000,588.262,290.269,601.407>max layer predicts the next token in the output se- 
			<line pos=72.000,574.713,127.838,587.858>quence: p(y ~ t 
		<box id=3 pos=72.000,628.083,115.393,673.614>
			<line pos=72.000,628.083,115.393,673.614>c ~ t = (cid:80) 
		<box id=4 pos=147.249,573.886,250.901,587.858>
			<line pos=147.249,573.886,250.901,587.858>, x ~ i ~ ) = softmax ~ (W ~ s ~ a ~ t ~ ) ~ . 
		<box id=5 pos=124.388,572.467,146.751,593.880>
			<line pos=124.388,572.467,146.751,593.880>i ~ | ~ y ~ <t 
		<box id=6 pos=72.000,289.916,290.272,574.045>
			<line pos=82.909,560.900,290.269,574.045>We add two auxiliary decoders when applying 
			<line pos=72.000,547.351,290.269,560.496>CVT. The auxiliary decoders share embedding and 
			<line pos=72.000,533.802,290.269,546.947>LSTM parameters with the primary decoder, but 
			<line pos=72.000,520.253,290.269,533.398>have different parameters for the attention mech- 
			<line pos=72.000,506.703,290.269,519.848>anisms and softmax layers. For the ﬁrst one, we 
			<line pos=72.000,493.154,290.269,506.299>restrict its view of the input by applying atten- 
			<line pos=72.000,479.605,290.269,492.750>tion dropout, randomly zeroing out a fraction of 
			<line pos=72.000,466.056,290.269,479.201>its attention weights. The second one is trained 
			<line pos=72.000,452.507,290.269,465.652>to predict the next word in the target sequence 
			<line pos=72.000,438.957,224.962,453.154>rather than the current one: p ~ future 
			<line pos=257.911,438.131,290.264,450.106>, x ~ i ~ ) = 
			<line pos=72.000,425.408,142.242,439.605>softmax ~ (W future 
			<line pos=142.740,425.648,167.539,439.605>a ~ future 
			<line pos=148.506,423.204,290.272,438.553>t ~ − ~ 1 ) ~ . Since there is no target se- 
			<line pos=72.000,411.859,290.269,425.004>quence for unlabeled examples, we cannot apply 
			<line pos=72.000,398.310,290.269,411.455>teacher forcing to get an output distribution over 
			<line pos=72.000,384.761,290.269,397.906>the vocabulary from the primary decoder at each 
			<line pos=72.000,371.211,290.269,384.356>time step. Instead, we produce hard targets for the 
			<line pos=72.000,357.662,290.269,370.807>auxiliary modules by running the primary decoder 
			<line pos=72.000,344.113,290.269,357.258>with beam search on the input sequence. This 
			<line pos=72.000,330.564,290.269,343.709>idea has previously been applied to sequence-level 
			<line pos=72.000,317.015,290.269,330.160>knowledge distillation by Kim and Rush (2016) 
			<line pos=72.000,303.465,290.269,316.610>and makes the training procedure similar to back- 
			<line pos=72.000,289.916,220.767,303.061>translation (Sennrich et al., 2016). 
		<box id=7 pos=225.460,436.712,257.414,458.124>
			<line pos=235.051,436.712,257.414,458.124>i ~ | ~ y ~ <t 
			<line pos=225.460,439.197,238.500,451.696>(y ~ t 
		<box id=8 pos=205.929,436.354,209.880,444.324>
			<line pos=205.929,436.354,209.880,444.324>θ 
		<box id=9 pos=121.694,423.521,125.610,431.491>
			<line pos=121.694,423.521,125.610,431.491>s 
		<box id=10 pos=247.377,436.511,250.260,444.481>
			<line pos=247.377,436.511,250.260,444.481>i 
		<box id=11 pos=72.000,265.094,155.017,280.648>
			<line pos=72.000,265.094,155.017,280.648>4 Experiments 
		<box id=12 pos=72.000,76.568,290.273,256.817>
			<line pos=72.000,243.672,290.269,256.817>We compare Cross-View Training against several 
			<line pos=72.000,230.122,210.338,243.267>strong baselines on seven tasks: 
			<line pos=72.000,212.134,290.269,226.327>Combinatory Categorial Grammar (CCG) Su- 
			<line pos=72.000,198.508,290.264,212.777>pertagging: We use data from CCGBank (Hock- 
			<line pos=72.000,184.959,205.004,198.104>enmaier and Steedman, 2007). 
			<line pos=72.000,166.894,290.270,181.163>Text Chunking ~ : We use the CoNLL-2000 data 
			<line pos=72.000,153.345,245.018,166.490>(Tjong Kim Sang and Buchholz, 2000). 
			<line pos=72.000,135.281,290.273,149.550>Named Entity Recognition (NER) ~ : We use the 
			<line pos=72.000,121.731,290.269,134.876>CoNLL-2003 data (Tjong Kim Sang and De Meul- 
			<line pos=72.000,108.182,119.564,121.327>der, 2003). 
			<line pos=72.000,90.117,239.692,104.386>Fine-Grained NER (FGN) ~ : We 
			<line pos=72.000,76.568,240.295,89.713>OntoNotes (Hovy et al., 2006) dataset. 
		<box id=13 pos=251.049,90.117,265.591,103.262>
			<line pos=251.049,90.117,265.591,103.262>use 
		<box id=14 pos=276.936,90.117,290.267,103.262>
			<line pos=276.936,90.117,290.267,103.262>the 
		<box id=15 pos=307.276,648.325,525.545,780.036>
			<line pos=307.276,765.767,525.545,780.036>Part-of-Speech (POS) Tagging ~ : We use the Wall 
			<line pos=307.276,752.218,525.545,765.363>Street Journal portion of the Penn Treebank (Mar- 
			<line pos=307.276,738.668,379.385,751.813>cus et al., 1993). 
			<line pos=307.276,720.595,525.544,734.864>Dependency Parsing: We use the Penn Treebank 
			<line pos=307.276,707.046,525.545,720.191>converted to Stanford Dependencies version 3.3.0. 
			<line pos=307.276,688.973,525.544,703.242>Machine Translation: We use the English- 
			<line pos=307.276,675.424,525.545,688.569>Vietnamese translation dataset from IWSLT 2015 
			<line pos=307.276,661.875,525.545,675.020>(Cettolo et al., 2015). We report (tokenized) 
			<line pos=307.276,648.325,465.752,661.470>BLEU scores on the tst2013 test set. 
		<box id=16 pos=307.276,593.857,525.545,634.100>
			<line pos=318.185,620.955,525.545,634.100>We use the 1 Billion Word Language Model 
			<line pos=307.276,607.406,525.545,620.551>Benchmark (Chelba et al., 2014) as a pool of un- 
			<line pos=307.276,593.857,513.589,607.002>labeled sentences for semi-supervised learning. 
		<box id=17 pos=307.276,76.568,525.545,584.799>
			<line pos=307.276,570.606,462.436,584.799>4.1 Model Details and Baselines 
			<line pos=307.276,552.513,525.545,565.658>We apply dropout during training, but not when 
			<line pos=307.276,538.964,525.545,552.109>running the primary prediction module to pro- 
			<line pos=307.276,525.415,490.833,538.560>duce soft targets on unlabeled examples. 
			<line pos=498.403,525.415,525.545,538.560>In ad- 
			<line pos=307.276,511.866,525.545,525.011>dition to the auxiliary prediction modules listed 
			<line pos=307.276,498.316,525.545,511.461>in Section 3, we ﬁnd it slightly improves re- 
			<line pos=307.276,484.767,525.545,497.912>sults to add another one that sees the whole in- 
			<line pos=307.276,471.218,525.545,484.363>put rather than a subset (but unlike the primary 
			<line pos=307.276,457.669,525.545,470.814>prediction module, does have dropout applied to 
			<line pos=307.276,444.120,525.545,457.265>its representations). Unless indicated otherwise, 
			<line pos=307.276,430.570,525.545,443.715>our models have LSTMs with 1024-sized hidden 
			<line pos=307.276,417.021,525.545,430.166>states and 512-sized projection layers. See the ap- 
			<line pos=307.276,403.472,525.545,416.617>pendix for full training details and hyperparame- 
			<line pos=307.276,389.923,525.545,403.068>ters. We compare CVT with the following other 
			<line pos=307.276,376.374,469.080,389.519>semi-supervised learning algorithms: 
			<line pos=307.276,357.810,380.302,372.003>Word Dropout. 
			<line pos=389.429,357.734,525.542,370.879>In this method, we only train 
			<line pos=307.276,344.185,525.545,357.330>the primary prediction module. When acting as 
			<line pos=307.276,330.635,525.545,343.780>a teacher it is run as normal, but when acting as 
			<line pos=307.276,317.086,525.545,330.231>a student, we randomly replace some of the input 
			<line pos=307.276,303.537,525.540,316.682>words with a REMOVED token. This is similar to 
			<line pos=307.276,289.988,525.545,303.133>CVT in that it exposes the model to a restricted 
			<line pos=307.276,276.439,525.545,289.584>view of the input. However, it is less data efﬁ- 
			<line pos=307.276,262.889,525.545,276.034>cient. By carefully designing the auxiliary pre- 
			<line pos=307.276,249.340,525.545,262.485>diction modules, it is possible to train the auxil- 
			<line pos=307.276,235.791,525.545,248.936>iary prediction modules to match the primary one 
			<line pos=307.276,222.242,525.545,235.387>across many different views of the input a once, 
			<line pos=307.276,208.693,457.887,221.838>rather than just one view at a time. 
			<line pos=307.276,190.053,525.544,204.322>Virtual Adversarial Training (VAT). VAT (Miy- 
			<line pos=307.276,176.504,525.545,189.649>ato et al., 2016) works like word dropout, but 
			<line pos=307.276,162.954,525.545,176.099>adds noise to the word embeddings of the stu- 
			<line pos=307.276,149.405,525.545,162.550>dent instead of dropping out words. Notably, the 
			<line pos=307.276,135.856,525.545,149.001>noise is chosen adversarially so it most changes 
			<line pos=307.276,122.307,525.545,135.452>the model’s prediction. This method was applied 
			<line pos=307.276,108.758,525.545,121.903>successfully to semi-supervised text classiﬁcation 
			<line pos=307.276,95.208,415.745,108.353>by Miyato et al. (2017a). 
			<line pos=307.276,76.568,525.545,90.837>ELMo. ELMo incorporates the representations 
	<page id=6>
		<box id=0 pos=75.985,759.470,109.923,772.615>
			<line pos=75.985,759.470,109.923,772.615>Method 
		<box id=1 pos=75.985,504.235,239.905,747.770>
			<line pos=75.985,734.625,221.185,747.770>Shortcut LSTM (Wu et al., 2017) 
			<line pos=75.985,721.075,234.156,734.220>ID-CNN-CRF (Strubell et al., 2017) 
			<line pos=75.985,707.517,209.934,726.106>JMT ~ † (Hashimoto et al., 2017) 
			<line pos=75.985,693.968,201.756,707.113>TagLM* (Peters et al., 2017) 
			<line pos=75.985,680.419,197.785,693.564>ELMo* (Peters et al., 2018) 
			<line pos=75.985,662.347,234.778,675.492>Biafﬁne (Dozat and Manning, 2017) 
			<line pos=75.985,648.798,210.211,661.943>Stack Pointer (Ma et al., 2018) 
			<line pos=75.985,630.727,239.905,643.872>Stanford (Luong and Manning, 2015) 
			<line pos=75.985,617.178,197.785,630.323>Google (Luong et al., 2017) 
			<line pos=75.985,599.107,124.465,612.252>Supervised 
			<line pos=75.985,585.557,205.061,598.702>Virtual Adversarial Training* 
			<line pos=75.985,572.008,144.494,585.153>Word Dropout* 
			<line pos=75.985,558.459,205.672,571.604>ELMo (our implementation)* 
			<line pos=75.985,544.901,170.751,563.489>ELMo + Multi-task* ~ † 
			<line pos=75.985,531.351,103.258,544.496>CVT* 
			<line pos=75.985,517.793,164.085,536.382>CVT + Multi-task* ~ † 
			<line pos=75.985,504.235,200.936,522.823>CVT + Multi-task + Large* ~ † 
		<box id=2 pos=263.066,734.625,521.559,779.390>
			<line pos=263.066,766.245,521.559,779.390>CCG Chunk NER FGN POS Dep. Parse Translate 
			<line pos=263.066,752.696,304.990,765.841>Acc. F1 
			<line pos=263.066,734.625,282.157,747.770>95.1 
		<box id=3 pos=390.102,734.625,509.832,765.841>
			<line pos=390.102,752.696,509.832,765.841>Acc. UAS LAS BLEU 
			<line pos=390.102,734.625,414.648,747.770>97.53 
		<box id=4 pos=330.528,752.696,342.048,765.841>
			<line pos=330.528,752.696,342.048,765.841>F1 
		<box id=5 pos=360.321,752.696,371.841,765.841>
			<line pos=360.321,752.696,371.841,765.841>F1 
		<box id=6 pos=330.530,721.075,379.414,734.220>
			<line pos=330.530,721.075,379.414,734.220>90.7 86.8 
		<box id=7 pos=293.465,693.968,312.556,720.662>
			<line pos=293.465,707.517,312.556,720.662>95.8 
			<line pos=293.465,693.968,312.556,707.113>96.4 
		<box id=8 pos=330.530,680.419,349.625,707.113>
			<line pos=330.534,693.968,349.625,707.113>91.9 
			<line pos=330.530,680.419,349.621,693.564>92.2 
		<box id=9 pos=390.109,707.517,471.467,720.662>
			<line pos=390.109,707.517,471.467,720.662>97.55 94.7 92.9 
		<box id=10 pos=423.020,648.798,471.467,675.492>
			<line pos=423.020,662.347,471.467,675.492>95.7 94.1 
			<line pos=423.020,648.798,471.467,661.943>95.9 94.2 
		<box id=11 pos=330.528,504.235,500.439,643.872>
			<line pos=481.348,630.727,500.439,643.872>23.3 
			<line pos=481.348,617.178,500.439,630.323>26.1 
			<line pos=330.528,599.107,500.439,612.252>91.2 87.5 97.60 95.1 93.3 28.9 
			<line pos=330.528,585.557,486.803,598.702>91.8 87.9 97.64 95.4 93.7 – 
			<line pos=330.528,572.008,500.439,585.153>92.1 88.1 97.66 95.6 93.8 29.3 
			<line pos=330.528,558.459,500.439,571.604>92.2 88.5 97.72 96.2 94.4 29.3 
			<line pos=330.528,544.901,486.803,559.170>92.3 88.4 97.79 96.4 94.8 – 
			<line pos=330.528,531.351,500.439,545.620>92.3 88.7 97.70 95.9 94.1 29.6 
			<line pos=330.528,517.793,486.803,530.938>92.4 88.4 97.76 96.4 94.8 – 
			<line pos=330.528,504.235,486.803,518.504>92.6 88.8 97.74 96.6 95.0 – 
		<box id=12 pos=263.066,504.311,282.157,612.252>
			<line pos=263.066,599.107,282.157,612.252>94.9 
			<line pos=263.066,585.557,282.157,598.702>95.1 
			<line pos=263.066,572.008,282.157,585.153>95.2 
			<line pos=263.066,558.459,282.157,571.604>95.8 
			<line pos=263.066,544.901,282.157,558.046>95.9 
			<line pos=263.066,531.351,282.157,544.496>95.7 
			<line pos=263.066,517.793,282.157,530.938>96.0 
			<line pos=263.066,504.311,282.157,518.504>96.1 
		<box id=13 pos=293.470,504.311,312.561,612.252>
			<line pos=293.470,599.107,312.561,612.252>95.1 
			<line pos=293.470,585.557,312.561,598.702>95.1 
			<line pos=293.470,572.008,312.561,585.153>95.8 
			<line pos=293.470,558.459,312.561,571.604>96.5 
			<line pos=293.470,544.901,312.561,558.046>96.8 
			<line pos=293.470,531.351,312.561,544.496>96.6 
			<line pos=293.470,517.793,312.561,530.938>96.9 
			<line pos=293.470,504.311,312.561,518.504>97.0 
		<box id=14 pos=72.000,442.588,525.547,490.459>
			<line pos=72.000,478.454,525.547,490.459>Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around 
			<line pos=72.000,466.499,525.547,478.504>0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with 
			<line pos=72.000,454.544,525.547,466.549>them included. The +Large model has four times as many hidden units as the others, making it similar in size to 
			<line pos=72.000,442.588,426.514,459.089>the models when ELMo is included. * denotes semi-supervised and † denotes multi-task. 
		<box id=15 pos=72.000,317.334,290.269,425.323>
			<line pos=72.000,412.178,290.269,425.323>from a large separately-trained language model 
			<line pos=72.000,398.629,290.269,411.774>into a task-speciﬁc model. Our implementaiton 
			<line pos=72.000,385.080,290.269,398.225>follows Peters et al. (2018). When combining 
			<line pos=72.000,371.530,290.269,384.675>ELMo with multi-task learning, we allow each 
			<line pos=72.000,357.981,290.269,371.126>task to learn its own weights for the ELMo em- 
			<line pos=72.000,344.432,290.269,357.577>beddings going into each prediction module. We 
			<line pos=72.000,330.883,290.269,344.028>found applying dropout to the ELMo embeddings 
			<line pos=72.000,317.334,266.706,330.479>was crucial for achieving good performance. 
		<box id=16 pos=72.000,289.001,130.484,303.194>
			<line pos=72.000,289.001,130.484,303.194>4.2 Results 
		<box id=17 pos=72.000,76.568,290.269,281.141>
			<line pos=72.000,267.996,290.269,281.141>Results are shown in Table 1. CVT on its own out- 
			<line pos=72.000,254.447,290.269,267.592>performs or is comparable to the best previously 
			<line pos=72.000,240.898,290.269,254.043>published results on all tasks. Figure 3 shows an 
			<line pos=72.000,227.348,286.364,240.493>example win for CVT over supervised learning. . 
			<line pos=82.909,212.060,290.269,225.205>Of the prior results listed in Table 1, only 
			<line pos=72.000,198.511,290.269,211.656>TagLM and ELMo are semi-supervised. These 
			<line pos=72.000,184.962,290.269,198.107>methods ﬁrst train an enormous language model 
			<line pos=72.000,171.413,290.269,184.558>on unlabeled data and incorporate the representa- 
			<line pos=72.000,157.863,290.269,171.008>tions produced by the language model into a su- 
			<line pos=72.000,144.314,290.269,157.459>pervised classiﬁer. Our base models use 1024 hid- 
			<line pos=72.000,130.765,290.269,143.910>den units in their LSTMs (compared to 4096 in 
			<line pos=72.000,117.216,290.269,130.361>ELMo), require fewer training steps (around one 
			<line pos=72.000,103.667,290.269,116.812>pass over the billion-word benchmark rather than 
			<line pos=72.000,90.117,290.269,103.262>many passes), and do not require a pipelined train- 
			<line pos=72.000,76.568,290.269,89.713>ing procedure. Therefore, although they perform 
		<box id=18 pos=307.276,236.216,525.547,331.907>
			<line pos=307.276,319.902,525.547,331.907>Figure 3: An NER example that CVT classiﬁes cor- 
			<line pos=307.276,307.947,525.547,319.952>rectly but supervised learning does not. “Warner” only 
			<line pos=307.276,295.992,525.547,307.997>occurs as a last name in the train set, so the supervised 
			<line pos=307.276,284.037,525.547,296.042>model classiﬁes “Warner Bros” as a person. The CVT 
			<line pos=307.276,272.082,525.547,284.087>model also mistakenly classiﬁes “Warner Bros” as a 
			<line pos=307.276,260.126,525.547,272.131>person to start with, but as it sees more of the unlabeled 
			<line pos=307.276,248.171,525.547,260.176>data (in which “Warner” occurs thousands of times) it 
			<line pos=307.276,236.216,485.806,248.221>learns that “Warner Bros” is an organization. 
		<box id=19 pos=307.276,99.333,525.545,207.322>
			<line pos=307.276,194.177,525.545,207.322>on par with ELMo, they are faster and simpler 
			<line pos=307.276,180.628,342.556,193.773>to train. 
			<line pos=349.974,180.628,525.545,193.773>Increasing the size of our CVT+Multi- 
			<line pos=307.276,167.079,525.545,180.224>task model so it has 4096 units in its LSTMs like 
			<line pos=307.276,153.530,525.545,166.675>ELMo improves results further so they are signiﬁ- 
			<line pos=307.276,139.981,525.545,153.126>cantly better than the ELMo+Multi-task ones. We 
			<line pos=307.276,126.431,525.545,139.576>suspect there could be further gains from combin- 
			<line pos=307.276,112.882,525.545,126.027>ing our method with language model pre-training, 
			<line pos=307.276,99.333,446.738,112.478>which we leave for future work. 
		<box id=20 pos=307.276,76.568,525.548,90.837>
			<line pos=307.276,76.568,525.548,90.837>CVT + Multi-Task. We train a single shared- 
	<page id=7>
		<box id=0 pos=72.000,305.094,290.269,778.912>
			<line pos=72.000,765.767,290.269,778.912>encoder CVT model to perform all of the tasks 
			<line pos=72.000,752.217,290.269,765.362>except machine translation (as it is quite differ- 
			<line pos=72.000,738.668,290.269,751.813>ent and requires more training time than the other 
			<line pos=72.000,725.119,290.269,738.264>ones). Multi-task learning improves results on 
			<line pos=72.000,711.570,290.269,724.715>all of the tasks except ﬁne-grained NER, some- 
			<line pos=72.000,698.021,290.269,711.166>times by large margins. Prior work on many-task 
			<line pos=72.000,684.472,290.269,697.617>NLP such as Hashimoto et al. (2017) uses compli- 
			<line pos=72.000,670.922,290.269,684.067>cated architectures and training algorithms. Our 
			<line pos=72.000,657.373,290.269,670.518>result shows that simple parameter sharing can be 
			<line pos=72.000,643.824,290.269,656.969>enough for effective many-task learning when the 
			<line pos=72.000,630.275,290.269,643.420>model is big and trained on a large amount of data. 
			<line pos=82.909,616.726,290.269,629.871>Interestingly, multi-task learning works better 
			<line pos=72.000,603.176,290.269,616.321>in conjunction with CVT than with ELMo. We 
			<line pos=72.000,589.627,290.269,602.772>hypothesize that the ELMo models quickly ﬁt to 
			<line pos=72.000,576.078,290.269,589.223>the data primarily using the ELMo vectors, which 
			<line pos=72.000,562.529,290.269,575.674>perhaps hinders the model from learning effective 
			<line pos=72.000,548.980,290.269,562.125>representations that transfer across tasks. We also 
			<line pos=72.000,535.430,290.269,548.575>believe CVT alleviates the danger of the model 
			<line pos=72.000,521.881,290.269,535.026>“forgetting” one task while training on the other 
			<line pos=72.000,508.332,290.269,521.477>ones, a well-known problem in many-task learn- 
			<line pos=72.000,494.783,290.269,507.928>ing (Kirkpatrick et al., 2017). During multi-task 
			<line pos=72.000,481.234,290.269,494.379>CVT, the model makes predictions about unla- 
			<line pos=72.000,467.684,290.269,480.829>beled examples across all tasks, creating (artiﬁ- 
			<line pos=72.000,454.135,290.269,467.280>cial) all-tasks-labeled examples, so the model does 
			<line pos=72.000,440.586,290.269,453.731>not only see one task at a time. In fact, multi-task 
			<line pos=72.000,427.037,290.269,440.182>learning plus self training is similar to the Learn- 
			<line pos=72.000,413.488,290.269,426.633>ing without Forgetting algorithm (Li and Hoiem, 
			<line pos=72.000,399.938,290.269,413.083>2016), which trains the model to keep its predic- 
			<line pos=72.000,386.389,290.269,399.534>tions on an old task unchanged when learning a 
			<line pos=72.000,372.840,290.269,385.985>new task. To test the value of all-tasks-labeled ex- 
			<line pos=72.000,359.291,290.269,372.436>amples, we trained a multi-task CVT model that 
			<line pos=72.000,344.698,290.265,364.909>only computes L ~ CVT on one task at a time (chosen 
			<line pos=72.000,332.193,290.269,345.338>randomly for each unlabeled minibatch) instead of 
			<line pos=72.000,318.643,290.269,331.788>for all tasks in parallel. The one-at-a-time model 
			<line pos=72.000,305.094,259.462,318.239>performs substantially worse (see Table 2). 
		<box id=1 pos=74.989,265.074,108.532,290.803>
			<line pos=74.989,279.998,98.400,290.803>Model 
			<line pos=74.989,265.074,108.532,275.879>CVT-MT 
		<box id=2 pos=80.658,255.111,141.172,265.916>
			<line pos=80.658,255.111,141.172,265.916>w/out all-labeled 
		<box id=3 pos=152.028,255.111,293.007,290.803>
			<line pos=152.028,279.998,293.007,290.803>CCG Chnk NER FGN POS Dep. 
			<line pos=201.855,265.074,291.519,275.879>96.0 86.7 97.74 94.4 
			<line pos=152.028,265.074,192.135,275.879>95.7 97.4 
			<line pos=152.031,255.111,192.137,265.916>95.4 97.1 
			<line pos=201.848,255.111,291.512,265.916>95.6 86.3 97.71 94.1 
		<box id=4 pos=72.000,214.996,290.271,238.956>
			<line pos=72.000,226.951,290.271,238.956>Table 2: Dev set performance of multi-task CVT with 
			<line pos=72.000,214.996,272.457,227.001>and without producing all-tasks-labeled examples. 
		<box id=5 pos=72.000,76.568,290.269,199.231>
			<line pos=72.000,184.962,290.263,199.231>Model Generalization. In order to evaluate how 
			<line pos=72.000,171.413,290.269,184.558>our models generalize to the dev set from the train 
			<line pos=72.000,157.863,290.269,171.008>set, we plot the dev vs. train accuracy for our dif- 
			<line pos=72.000,144.314,290.269,157.459>ferent methods as they learn (see Figure 4). Both 
			<line pos=72.000,130.765,290.269,143.910>CVT and multi-task learning improve model gen- 
			<line pos=72.000,117.216,290.269,130.361>eralization: for the same train accuracy, the mod- 
			<line pos=72.000,103.667,290.269,116.812>els get better dev accuracy than purely supervised 
			<line pos=72.000,90.117,290.269,103.262>learning. Interestingly, CVT continues to improve 
			<line pos=72.000,76.568,290.269,89.713>in dev set accuracy while close to 100% train ac- 
		<box id=6 pos=307.276,536.662,525.547,572.577>
			<line pos=307.276,560.572,525.547,572.577>Figure 4: Dev set vs. Train set accuracy for various 
			<line pos=307.276,548.617,525.547,560.622>methods. The “small” model has 1/4 the LSTM hidden 
			<line pos=307.276,536.662,503.171,548.667>state size of the other ones (256 instead of 1024). 
		<box id=7 pos=307.276,256.476,525.545,518.325>
			<line pos=307.276,505.180,525.545,518.325>curacy for CCG, Chunking, and NER, perhaps be- 
			<line pos=307.276,491.631,525.545,504.776>cause the model is still learning from unlabeled 
			<line pos=307.276,478.081,525.545,491.226>data even when it has completely ﬁt to the train 
			<line pos=307.276,464.532,525.545,477.677>set. We also show results for a smaller multi-task 
			<line pos=307.276,450.983,525.545,464.128>+ CVT model. Although it generalizes at least as 
			<line pos=307.276,437.434,525.545,450.579>well as the larger one, it halts making progress on 
			<line pos=307.276,423.885,525.545,437.030>the train set earlier. This suggests it is important 
			<line pos=307.276,410.336,525.545,423.481>to use sufﬁciently large neural networks for multi- 
			<line pos=307.276,396.786,525.545,409.931>task learning: otherwise the model does not have 
			<line pos=307.276,383.237,483.927,396.382>the capacity to ﬁt to all the training data. 
			<line pos=307.276,364.869,525.543,379.138>Auxiliary Prediction Module Ablation. We 
			<line pos=307.276,351.320,525.545,364.465>brieﬂy explore which auxiliary prediction modules 
			<line pos=307.276,337.771,525.545,350.916>are more important for the sequence tagging tasks 
			<line pos=307.276,324.221,525.545,337.366>in Table 3. We ﬁnd that both kinds of auxiliary 
			<line pos=307.276,310.672,525.545,323.817>prediction modules improve performance, but that 
			<line pos=307.276,297.123,525.545,310.268>the future and past modules improve results more 
			<line pos=307.276,283.574,525.545,296.719>than the forward and backward ones, perhaps be- 
			<line pos=307.276,270.025,525.545,283.170>cause they see a more restricted and challenging 
			<line pos=307.276,256.476,383.967,269.621>view of the input. 
		<box id=8 pos=312.257,206.891,352.104,242.583>
			<line pos=312.257,231.778,335.668,242.583>Model 
			<line pos=312.257,216.854,352.104,227.659>Supervised 
			<line pos=312.257,206.891,330.190,217.696>CVT 
		<box id=9 pos=317.926,186.966,367.483,207.734>
			<line pos=317.926,196.929,361.010,207.734>no fwd/bwd 
			<line pos=317.926,186.966,367.483,197.771>no future/past 
		<box id=10 pos=393.071,186.966,526.825,242.583>
			<line pos=393.071,231.778,523.093,242.583>CCG Chnk NER FGN POS 
			<line pos=506.649,216.854,526.823,227.659>97.59 
			<line pos=393.071,216.854,408.762,227.659>94.8 
			<line pos=393.071,206.891,408.762,217.696>95.6 
			<line pos=506.649,206.891,526.823,217.696>97.66 
			<line pos=506.651,196.929,526.825,207.734>–0.01 
			<line pos=393.073,196.929,408.765,207.734>–0.1 
			<line pos=393.073,186.966,408.765,197.771>–0.3 
			<line pos=506.651,186.966,526.825,197.771>–0.04 
		<box id=11 pos=450.860,186.966,466.553,227.659>
			<line pos=450.860,216.854,466.551,227.659>95.0 
			<line pos=450.860,206.891,466.551,217.696>95.9 
			<line pos=450.862,196.929,466.553,207.734>–0.2 
			<line pos=450.862,186.966,466.553,197.771>–0.4 
		<box id=12 pos=478.754,186.966,494.448,227.659>
			<line pos=478.754,216.854,494.445,227.659>86.0 
			<line pos=478.754,206.891,494.445,217.696>87.3 
			<line pos=478.756,196.929,494.448,207.734>–0.1 
			<line pos=478.756,186.966,494.448,197.771>–0.3 
		<box id=13 pos=421.468,186.966,437.161,227.659>
			<line pos=421.468,216.854,437.159,227.659>95.5 
			<line pos=421.468,206.891,437.159,217.696>97.0 
			<line pos=421.470,196.929,437.161,207.734>–0.2 
			<line pos=421.470,186.966,437.161,197.771>–0.4 
		<box id=14 pos=307.276,146.851,525.547,170.811>
			<line pos=307.276,158.806,525.547,170.811>Table 3: Ablation study on auxiliary prediction mod- 
			<line pos=307.276,146.851,411.305,158.856>ules for sequence tagging. 
		<box id=15 pos=307.276,76.568,525.547,131.485>
			<line pos=307.276,117.216,525.547,131.485>Training Models on Small Datasets. We ex- 
			<line pos=307.276,103.667,525.545,116.812>plore how CVT scales with dataset size by vary- 
			<line pos=307.276,90.117,525.545,103.262>ing the amount of training data the model has ac- 
			<line pos=307.276,76.568,525.545,89.713>cess to. Unsurprisingly, the improvement of CVT 
	<page id=8>
		<box id=0 pos=72.000,572.092,525.547,608.007>
			<line pos=72.000,596.002,525.547,608.007>Figure 5: Left: Dev set performance vs. percent of the training set provided to the model. Right: Dev set perfor- 
			<line pos=72.000,584.047,525.544,596.052>mance vs. model size. The x axis shows the number of hidden units in the LSTM layers; the projection layers and 
			<line pos=72.000,572.092,457.921,584.097>other hidden layers in the network are half that size. Points correspond to the mean of three runs. 
		<box id=1 pos=72.000,119.055,290.273,551.992>
			<line pos=72.000,538.847,290.269,551.992>over purely supervised learning grows larger as the 
			<line pos=72.000,525.298,290.269,538.443>amount of labeled data decreases (see Figure 5, 
			<line pos=72.000,511.748,290.269,524.893>left). Using only 25% of the labeled data, our ap- 
			<line pos=72.000,498.199,290.269,511.344>proach already performs as well or better than a 
			<line pos=72.000,484.650,290.269,497.795>fully supervised model using 100% of the training 
			<line pos=72.000,471.101,290.269,484.246>data, demonstrating that CVT is particularly use- 
			<line pos=72.000,457.552,164.116,470.697>ful on small datasets. 
			<line pos=72.000,437.344,290.272,451.613>Training Larger Models. Most sequence taggers 
			<line pos=72.000,423.795,290.269,436.940>and dependency parsers in prior work use small 
			<line pos=72.000,410.246,290.269,423.391>LSTMs (hidden state sizes of around 300) because 
			<line pos=72.000,396.697,290.269,409.842>larger models yield little to no gains in perfor- 
			<line pos=72.000,383.148,290.269,396.293>mance (Reimers and Gurevych, 2017). We found 
			<line pos=72.000,369.598,290.269,382.743>our own supervised approaches also do not ben- 
			<line pos=72.000,356.049,271.517,369.194>eﬁt greatly from increasing the model size. 
			<line pos=281.182,356.049,290.269,369.194>In 
			<line pos=72.000,342.500,290.269,355.645>contrast, when using CVT accuracy scales better 
			<line pos=72.000,328.951,290.269,342.096>with model size (see Figure 5, right). This ﬁnding 
			<line pos=72.000,315.402,290.269,328.547>suggests the appropriate semi-supervised learning 
			<line pos=72.000,301.853,290.269,314.998>methods may enable the development of larger, 
			<line pos=72.000,288.303,290.269,301.448>more sophisticated models for NLP tasks with lim- 
			<line pos=72.000,274.754,197.738,287.899>ited amounts of labeled data. 
			<line pos=72.000,254.547,290.273,268.816>Generalizable Representations. Lastly, we ex- 
			<line pos=72.000,240.998,290.269,254.143>plore training the CVT+multi-task model on ﬁve 
			<line pos=72.000,227.449,290.269,240.594>tasks, freezing the encoder, and then only training 
			<line pos=72.000,213.899,290.269,227.044>a prediction module on the sixth task. This tests 
			<line pos=72.000,200.350,290.269,213.495>whether the encoder’s representations generalize 
			<line pos=72.000,186.801,290.269,199.946>to a new task not seen during its training. Only 
			<line pos=72.000,173.252,290.269,186.397>training the prediction module is very fast because 
			<line pos=72.000,159.703,290.269,172.848>(1) the encoder (which is by far the slowest part of 
			<line pos=72.000,146.153,290.269,159.298>the model) has to be run over each example only 
			<line pos=72.000,132.604,290.269,145.749>once and (2) we do not back-propagate into the 
			<line pos=72.000,119.055,239.967,132.200>encoder. Results are shown in Table 4. 
		<box id=2 pos=72.000,76.568,290.269,116.812>
			<line pos=82.909,103.667,290.269,116.812>Training only a prediction module on top of 
			<line pos=72.000,90.117,290.269,103.262>multi-task representations works remarkably well, 
			<line pos=72.000,76.568,290.269,89.713>outperforming ELMo embeddings and sometimes 
		<box id=3 pos=310.264,503.889,368.949,549.543>
			<line pos=310.264,538.738,333.675,549.543>Model 
			<line pos=310.264,523.814,350.111,534.619>Supervised 
			<line pos=310.264,513.851,368.949,524.656>CVT-MT frozen 
			<line pos=310.264,503.889,358.817,514.694>ELMo frozen 
		<box id=4 pos=387.303,503.889,528.282,549.543>
			<line pos=387.303,538.738,528.282,549.543>CCG Chnk NER FGN POS Dep. 
			<line pos=437.130,523.814,526.794,534.619>95.0 86.0 97.59 92.9 
			<line pos=387.303,523.814,427.410,534.619>94.8 95.6 
			<line pos=437.130,513.851,526.794,524.656>94.6 83.2 97.66 92.5 
			<line pos=387.303,513.851,427.410,524.656>95.1 96.6 
			<line pos=387.303,503.889,427.410,514.694>94.3 92.2 
			<line pos=437.130,503.889,526.794,514.694>91.3 80.6 97.50 89.4 
		<box id=5 pos=307.276,415.953,525.547,487.733>
			<line pos=307.276,475.728,525.547,487.733>Table 4: Comparison of single-task models on the dev 
			<line pos=307.276,463.773,525.547,475.778>sets. “CVT-MT frozen” means we pretrain a CVT + 
			<line pos=307.276,451.818,525.547,463.823>multi-task model on ﬁve tasks, and then train only the 
			<line pos=307.276,439.863,525.547,451.868>prediction module for the sixth. “ELMo frozen” means 
			<line pos=307.276,427.908,525.547,439.913>we train prediction modules (but no LSTMs) on top of 
			<line pos=307.276,415.953,386.419,427.958>ELMo embeddings. 
		<box id=6 pos=307.276,315.493,525.545,396.384>
			<line pos=307.276,383.239,525.545,396.384>even a vanilla supervised model, showing the 
			<line pos=307.276,369.690,525.545,382.835>multi-task model is building up effective repre- 
			<line pos=307.276,356.140,525.545,369.285>sentations for language. In particular, the repre- 
			<line pos=307.276,342.591,525.545,355.736>sentations could be used like skip-thought vectors 
			<line pos=307.276,329.042,525.545,342.187>(Kiros et al., 2015) to quickly train models on new 
			<line pos=307.276,315.493,493.953,328.638>tasks without slow representation learning. 
		<box id=7 pos=307.276,291.715,396.366,307.269>
			<line pos=307.276,291.715,396.366,307.269>5 Related Work 
		<box id=8 pos=307.276,76.568,525.545,280.526>
			<line pos=307.276,266.257,525.543,280.526>Unsupervised Representation Learning. Early 
			<line pos=307.276,252.708,525.545,265.853>approaches to deep semi-supervised learning pre- 
			<line pos=307.276,239.159,525.545,252.304>train neural models on unlabeled data, which has 
			<line pos=307.276,225.609,525.545,238.754>been successful for applications in computer vi- 
			<line pos=307.276,212.060,525.545,225.205>sion (Jarrett et al., 2009; LeCun et al., 2010) and 
			<line pos=307.276,198.511,525.545,211.656>NLP. Particularly noteworthy for NLP are al- 
			<line pos=307.276,184.962,525.545,198.107>gorithms for learning effective word embeddings 
			<line pos=307.276,171.413,525.545,184.558>(Collobert et al., 2011; Mikolov et al., 2013; Pen- 
			<line pos=307.276,157.863,525.545,171.008>nington et al., 2014) and language model pretrain- 
			<line pos=307.276,144.314,525.545,157.459>ing (Dai and Le, 2015; Ramachandran et al., 2017; 
			<line pos=307.276,130.765,525.545,143.910>Peters et al., 2018; Howard and Ruder, 2018; Rad- 
			<line pos=307.276,117.216,525.545,130.361>ford et al., 2018). Pre-training on other tasks 
			<line pos=307.276,103.667,525.545,116.812>such as machine translation has also been stud- 
			<line pos=307.276,90.117,525.545,103.262>ied (McCann et al., 2017). Other approaches train 
			<line pos=307.276,76.568,525.545,89.713>“thought vectors” representing sentences through 
	<page id=9>
		<box id=0 pos=72.000,752.217,290.269,778.912>
			<line pos=72.000,765.767,290.269,778.912>unsupervised (Kiros et al., 2015; Hill et al., 2016) 
			<line pos=72.000,752.217,271.658,765.362>or supervised (Conneau et al., 2017) learning. 
		<box id=1 pos=72.000,499.903,290.269,744.508>
			<line pos=72.000,730.239,290.264,744.508>Self-Training. One of the earliest approaches 
			<line pos=72.000,716.690,290.269,729.835>to semi-supervised learning is self-training (Scud- 
			<line pos=72.000,703.141,290.269,716.286>der, 1965), which has been successfully applied 
			<line pos=72.000,689.591,290.269,702.736>to NLP tasks such as word-sense disambiguation 
			<line pos=72.000,676.042,290.269,689.187>(Yarowsky, 1995) and parsing (McClosky et al., 
			<line pos=72.000,662.493,100.178,675.638>2006). 
			<line pos=108.644,662.493,290.269,675.638>In each round of training, the classiﬁer, 
			<line pos=72.000,648.944,290.269,662.089>acting as a “teacher,” labels some of the unlabeled 
			<line pos=72.000,635.395,290.269,648.540>data and adds it to the training set. Then, acting as 
			<line pos=72.000,621.845,290.269,634.990>a “student,” it is retrained on the new training set. 
			<line pos=72.000,608.296,290.269,621.441>Many recent approaches (including the consisten- 
			<line pos=72.000,594.747,290.269,607.892>tency regularization methods discussed below and 
			<line pos=72.000,581.198,290.269,594.343>our own method) train the student with soft tar- 
			<line pos=72.000,567.649,290.269,580.794>gets from the teacher’s output distribution rather 
			<line pos=72.000,554.099,290.269,567.244>than a hard label, making the procedure more akin 
			<line pos=72.000,540.550,290.269,553.695>to knowledge distillation (Hinton et al., 2015). It 
			<line pos=72.000,527.001,290.269,540.146>is also possible to use multiple models or predic- 
			<line pos=72.000,513.452,290.269,526.597>tion modules for the teacher, such as in tri-training 
			<line pos=72.000,499.903,268.953,513.048>(Zhou and Li, 2005; Ruder and Plank, 2018). 
		<box id=2 pos=72.000,247.588,290.271,492.193>
			<line pos=72.000,477.924,290.271,492.193>Consistency Regularization. Recent works add 
			<line pos=72.000,464.375,290.269,477.520>noise (e.g., drawn from a Gaussian distribution) 
			<line pos=72.000,450.826,290.269,463.971>or apply stochastic transformations (e.g., horizon- 
			<line pos=72.000,437.277,290.269,450.422>tally ﬂipping an image) to the student’s inputs. 
			<line pos=72.000,423.727,290.269,436.872>This trains the model to give consistent predictions 
			<line pos=72.000,410.178,290.269,423.323>to nearby data points, encouraging distributional 
			<line pos=72.000,396.629,290.269,409.774>smoothness in the model. Consistency regular- 
			<line pos=72.000,383.080,290.269,396.225>ization has been very successful for computer vi- 
			<line pos=72.000,369.531,290.269,382.676>sion applications (Bachman et al., 2014; Laine and 
			<line pos=72.000,355.982,290.269,369.127>Aila, 2017; Tarvainen and Valpola, 2017). How- 
			<line pos=72.000,342.432,290.269,355.577>ever, stochastic input alterations are more difﬁcult 
			<line pos=72.000,328.883,290.269,342.028>to apply to discrete data like text, making consis- 
			<line pos=72.000,315.334,290.269,328.479>tency regularization less used for natural language 
			<line pos=72.000,301.785,290.269,314.930>processing. One solution is to add noise to the 
			<line pos=72.000,288.236,290.269,301.381>model’s word embeddings (Miyato et al., 2017a); 
			<line pos=72.000,274.686,290.269,287.831>we compare against this approach in our experi- 
			<line pos=72.000,261.137,290.269,274.282>ments. CVT is easily applicable to text because it 
			<line pos=72.000,247.588,274.713,260.733>does not require changing the student’s inputs. 
		<box id=3 pos=72.000,76.568,290.269,239.879>
			<line pos=72.000,225.610,290.265,239.879>Multi-View Learning. Multi-view learning on 
			<line pos=72.000,212.060,290.269,225.205>data where features can be separated into distinct 
			<line pos=72.000,198.511,290.269,211.656>subsets has been well studied (Xu et al., 2013). 
			<line pos=72.000,184.962,290.269,198.107>Particularly relevant are co-training (Blum and 
			<line pos=72.000,171.413,290.269,184.558>Mitchell, 1998) and co-regularization (Sindhwani 
			<line pos=72.000,157.864,290.269,171.009>and Belkin, 2005), which trains two models with 
			<line pos=72.000,144.314,290.269,157.459>disjoint views of the input. On unlabeled data, 
			<line pos=72.000,130.765,290.269,143.910>each one acts as a “teacher” for the other model. 
			<line pos=72.000,117.216,290.269,130.361>In contrast to these methods, our approach trains 
			<line pos=72.000,103.667,290.269,116.812>a single uniﬁed model where auxiliary prediction 
			<line pos=72.000,90.118,290.269,103.263>modules see different, but not necessarily indepen- 
			<line pos=72.000,76.568,174.447,89.713>dent views of the input. 
		<box id=4 pos=307.276,394.157,525.549,780.036>
			<line pos=307.276,765.767,525.549,780.036>Self Supervision. Self-supervised learning meth- 
			<line pos=307.276,752.218,525.545,765.363>ods train auxiliary prediction modules on tasks 
			<line pos=307.276,738.668,525.545,751.813>where performance can be measured without 
			<line pos=307.276,725.119,525.545,738.264>human-provided labels. Recent work has jointly 
			<line pos=307.276,711.570,525.545,724.715>trained image classiﬁers with tasks like relative 
			<line pos=307.276,698.021,525.545,711.166>position and colorization (Doersch and Zisserman, 
			<line pos=307.276,684.472,525.545,697.617>2017), sequence taggers with language modeling 
			<line pos=307.276,670.923,525.545,684.068>(Rei, 2017), and reinforcement learning agents 
			<line pos=307.276,657.373,525.545,670.518>with predicting changes in the environment (Jader- 
			<line pos=307.276,643.824,525.545,656.969>berg et al., 2017). Unlike these approaches, our 
			<line pos=307.276,630.275,525.545,643.420>auxiliary losses are based on self-labeling, not la- 
			<line pos=307.276,616.726,521.803,629.871>bels deterministically constructed from the input. 
			<line pos=307.276,597.395,525.543,611.664>Multi-Task Learning. There has been extensive 
			<line pos=307.276,583.846,525.545,596.991>prior work on multi-task learning (Caruana, 1997; 
			<line pos=307.276,570.296,525.545,583.441>Ruder, 2017). For NLP, most work has focused 
			<line pos=307.276,556.747,525.545,569.892>on a small number of closely related tasks (Lu- 
			<line pos=307.276,543.198,525.545,556.343>ong et al., 2016; Zhang and Weiss, 2016; Søgaard 
			<line pos=307.276,529.649,525.545,542.794>and Goldberg, 2016; Peng et al., 2017). Many- 
			<line pos=307.276,516.100,525.545,529.245>task systems are less commonly developed. Col- 
			<line pos=307.276,502.550,525.545,515.695>lobert and Weston (2008) propose a many-task 
			<line pos=307.276,489.001,525.545,502.146>system sharing word embeddings between the 
			<line pos=307.276,475.452,525.545,488.597>tasks, Hashimoto et al. (2017) train a many-task 
			<line pos=307.276,461.903,525.545,475.048>model where the tasks are arranged hierarchically 
			<line pos=307.276,448.354,525.545,461.499>according to their linguistic level, and Subrama- 
			<line pos=307.276,434.804,525.545,447.949>nian et al. (2018) train a shared-encoder many-task 
			<line pos=307.276,421.255,525.545,434.400>model for the purpose of learning better sentence 
			<line pos=307.276,407.706,525.545,420.851>representations for use in downstream tasks, not 
			<line pos=307.276,394.157,492.862,407.302>for improving results on the original tasks. 
		<box id=5 pos=307.276,366.913,382.343,382.467>
			<line pos=307.276,366.913,382.343,382.467>6 Conclusion 
		<box id=6 pos=307.276,235.363,525.545,356.901>
			<line pos=307.276,343.756,525.545,356.901>We propose Cross-View Training, a new method 
			<line pos=307.276,330.207,525.545,343.352>for semi-supervised learning. Our approach al- 
			<line pos=307.276,316.658,525.545,329.803>lows models to effectively leverage their own pre- 
			<line pos=307.276,303.109,525.545,316.254>dictions on unlabeled data, training them to pro- 
			<line pos=307.276,289.560,525.545,302.705>duce effective representations that yield accurate 
			<line pos=307.276,276.010,525.545,289.155>predictions even when some of the input is not 
			<line pos=307.276,262.461,525.545,275.606>available. We achieve excellent results across 
			<line pos=307.276,248.912,525.545,262.057>seven NLP tasks, especially when CVT is com- 
			<line pos=307.276,235.363,441.818,248.508>bined with multi-task learning. 
		<box id=7 pos=307.276,208.118,406.110,223.672>
			<line pos=307.276,208.118,406.110,223.672>Acknowledgements 
		<box id=8 pos=307.276,90.118,525.545,198.107>
			<line pos=307.276,184.962,525.545,198.107>We thank Abi See, Christopher Clark, He He, 
			<line pos=307.276,171.413,525.545,184.558>Peng Qi, Reid Pryzant, Yuaho Zhang, and the 
			<line pos=307.276,157.864,525.545,171.009>anonymous reviewers for their thoughtful com- 
			<line pos=307.276,144.315,525.545,157.460>ments and suggestions. We thank Takeru Miyato 
			<line pos=307.276,130.765,525.545,143.910>for help with his virtual adversarial training code 
			<line pos=307.276,117.216,525.545,130.361>and Emma Strubell for answering our questions 
			<line pos=307.276,103.667,525.545,116.812>about OntoNotes NER. Kevin is supported by a 
			<line pos=307.276,90.118,415.189,103.263>Google PhD Fellowship. 
	<page id=10>
		<box id=0 pos=72.000,748.605,290.271,781.178>
			<line pos=72.000,765.624,127.544,781.178>References 
			<line pos=72.000,748.605,290.271,760.610>Philip Bachman, Ouais Alsharif, and Doina Precup. 
		<box id=1 pos=82.909,737.646,281.005,749.771>
			<line pos=82.909,737.646,281.005,749.771>2014. Learning with pseudo-ensembles. In NIPS ~ . 
		<box id=2 pos=72.000,696.510,290.271,730.433>
			<line pos=72.000,718.428,290.271,730.433>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- 
			<line pos=82.909,707.469,290.271,719.474>gio. 2015. Neural machine translation by jointly 
			<line pos=82.909,696.510,242.280,708.635>learning to align and translate. In ICLR ~ . 
		<box id=3 pos=72.000,655.374,290.271,689.297>
			<line pos=72.000,677.292,290.271,689.297>Avrim Blum and Tom Mitchell. 1998. Combining la- 
			<line pos=82.909,666.333,290.268,678.458>beled and unlabeled data with co-training. In COLT ~ . 
			<line pos=82.909,655.374,107.696,667.379>ACM. 
		<box id=4 pos=72.000,636.156,290.268,648.281>
			<line pos=72.000,636.156,290.268,648.281>Rich Caruana. 1997. Multitask learning. Machine 
		<box id=5 pos=82.909,625.197,164.553,637.322>
			<line pos=82.909,625.197,164.553,637.322>Learning ~ , 28:41–75. 
		<box id=6 pos=72.000,562.144,290.271,618.034>
			<line pos=72.000,605.979,290.266,618.034>Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa 
			<line pos=82.909,595.020,290.271,607.025>Bentivogli, Roldano Cattoni, and Marcello Federico. 
			<line pos=82.909,584.062,290.267,596.187>2015. The IWSLT 2015 evaluation campaign. In In- 
			<line pos=82.909,573.332,290.271,585.228>ternational Workshop on Spoken Language Transla- 
			<line pos=82.909,562.144,100.902,574.269>tion ~ . 
		<box id=7 pos=72.000,499.090,290.271,554.931>
			<line pos=72.000,542.926,290.271,554.931>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, 
			<line pos=82.909,531.967,290.271,543.972>Thorsten Brants, Phillipp Koehn, and Tony Robin- 
			<line pos=82.909,521.008,290.271,533.013>son. 2014. One billion word benchmark for mea- 
			<line pos=82.909,510.049,290.271,522.054>suring progress in statistical language modeling. In 
			<line pos=82.909,499.090,150.157,511.215>INTERSPEECH ~ . 
		<box id=8 pos=72.000,446.995,290.271,491.877>
			<line pos=72.000,479.872,290.271,491.877>Jason PC Chiu and Eric Nichols. 2016. Named entity 
			<line pos=82.909,468.913,290.268,481.038>recognition with bidirectional LSTM-CNNs. Trans- 
			<line pos=82.909,458.183,290.271,470.079>actions of the Association for Computational Lin- 
			<line pos=82.909,446.995,115.845,459.120>guistics ~ . 
		<box id=9 pos=72.000,427.777,290.271,439.782>
			<line pos=72.000,427.777,290.271,439.782>Do Kook Choe and Eugene Charniak. 2016. Parsing as 
		<box id=10 pos=82.909,416.818,211.018,428.943>
			<line pos=82.909,416.818,211.018,428.943>language modeling. In EMNLP ~ . 
		<box id=11 pos=307.276,733.094,525.547,778.025>
			<line pos=307.276,765.971,525.547,778.025>Alex Graves and J¨urgen Schmidhuber. 2005. Frame- 
			<line pos=318.185,755.012,525.547,767.017>wise phoneme classiﬁcation with bidirectional 
			<line pos=318.185,744.053,525.543,756.178>LSTM and other neural network architectures. Neu- 
			<line pos=318.185,733.094,436.481,745.219>ral Networks ~ , 18(5):602–610. 
		<box id=12 pos=307.276,677.487,525.547,722.369>
			<line pos=307.276,710.364,525.547,722.369>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu- 
			<line pos=318.185,699.405,525.547,711.410>ruoka, and Richard Socher. 2017. A joint many-task 
			<line pos=318.185,688.446,525.547,700.451>model: Growing a neural network for multiple nlp 
			<line pos=318.185,677.487,390.125,689.612>tasks. In EMNLP ~ . 
		<box id=13 pos=307.276,632.839,525.547,666.762>
			<line pos=307.276,654.757,525.547,666.762>Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. 
			<line pos=318.185,643.798,525.547,655.803>Learning distributed representations of sentences 
			<line pos=318.185,632.839,471.081,644.964>from unlabelled data. In HLT-NAACL ~ . 
		<box id=14 pos=307.276,588.192,525.547,622.114>
			<line pos=307.276,610.109,525.547,622.114>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. 
			<line pos=318.185,599.150,525.544,611.275>Distilling the knowledge in a neural network. arXiv 
			<line pos=318.185,588.192,428.232,600.317>preprint arXiv:1503.02531 ~ . 
		<box id=15 pos=307.276,521.626,525.547,577.467>
			<line pos=307.276,565.462,525.547,577.467>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, 
			<line pos=318.185,554.503,525.547,566.508>Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. 
			<line pos=318.185,543.544,525.547,555.549>Improving neural networks by preventing co- 
			<line pos=465.368,532.814,525.542,544.710>arXiv preprint 
			<line pos=318.185,532.585,451.345,544.590>adaptation of feature detectors. 
			<line pos=318.185,521.626,388.471,533.751>arXiv:1207.0580 ~ . 
		<box id=16 pos=307.276,487.937,525.544,510.951>
			<line pos=307.276,498.896,525.542,510.951>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. 
			<line pos=440.176,487.937,525.544,500.062>Neural computation ~ , 
		<box id=17 pos=318.185,476.978,426.837,499.942>
			<line pos=318.185,487.937,426.837,499.942>Long short-term memory. 
			<line pos=318.185,476.978,384.875,488.983>9(8):1735–1780. 
		<box id=18 pos=307.276,421.371,525.547,466.253>
			<line pos=307.276,454.248,525.547,466.253>Julia Hockenmaier and Mark Steedman. 2007. CCG- 
			<line pos=318.185,443.289,525.547,455.294>bank: a corpus of CCG derivations and dependency 
			<line pos=318.185,432.330,525.544,444.455>structures extracted from the Penn treebank. Com- 
			<line pos=318.185,421.371,472.067,433.496>putational Linguistics ~ , 33(3):355–396. 
		<box id=19 pos=72.000,375.682,290.271,409.605>
			<line pos=72.000,397.600,290.271,409.605>Ronan Collobert and Jason Weston. 2008. A uniﬁed 
			<line pos=82.909,386.641,290.271,398.646>architecture for natural language processing: deep 
			<line pos=82.909,375.682,284.800,387.807>neural networks with multitask learning. In ICML ~ . 
		<box id=20 pos=307.276,376.724,525.547,410.646>
			<line pos=307.276,398.641,525.547,410.646>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance 
			<line pos=318.185,387.682,525.547,399.687>Ramshaw, and Ralph Weischedel. 2006. Ontonotes: 
			<line pos=318.185,376.724,456.157,388.849>the 90% solution. In HLT-NAACL ~ . 
		<box id=21 pos=72.000,323.587,290.271,368.519>
			<line pos=72.000,356.464,290.268,368.519>Ronan Collobert, Jason Weston, L´eon Bottou, Michael 
			<line pos=82.909,345.505,290.271,357.510>Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 
			<line pos=82.909,334.546,290.271,346.551>2011. Natural language processing (almost) from 
			<line pos=82.909,323.587,275.944,335.712>scratch. Journal of Machine Learning Research ~ . 
		<box id=22 pos=72.000,271.493,290.271,316.424>
			<line pos=72.000,304.369,290.268,316.424>Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc 
			<line pos=82.909,293.410,290.271,305.415>Barrault, and Antoine Bordes. 2017. Supervised 
			<line pos=82.909,282.452,290.271,294.457>learning of universal sentence representations from 
			<line pos=82.909,271.493,259.685,283.618>natural language inference data. In EMNLP ~ . 
		<box id=23 pos=72.000,252.275,290.271,264.280>
			<line pos=72.000,252.275,290.271,264.280>Andrew M Dai and Quoc V Le. 2015. Semi-supervised 
		<box id=24 pos=82.909,241.316,194.948,253.441>
			<line pos=82.909,241.316,194.948,253.441>sequence learning. In NIPS ~ . 
		<box id=25 pos=72.000,200.180,290.271,234.103>
			<line pos=72.000,222.098,290.271,234.103>Zihang Dai, Zhilin Yang, Fan Yang, William W Co- 
			<line pos=82.909,211.139,290.271,223.144>hen, and Ruslan Salakhutdinov. 2017. Good semi- 
			<line pos=82.909,200.180,290.268,212.305>supervised learning that requires a bad gan. In NIPS ~ . 
		<box id=26 pos=72.000,159.044,290.271,192.967>
			<line pos=72.000,180.962,290.271,192.967>Carl Doersch and Andrew Zisserman. 2017. Multi- 
			<line pos=82.909,170.003,290.269,182.128>task self-supervised visual learning. arXiv preprint 
			<line pos=82.909,159.044,158.177,171.169>arXiv:1708.07860 ~ . 
		<box id=27 pos=72.000,117.908,290.271,151.831>
			<line pos=72.000,139.826,290.271,151.831>Timothy Dozat and Christopher D. Manning. 2017. 
			<line pos=82.909,128.867,290.271,140.872>Deep biafﬁne attention for neural dependency pars- 
			<line pos=82.909,117.908,136.588,130.033>ing. In ICLR ~ . 
		<box id=28 pos=72.000,76.772,290.271,110.695>
			<line pos=72.000,98.690,290.271,110.695>Tommaso Furlanello, Zachary C Lipton, Michael 
			<line pos=82.909,87.731,290.271,99.736>Tschannen, Laurent Itti, and Anima Anandkumar. 
			<line pos=82.909,76.772,262.315,88.897>2018. Born again neural networks. In ICML ~ . 
		<box id=29 pos=307.276,332.076,525.547,365.999>
			<line pos=307.276,353.994,525.547,365.999>Jeremy Howard and Sebastian Ruder. 2018. Universal 
			<line pos=318.185,343.035,525.547,355.040>language model ﬁne-tuning for text classiﬁcation. In 
			<line pos=318.185,332.076,338.648,344.201>ACL ~ . 
		<box id=30 pos=307.276,276.469,525.547,321.351>
			<line pos=307.276,309.346,525.547,321.351>Max Jaderberg, Volodymyr Mnih, Wojciech Marian 
			<line pos=318.185,298.387,525.547,310.392>Czarnecki, Tom Schaul, Joel Z Leibo, David Sil- 
			<line pos=318.185,287.428,525.547,299.433>ver, and Koray Kavukcuoglu. 2017. Reinforcement 
			<line pos=318.185,276.469,525.544,288.594>learning with unsupervised auxiliary tasks. In ICLR ~ . 
		<box id=31 pos=307.276,220.862,525.547,265.744>
			<line pos=307.276,253.739,525.547,265.744>Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. 
			<line pos=318.185,242.780,525.547,254.785>2009. What is the best multi-stage architecture for 
			<line pos=318.185,231.821,525.547,243.946>object recognition? In IEEE Conference on Com- 
			<line pos=318.185,220.862,368.924,232.987>puter Vision ~ . 
		<box id=32 pos=307.276,198.132,525.547,210.137>
			<line pos=307.276,198.132,525.547,210.137>Yoon Kim and Alexander M. Rush. 2016. Sequence- 
		<box id=33 pos=318.185,187.173,480.246,199.298>
			<line pos=318.185,187.173,480.246,199.298>level knowledge distillation. In EMNLP ~ . 
		<box id=34 pos=307.276,76.772,525.547,176.448>
			<line pos=307.276,164.443,525.547,176.448>James Kirkpatrick, Razvan Pascanu, Neil C. Rabi- 
			<line pos=318.185,153.485,525.547,165.490>nowitz, Joel Veness, Guillaume Desjardins, An- 
			<line pos=318.185,142.526,525.547,154.531>drei A. Rusu, Kieran Milan, John Quan, Tiago Ra- 
			<line pos=318.185,131.567,525.547,143.572>malho, Agnieszka Grabska-Barwinska, Demis Has- 
			<line pos=318.185,120.608,525.547,132.613>sabis, Claudia Clopath, Dharshan Kumaran, and 
			<line pos=318.185,109.649,525.547,121.654>Raia Hadsell. 2017. Overcoming catastrophic for- 
			<line pos=318.185,98.690,525.547,110.815>getting in neural networks. Proceedings of the Na- 
			<line pos=318.185,87.960,525.547,99.856>tional Academy of Sciences of the United States of 
			<line pos=318.185,76.772,434.408,88.897>America ~ , 114 13:3521–3526. 
	<page id=11>
		<box id=0 pos=72.000,722.135,290.271,777.976>
			<line pos=72.000,765.971,290.271,777.976>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, 
			<line pos=82.909,755.012,290.271,767.017>Richard Zemel, Raquel Urtasun, Antonio Torralba, 
			<line pos=82.909,744.053,274.041,756.058>and Sanja Fidler. 2015. Skip-thought vectors. 
			<line pos=281.972,744.053,290.271,756.058>In 
			<line pos=82.909,733.094,290.268,745.219>Advances in neural information processing systems ~ , 
			<line pos=82.909,722.135,155.407,734.140>pages 3294–3302. 
		<box id=1 pos=72.000,702.348,290.271,714.353>
			<line pos=72.000,702.348,290.271,714.353>Alex Krizhnevsky and Geoffrey Hinton. 2009. Learn- 
		<box id=2 pos=82.909,691.389,275.347,703.394>
			<line pos=82.909,691.389,275.347,703.394>ing multiple layers of features from tiny images. 
		<box id=3 pos=72.000,638.725,290.271,683.607>
			<line pos=72.000,671.602,290.271,683.607>Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng 
			<line pos=82.909,660.643,290.271,672.648>Kong, Chris Dyer, Graham Neubig, and Noah A. 
			<line pos=82.909,649.684,290.271,661.689>Smith. 2017. What do recurrent neural network 
			<line pos=82.909,638.725,244.233,650.850>grammars learn about syntax? In EACL ~ . 
		<box id=4 pos=72.000,618.938,290.271,630.943>
			<line pos=72.000,618.938,290.271,630.943>Samuli Laine and Timo Aila. 2017. Temporal ensem- 
		<box id=5 pos=82.909,607.979,260.820,620.104>
			<line pos=82.909,607.979,260.820,620.104>bling for semi-supervised learning. In ICLR ~ . 
		<box id=6 pos=72.000,555.315,290.271,600.197>
			<line pos=72.000,588.192,290.271,600.197>Guillaume Lample, Miguel Ballesteros, Sandeep Sub- 
			<line pos=82.909,577.233,290.271,589.238>ramanian, Kazuya Kawakami, and Chris Dyer. 2016. 
			<line pos=82.909,566.274,290.271,578.279>Neural architectures for named entity recognition. 
			<line pos=82.909,555.315,114.162,567.440>In ACL ~ . 
		<box id=7 pos=72.000,513.610,290.271,547.583>
			<line pos=72.000,535.528,290.263,547.583>Yann LeCun, Koray Kavukcuoglu, and Cl´ement Fara- 
			<line pos=82.909,524.569,290.271,536.574>bet. 2010. Convolutional networks and applications 
			<line pos=82.909,513.610,189.439,525.735>in vision. In ISCAS ~ . IEEE. 
		<box id=8 pos=72.000,493.823,290.271,505.828>
			<line pos=72.000,493.823,290.271,505.828>Mike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016. 
		<box id=9 pos=82.909,482.864,234.719,494.989>
			<line pos=82.909,482.864,234.719,494.989>LSTM CCG parsing. In HLT-NAACL ~ . 
		<box id=10 pos=72.000,463.077,290.271,475.082>
			<line pos=72.000,463.077,290.271,475.082>Zhizhong Li and Derek Hoiem. 2016. Learning with- 
		<box id=11 pos=82.909,452.118,182.067,464.243>
			<line pos=82.909,452.118,182.067,464.243>out forgetting. In ECCV ~ . 
		<box id=12 pos=72.000,432.331,240.776,444.336>
			<line pos=72.000,432.331,240.776,444.336>Jiangming Liu and Yue Zhang. 2017. 
		<box id=13 pos=82.909,421.372,256.168,433.497>
			<line pos=82.909,421.372,256.168,433.497>transition-based constituent parsing. TACL ~ . 
		<box id=14 pos=257.633,432.331,290.271,444.336>
			<line pos=257.633,432.331,290.271,444.336>In-order 
		<box id=15 pos=72.000,368.708,290.271,413.589>
			<line pos=72.000,401.584,290.271,413.589>Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan 
			<line pos=82.909,390.625,290.271,402.630>Gui, Jian Peng, and Jiawei Han. 2017. Empower 
			<line pos=82.909,379.667,290.271,391.672>sequence labeling with task-aware neural language 
			<line pos=82.909,368.708,248.567,380.833>model. arXiv preprint arXiv:1709.04109 ~ . 
		<box id=16 pos=72.000,327.003,290.271,360.925>
			<line pos=72.000,348.920,290.271,360.925>Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. 
			<line pos=82.909,337.962,290.271,349.967>2017. Neural machine translation (seq2seq) tutorial. 
			<line pos=82.909,327.003,219.785,339.128>https://github.com/tensorﬂow/nmt ~ . 
		<box id=17 pos=72.000,285.298,290.271,319.220>
			<line pos=72.000,307.215,290.271,319.220>Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol 
			<line pos=82.909,296.257,290.271,308.262>Vinyals, and Lukasz Kaiser. 2016. Multi-task se- 
			<line pos=82.909,285.298,236.452,297.423>quence to sequence learning. In ICLR ~ . 
		<box id=18 pos=72.000,243.593,290.271,277.515>
			<line pos=72.000,265.510,290.271,277.515>Minh-Thang Luong and Christopher D. Manning. 
			<line pos=82.909,254.551,290.271,266.556>2015. Stanford neural machine translation systems 
			<line pos=82.909,243.593,246.684,255.718>for spoken language domains. In IWSLT ~ . 
		<box id=19 pos=72.000,201.888,290.271,235.810>
			<line pos=72.000,223.805,290.271,235.810>Thang Luong, Hieu Pham, and Christopher D. Man- 
			<line pos=82.909,212.846,290.271,224.851>ning. 2015. Effective approaches to attention-based 
			<line pos=82.909,201.888,240.616,214.013>neural machine translation. In EMNLP ~ . 
		<box id=20 pos=72.000,182.100,232.378,194.105>
			<line pos=72.000,182.100,232.378,194.105>Xuezhe Ma and Eduard Hovy. 2016. 
		<box id=21 pos=82.909,160.182,290.271,194.105>
			<line pos=245.449,182.100,290.271,194.105>End-to-end 
			<line pos=82.909,171.141,290.271,183.146>sequence labeling via bi-directional LSTM-CNN- 
			<line pos=82.909,160.182,139.068,172.307>CRF. In ACL ~ . 
		<box id=22 pos=72.000,129.436,290.271,152.400>
			<line pos=72.000,140.395,290.271,152.400>Xuezhe Ma and Eduard Hovy. 2017. Neural proba- 
			<line pos=281.972,129.436,290.271,141.441>In 
		<box id=23 pos=82.909,118.477,272.348,141.441>
			<line pos=82.909,129.436,272.348,141.441>bilistic model for non-projective mst parsing. 
			<line pos=82.909,118.477,118.057,130.602>IJCNLP ~ . 
		<box id=24 pos=72.000,76.772,290.271,110.695>
			<line pos=72.000,98.690,290.271,110.695>Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng, 
			<line pos=82.909,87.731,290.271,99.736>Graham Neubig, and Eduard Hovy. 2018. Stack- 
			<line pos=82.909,76.772,283.227,88.897>pointer networks for dependency parsing. In ACL ~ . 
		<box id=25 pos=307.276,733.094,525.547,777.976>
			<line pos=307.276,765.971,525.547,777.976>Mitchell P Marcus, Mary Ann Marcinkiewicz, and 
			<line pos=318.185,755.012,525.547,767.017>Beatrice Santorini. 1993. Building a large annotated 
			<line pos=318.185,744.053,525.543,756.178>corpus of english: The Penn treebank. Computa- 
			<line pos=318.185,733.094,451.583,745.219>tional linguistics ~ , 19(2):313–330. 
		<box id=26 pos=307.276,690.195,525.547,724.118>
			<line pos=307.276,712.113,525.547,724.118>Bryan McCann, James Bradbury, Caiming Xiong, and 
			<line pos=318.185,701.154,525.547,713.159>Richard Socher. 2017. Learned in translation: Con- 
			<line pos=318.185,690.195,456.117,702.320>textualized word vectors. In NIPS ~ . 
		<box id=27 pos=307.276,658.254,525.547,681.218>
			<line pos=307.276,669.213,525.547,681.218>David McClosky, Eugene Charniak, and Mark John- 
			<line pos=517.248,658.254,525.547,670.259>In 
		<box id=28 pos=318.185,647.295,508.989,670.259>
			<line pos=318.185,658.254,508.989,670.259>son. 2006. Effective self-training for parsing. 
			<line pos=318.185,647.295,338.648,659.420>ACL ~ . 
		<box id=29 pos=307.276,593.437,525.547,638.319>
			<line pos=307.276,626.314,525.547,638.319>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. 
			<line pos=318.185,615.355,525.547,627.360>Corrado, and Jeffrey Dean. 2013. Distributed repre- 
			<line pos=318.185,604.396,525.547,616.401>sentations of words and phrases and their composi- 
			<line pos=318.185,593.437,391.141,605.562>tionality. In NIPS ~ . 
		<box id=30 pos=307.276,550.538,525.547,584.461>
			<line pos=307.276,572.456,525.547,584.461>Takeru Miyato, Andrew M Dai, and Ian Goodfel- 
			<line pos=318.185,561.497,525.547,573.502>low. 2017a. Adversarial training methods for semi- 
			<line pos=318.185,550.538,473.541,562.663>supervised text classiﬁcation. In ICLR ~ . 
		<box id=31 pos=307.276,485.721,525.547,541.561>
			<line pos=307.276,529.556,525.547,541.561>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, 
			<line pos=318.185,518.597,525.547,530.602>and Shin Ishii. 2017b. Virtual adversarial train- 
			<line pos=318.185,507.638,333.687,519.643>ing: 
			<line pos=482.937,507.638,525.547,519.643>supervised 
			<line pos=464.175,496.909,525.545,508.805>arXiv preprint 
			<line pos=318.185,496.680,446.344,508.685>and semi-supervised learning. 
			<line pos=318.185,485.721,393.453,497.846>arXiv:1704.03976 ~ . 
		<box id=32 pos=348.471,507.638,474.598,519.643>
			<line pos=348.471,507.638,474.598,519.643>a regularization method for 
		<box id=33 pos=307.276,431.862,525.547,476.744>
			<line pos=307.276,464.739,525.547,476.744>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, 
			<line pos=318.185,453.780,525.547,465.785>Ken Nakae, and Shin Ishii. 2016. Distributional 
			<line pos=318.185,442.821,464.456,454.826>smoothing with virtual adversarial 
			<line pos=517.248,442.821,525.547,454.826>In 
			<line pos=318.185,431.862,342.265,443.987>ICLR ~ . 
		<box id=34 pos=470.095,442.821,503.579,454.826>
			<line pos=470.095,442.821,503.579,454.826>training. 
		<box id=35 pos=307.276,378.004,525.547,422.886>
			<line pos=307.276,410.881,525.547,422.886>Sungrae Park, Jun-Keon Park, Su-Jin Shin, and Il- 
			<line pos=318.185,399.922,525.547,411.927>Chul Moon. 2017. Adversarial dropout for super- 
			<line pos=318.185,388.963,525.544,401.088>vised and semi-supervised learning. arXiv preprint 
			<line pos=318.185,378.004,393.453,390.129>arXiv:1707.03631 ~ . 
		<box id=36 pos=307.276,335.105,525.547,369.028>
			<line pos=307.276,357.023,525.547,369.028>Hao Peng, Sam Thomson, and Noah A. Smith. 2017. 
			<line pos=318.185,346.064,525.547,358.069>Deep multitask learning for semantic dependency 
			<line pos=318.185,335.105,384.845,347.230>parsing. In ACL ~ . 
		<box id=37 pos=307.276,292.205,525.547,326.128>
			<line pos=307.276,314.123,525.547,326.128>Jeffrey Pennington, Richard Socher, and Christopher 
			<line pos=318.185,303.164,525.547,315.169>Manning. 2014. Glove: Global vectors for word 
			<line pos=318.185,292.205,426.638,304.330>representation. In EMNLP ~ . 
		<box id=38 pos=307.276,271.224,451.076,283.229>
			<line pos=307.276,271.224,451.076,283.229>Gabriel Pereyra, George Tucker, 
		<box id=39 pos=318.185,238.347,525.547,283.229>
			<line pos=459.255,271.224,525.547,283.229>Jan Chorowski, 
			<line pos=318.185,260.265,525.547,272.270>Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular- 
			<line pos=318.185,249.306,525.547,261.311>izing neural networks by penalizing conﬁdent output 
			<line pos=318.185,238.347,408.755,250.472>distributions. In ICLR ~ . 
		<box id=40 pos=307.276,184.489,525.547,229.371>
			<line pos=307.276,217.366,525.547,229.371>Matthew E Peters, Waleed Ammar, Chandra Bhagavat- 
			<line pos=318.185,206.407,525.547,218.412>ula, and Russell Power. 2017. Semi-supervised se- 
			<line pos=318.185,195.448,525.547,207.453>quence tagging with bidirectional language models. 
			<line pos=318.185,184.489,349.438,196.614>In ACL ~ . 
		<box id=41 pos=307.276,130.631,525.547,175.512>
			<line pos=307.276,163.507,525.547,175.512>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt 
			<line pos=318.185,152.549,525.547,164.554>Gardner, Christopher Clark, Kenton Lee, and Luke 
			<line pos=318.185,141.590,525.547,153.595>Zettlemoyer. 2018. Deep contextualized word rep- 
			<line pos=318.185,130.631,506.527,142.756>resentations. arXiv preprint arXiv:1802.05365 ~ . 
		<box id=42 pos=307.276,76.772,525.547,121.654>
			<line pos=307.276,109.649,525.547,121.654>Boris T Polyak. 1964. Some methods of speeding up 
			<line pos=318.185,98.690,525.541,110.815>the convergence of iteration methods. USSR Com- 
			<line pos=318.185,87.731,525.544,99.856>putational Mathematics and Mathematical Physics ~ , 
			<line pos=318.185,76.772,359.968,88.777>4(5):1–17. 
	<page id=12>
		<box id=0 pos=72.000,733.094,290.271,777.976>
			<line pos=72.000,765.971,290.271,777.976>Alec Radford, Karthik Narasimhan, Tim Salimans, 
			<line pos=82.909,755.012,201.484,767.017>and Ilya Sutskever. 2018. 
			<line pos=224.348,755.012,290.271,767.017>Improving lan- 
			<line pos=82.909,744.053,290.271,756.058>guage understanding by generative pre-training. 
			<line pos=82.909,733.094,275.336,745.219>https://blog.openai.com/language-unsupervised ~ . 
		<box id=1 pos=72.000,689.345,290.271,723.268>
			<line pos=72.000,711.263,290.271,723.268>Prajit Ramachandran, Peter J Liu, and Quoc V Le. 
			<line pos=82.909,700.304,290.271,712.309>2017. Unsupervised pretraining for sequence to se- 
			<line pos=82.909,689.345,198.276,701.470>quence learning. In EMNLP ~ . 
		<box id=2 pos=72.000,667.514,290.271,679.519>
			<line pos=72.000,667.514,290.271,679.519>Marek Rei. 2017. Semi-supervised multitask learning 
		<box id=3 pos=82.909,656.555,205.449,668.680>
			<line pos=82.909,656.555,205.449,668.680>for sequence labeling. In ACL ~ . 
		<box id=4 pos=72.000,601.847,290.271,646.729>
			<line pos=72.000,634.724,290.271,646.729>Nils Reimers and Iryna Gurevych. 2017. Reporting 
			<line pos=82.909,623.765,290.271,635.770>score distributions makes a difference: Performance 
			<line pos=82.909,612.806,290.271,624.811>study of LSTM-networks for sequence tagging. In 
			<line pos=82.909,601.847,118.057,613.972>EMNLP ~ . 
		<box id=5 pos=72.000,558.098,290.271,592.021>
			<line pos=72.000,580.016,290.271,592.021>Sebastian Ruder. 2017. An overview of multi-task 
			<line pos=82.909,569.057,290.270,581.182>learning in deep neural networks. arXiv preprint 
			<line pos=82.909,558.098,158.177,570.223>arXiv:1706.05098 ~ . 
		<box id=6 pos=72.000,536.267,252.463,548.272>
			<line pos=72.000,536.267,252.463,548.272>Sebastian Ruder and Barbara Plank. 2018. 
		<box id=7 pos=82.909,514.349,290.271,548.272>
			<line pos=263.700,536.267,290.271,548.272>Strong 
			<line pos=82.909,525.308,290.271,537.313>baselines for neural semi-supervised learning under 
			<line pos=82.909,514.349,170.331,526.474>domain shift. In ACL ~ . 
		<box id=8 pos=72.000,459.641,290.271,504.523>
			<line pos=72.000,492.518,290.271,504.523>Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tas- 
			<line pos=82.909,481.559,137.414,493.564>dizen. 2016. 
			<line pos=158.704,481.559,290.271,493.564>Regularization with stochastic 
			<line pos=82.909,470.600,290.271,482.605>transformations and perturbations for deep semi- 
			<line pos=82.909,459.641,201.045,471.766>supervised learning. In NIPS ~ . 
		<box id=9 pos=72.000,415.892,290.271,449.815>
			<line pos=72.000,437.810,290.271,449.815>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, 
			<line pos=82.909,426.851,290.271,438.856>Vicki Cheung, Alec Radford, and Xi Chen. 2016. 
			<line pos=82.909,415.892,275.127,428.017>Improved techniques for training gans. In NIPS ~ . 
		<box id=10 pos=72.000,383.331,290.271,406.066>
			<line pos=72.000,394.061,290.271,406.066>H Scudder. 1965. Probability of error of some adap- 
			<line pos=229.564,383.331,290.266,395.227>IEEE Transac- 
		<box id=11 pos=82.909,372.143,262.236,395.107>
			<line pos=82.909,383.102,221.628,395.107>tive pattern-recognition machines. 
			<line pos=82.909,372.143,262.236,384.268>tions on Information Theory ~ , 11(3):363–371. 
		<box id=12 pos=72.000,328.394,290.271,362.317>
			<line pos=72.000,350.312,290.271,362.317>Rico Sennrich, Barry Haddow, and Alexandra Birch. 
			<line pos=82.909,339.353,290.271,351.358>2016. Improving neural machine translation models 
			<line pos=82.909,328.394,209.902,340.519>with monolingual data. In ACL ~ . 
		<box id=13 pos=72.000,273.686,290.271,318.568>
			<line pos=72.000,306.563,290.271,318.568>Vikas Sindhwani and Mikhail Belkin. 2005. A co- 
			<line pos=82.909,295.604,290.271,307.609>regularization approach to semi-supervised learning 
			<line pos=82.909,284.645,290.270,296.770>with multiple views. In ICML Workshop on Learn- 
			<line pos=82.909,273.686,179.446,285.811>ing with Multiple Views ~ . 
		<box id=14 pos=72.000,229.937,290.271,263.860>
			<line pos=72.000,251.855,290.271,263.860>Anders Søgaard and Yoav Goldberg. 2016. Deep 
			<line pos=82.909,240.896,290.271,252.901>multi-task learning with low level tasks supervised 
			<line pos=82.909,229.937,178.639,242.062>at lower layers. In ACL ~ . 
		<box id=15 pos=72.000,175.229,290.271,220.111>
			<line pos=72.000,208.106,290.271,220.111>Emma Strubell, Patrick Verga, David Belanger, and 
			<line pos=82.909,197.147,290.271,209.152>Andrew McCallum. 2017. Fast and accurate se- 
			<line pos=82.909,186.188,290.271,198.193>quence labeling with iterated dilated convolutions. 
			<line pos=82.909,175.229,128.847,187.354>In EMNLP ~ . 
		<box id=16 pos=72.000,120.521,290.271,165.403>
			<line pos=72.000,153.398,290.271,165.403>Sandeep Subramanian, Adam Trischler, Yoshua Ben- 
			<line pos=82.909,142.439,290.271,154.444>gio, and Christopher J Pal. 2018. Learning gen- 
			<line pos=82.909,131.480,290.271,143.485>eral purpose distributed sentence representations via 
			<line pos=82.909,120.521,244.033,132.646>large scale multi-task learning. In ICLR ~ . 
		<box id=17 pos=72.000,76.772,290.271,110.695>
			<line pos=72.000,98.690,290.271,110.695>Ilya Sutskever, James Martens, George Dahl, and Ge- 
			<line pos=82.909,87.731,290.271,99.736>offrey Hinton. 2013. On the importance of initial- 
			<line pos=82.909,76.772,284.621,88.897>ization and momentum in deep learning. In ICML ~ . 
		<box id=18 pos=307.276,744.053,525.547,777.976>
			<line pos=307.276,765.971,525.547,777.976>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. 
			<line pos=318.185,755.012,525.547,767.017>Sequence to sequence learning with neural net- 
			<line pos=318.185,744.053,382.822,756.178>works. In NIPS ~ . 
		<box id=19 pos=307.276,688.642,525.547,733.524>
			<line pos=307.276,721.519,525.547,733.524>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, 
			<line pos=318.185,710.560,525.547,722.565>Jon Shlens, and Zbigniew Wojna. 2016. Rethinking 
			<line pos=318.185,699.601,509.507,711.606>the inception architecture for computer vision. 
			<line pos=517.248,699.601,525.547,711.606>In 
			<line pos=318.185,688.642,345.582,700.767>CVPR ~ . 
		<box id=20 pos=307.276,644.190,525.547,678.113>
			<line pos=307.276,666.108,525.547,678.113>Antti Tarvainen and Harri Valpola. 2017. Weight- 
			<line pos=459.504,655.149,492.411,667.154>improve 
			<line pos=503.410,655.149,525.547,667.154>semi- 
			<line pos=460.132,644.190,525.546,656.315>In Workshop on 
		<box id=21 pos=318.185,633.231,492.818,667.154>
			<line pos=318.185,655.149,353.791,667.154>averaged 
			<line pos=422.125,655.149,448.506,667.154>targets 
			<line pos=318.185,644.190,452.252,656.195>supervised deep learning results. 
			<line pos=318.185,633.231,492.818,645.356>Learning with Limited Labeled Data, NIPS ~ . 
		<box id=22 pos=364.790,655.149,411.126,667.154>
			<line pos=364.790,655.149,411.126,667.154>consistency 
		<box id=23 pos=307.276,588.779,525.547,622.702>
			<line pos=307.276,610.697,525.547,622.702>Erik F Tjong Kim Sang and Sabine Buchholz. 
			<line pos=318.185,599.738,525.547,611.743>2000. Introduction to the CoNLL-2000 shared task: 
			<line pos=318.185,588.779,406.194,600.904>Chunking. In CoNLL ~ . 
		<box id=24 pos=307.276,533.369,525.547,578.250>
			<line pos=307.276,566.245,525.547,578.250>Erik F Tjong Kim Sang and Fien De Meulder. 
			<line pos=318.185,555.286,525.547,567.291>2003. Introduction to the CoNLL-2003 shared task: 
			<line pos=318.185,544.327,525.547,556.332>Language-independent named entity recognition. In 
			<line pos=318.185,533.369,371.764,545.494>HLT-NAACL ~ . 
		<box id=25 pos=307.276,466.999,525.547,522.839>
			<line pos=307.276,510.834,525.547,522.839>Vikas Verma, Alex Lamb, Christopher Beckham, 
			<line pos=318.185,499.876,525.547,511.881>Aaron Courville, Ioannis Mitliagkis, and Yoshua 
			<line pos=318.185,488.917,525.547,500.922>Bengio. 2018. Manifold mixup: Encouraging mean- 
			<line pos=318.185,477.958,525.547,489.963>ingful on-manifold interpolation as a regularizer. 
			<line pos=318.185,466.999,452.859,479.124>arXiv preprint arXiv:1806.05236 ~ . 
		<box id=26 pos=307.276,433.506,525.547,456.470>
			<line pos=307.276,444.465,525.547,456.470>Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing 
			<line pos=377.971,433.506,525.547,445.511>Improving the improved training of 
		<box id=27 pos=318.185,422.547,434.418,445.511>
			<line pos=318.185,433.506,369.303,445.511>Gong. 2018. 
			<line pos=318.185,422.547,434.418,434.672>Wasserstein GANs. In ICLR ~ . 
		<box id=28 pos=307.276,400.013,355.465,412.018>
			<line pos=307.276,400.013,355.465,412.018>Huijia Wu, 
		<box id=29 pos=318.185,378.095,525.547,412.018>
			<line pos=363.465,400.013,525.547,412.018>Jiajun Zhang, and Chengqing Zong. 
			<line pos=318.185,389.054,525.542,401.179>2017. Shortcut sequence tagging. arXiv preprint 
			<line pos=318.185,378.095,393.452,390.220>arXiv:1701.00576 ~ . 
		<box id=30 pos=307.276,344.831,525.547,367.566>
			<line pos=307.276,355.561,525.547,367.566>Chang Xu, Dacheng Tao, and Chao Xu. 2013. A 
			<line pos=465.218,344.831,525.542,356.727>arXiv preprint 
		<box id=31 pos=318.185,333.643,450.717,356.607>
			<line pos=318.185,344.602,450.717,356.607>survey on multi-view learning. 
			<line pos=318.185,333.643,388.471,345.768>arXiv:1304.5634 ~ . 
		<box id=32 pos=307.276,300.150,525.547,323.114>
			<line pos=307.276,311.109,525.547,323.114>David Yarowsky. 1995. Unsupervised word sense dis- 
			<line pos=318.185,300.150,519.668,312.275>ambiguation rivaling supervised methods. In ACL ~ . 
		<box id=33 pos=307.276,255.698,525.547,289.621>
			<line pos=307.276,277.616,525.547,289.621>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, 
			<line pos=318.185,266.657,525.547,278.662>and David Lopez-Paz. 2018. mixup: Beyond em- 
			<line pos=318.185,255.698,457.650,267.823>pirical risk minimization. In ICLR ~ . 
		<box id=34 pos=307.276,233.164,479.390,245.169>
			<line pos=307.276,233.164,479.390,245.169>Yuan Zhang and David Weiss. 2016. 
		<box id=35 pos=318.185,211.246,525.547,245.169>
			<line pos=500.092,233.164,525.547,245.169>Stack- 
			<line pos=318.185,222.205,525.547,234.210>propagation: Improved representation learning for 
			<line pos=318.185,211.246,381.527,223.371>syntax. In ACL ~ . 
		<box id=36 pos=307.276,166.794,525.547,200.717>
			<line pos=307.276,188.712,525.547,200.717>Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex- 
			<line pos=318.185,177.753,525.543,189.878>ploiting unlabeled data using three classiﬁers. IEEE 
			<line pos=318.185,166.794,521.063,178.919>Transactions on knowledge and Data Engineering ~ . 
		<box id=37 pos=307.276,139.602,410.545,155.156>
			<line pos=307.276,139.602,410.545,155.156>A Detailed Results 
		<box id=38 pos=307.276,76.568,525.545,130.361>
			<line pos=307.276,117.216,525.545,130.361>We provide a more detailed version of the test set 
			<line pos=307.276,103.667,525.545,116.812>results in the paper, adding two decimals of pre- 
			<line pos=307.276,90.118,525.545,103.263>cision, standard deviations of the 5 runs for each 
			<line pos=307.276,76.568,481.418,89.713>model, and more prior work, in Table 5. 
	<page id=13>
		<box id=0 pos=72.000,765.624,162.656,781.178>
			<line pos=72.000,765.624,162.656,781.178>B Model Details 
		<box id=1 pos=72.000,96.380,290.272,757.629>
			<line pos=72.000,744.484,290.269,757.629>Our models use two layer CNN-BiLSTM en- 
			<line pos=72.000,730.934,290.269,744.079>coders (Chiu and Nichols, 2016; Ma and Hovy, 
			<line pos=72.000,717.385,290.269,730.530>2016; Lample et al., 2016) and task-speciﬁc pre- 
			<line pos=72.000,703.836,290.269,716.981>diction modules. See Section 3 of the paper for 
			<line pos=72.000,690.287,290.269,703.432>details. We provide a few minor details not cov- 
			<line pos=72.000,676.738,146.444,689.883>ered there below. 
			<line pos=72.000,658.220,290.271,672.489>Sequence Tagging. For Chunking and Named En- 
			<line pos=72.000,644.671,290.269,657.816>tity Recognition, we use a BIOES tagging scheme. 
			<line pos=72.000,631.122,290.269,644.267>We apply label smoothing (Szegedy et al., 2016; 
			<line pos=72.000,617.573,290.269,630.718>Pereyra et al., 2017) with a rate of 0.1 to the target 
			<line pos=72.000,604.024,248.029,617.169>labels when training on the labeled data. 
			<line pos=72.000,585.506,290.272,599.775>Dependency Parsing. We omit punctuation from 
			<line pos=72.000,571.957,290.269,585.102>evaluation, which is standard practice for the PTB- 
			<line pos=72.000,558.408,290.269,571.553>SD 3.3.0 dataset. ROOT is represented with a 
			<line pos=72.000,544.406,290.272,558.004>ﬁxed vector h ~ ROOT instead of using a vector from 
			<line pos=72.000,531.309,290.269,544.454>the encoder, but otherwise dependencies coming 
			<line pos=72.000,517.760,290.269,530.905>from ROOT are scored the same way as the other 
			<line pos=72.000,504.211,133.495,517.356>dependencies. 
			<line pos=72.000,485.693,290.268,499.962>Machine Translation. We apply dropout to the 
			<line pos=72.000,472.144,290.269,485.289>output of each LSTM layer in the decoder. Our 
			<line pos=72.000,458.595,290.269,471.740>implementation is heavily based off of the Google 
			<line pos=72.000,445.046,290.266,459.243>NMT Tutorial ~ 3 (Luong et al., 2017). We attribute 
			<line pos=72.000,431.497,290.269,444.642>our signiﬁcantly better results to using pre-trained 
			<line pos=72.000,417.948,290.269,431.093>word embeddings, a character-level CNN, a larger 
			<line pos=72.000,404.398,290.269,417.543>model, stronger regularization, and better hyper- 
			<line pos=72.000,390.849,290.269,403.994>parameter tuning. Target words occurring 5 or 
			<line pos=72.000,377.300,290.267,390.445>fewer times in the train set are replaced with a UNK 
			<line pos=72.000,363.751,290.269,376.896>token (but not during evaluation). We use a beam 
			<line pos=72.000,350.202,290.269,363.347>size of 10 when performing beam search. We 
			<line pos=72.000,336.652,290.269,349.797>found it slightly beneﬁcial to apply label smooth- 
			<line pos=72.000,323.103,290.269,336.248>ing with a rate of 0.1 to the teacher’s predictions 
			<line pos=72.000,309.554,290.269,322.699>(unlike our other tasks, the teacher only provides 
			<line pos=72.000,296.005,259.353,309.150>hard targets to the students for translation). 
			<line pos=72.000,277.487,290.270,291.756>Multi-Task Learning. Several of our datasets 
			<line pos=72.000,263.938,290.269,277.083>are constructed from the Penn Treebank. How- 
			<line pos=72.000,250.389,290.269,263.534>ever, we treat them as separate rather than provid- 
			<line pos=72.000,236.840,290.269,249.985>ing examples labeled across multiple tasks to our 
			<line pos=72.000,223.291,290.269,236.436>model during supervised training. Furthermore, 
			<line pos=72.000,209.741,290.269,222.886>the Penn Treebank tasks do not all use the same 
			<line pos=72.000,196.192,290.269,209.337>train/dev/test splits. We ensure the training split 
			<line pos=72.000,182.643,290.269,195.788>of one task never overlaps the evaluation split of 
			<line pos=72.000,169.094,290.269,182.239>another by discarding the overlapping examples 
			<line pos=72.000,155.545,153.807,168.690>from the train sets. 
			<line pos=72.000,137.027,290.265,151.296>Other Details. We apply dropout (Hinton et al., 
			<line pos=72.000,123.478,290.269,136.623>2012) to the word embeddings and outputs of each 
			<line pos=72.000,109.929,290.269,123.074>Bi-LSTM. We use an exponential-moving-average 
			<line pos=72.000,96.380,290.269,109.525>(EMA) of the model weights from training for the 
		<box id=2 pos=84.653,77.606,265.674,88.645>
			<line pos=84.653,77.606,265.674,88.645>3 ~ https://github.com/tensorflow/nmt 
		<box id=3 pos=307.276,327.300,525.549,778.912>
			<line pos=307.276,765.767,525.545,778.912>ﬁnal model; we found this to slightly improve ac- 
			<line pos=307.276,752.217,525.545,765.362>curacy and signiﬁcantly reduce the variance in ac- 
			<line pos=307.276,738.668,525.545,751.813>curacy between models trained with different ran- 
			<line pos=307.276,725.119,525.545,738.264>dom initializations. The model is trained using 
			<line pos=307.276,711.570,525.545,724.715>SGD with momentum (Polyak, 1964; Sutskever 
			<line pos=307.276,698.021,525.545,711.166>et al., 2013). Word embeddings are initialized with 
			<line pos=307.276,684.471,525.545,697.616>GloVe vectors (Pennington et al., 2014) and ﬁne- 
			<line pos=307.276,670.922,525.545,684.067>tuned during training. The full set of model hyper- 
			<line pos=307.276,657.373,446.367,670.518>parameters are listed in Table 6. 
			<line pos=307.276,638.931,525.541,653.200>Baselines. Baselines were run with the same ar- 
			<line pos=307.276,625.382,525.545,638.527>chitecture and hyperparameters as the CVT model. 
			<line pos=307.276,611.833,525.545,624.978>For the “word dropout” model, we randomly re- 
			<line pos=307.276,598.283,525.543,611.428>place words in the input sentence with a REMOVED 
			<line pos=307.276,584.734,525.545,597.879>token with probability 0.1 (this value worked well 
			<line pos=307.276,571.185,525.545,584.330>on the dev sets). For Virtual Adversarial Train- 
			<line pos=307.276,557.636,525.545,570.781>ing, we set the norm of the perturbation to be 1.5 
			<line pos=307.276,544.087,525.545,557.232>for CCG, 1.0 for Dependency Parsing, and 0.5 for 
			<line pos=307.276,530.537,525.545,543.682>the other tasks (these values worked best on the 
			<line pos=307.276,516.988,525.545,530.133>dev sets). Otherwise, the implementation is as de- 
			<line pos=307.276,503.439,525.545,516.584>scribed in (Miyato et al., 2017a); we based our im- 
			<line pos=307.276,489.890,525.548,504.087>plementation off of their code ~ 4 ~ . We were unable 
			<line pos=307.276,476.341,525.545,489.486>to successfully apply VAT to machine translation, 
			<line pos=307.276,462.791,525.545,475.936>perhaps because the student is provided hard tar- 
			<line pos=307.276,449.242,525.545,462.387>gets for that task. For ELMo, we applied dropout 
			<line pos=307.276,435.693,525.545,448.838>to the ELMo embeddings before they are incor- 
			<line pos=307.276,422.144,525.545,435.289>porated into the rest of the model. When training 
			<line pos=307.276,408.595,525.545,421.740>the multi-task ELMo model, each prediction mod- 
			<line pos=307.276,395.045,525.545,408.190>ule has its own set of softmax-normalized weights 
			<line pos=307.276,381.496,331.903,394.641>( ~ s ~ task 
			<line pos=335.065,381.496,525.549,394.641>in (Peters et al., 2018)) for the ELMo emed- 
			<line pos=307.276,367.947,525.545,381.092>dings going into the task-speciﬁc prediction mod- 
			<line pos=307.276,353.572,525.539,367.543>ules. All tasks share the same s ~ j weights for 
			<line pos=307.276,340.849,525.545,353.994>the ELMo embeddings going into the shared Bi- 
			<line pos=307.276,327.300,375.752,340.445>LSTM encoder. 
		<box id=4 pos=316.022,379.251,319.447,387.221>
			<line pos=316.022,379.251,319.447,387.221>j 
		<box id=5 pos=307.276,303.134,470.010,318.688>
			<line pos=307.276,303.134,470.010,318.688>C CVT for Image Recognition 
		<box id=6 pos=307.276,106.041,525.545,295.326>
			<line pos=307.276,282.181,525.545,295.326>Although the focus of our work is on NLP, we 
			<line pos=307.276,268.631,525.545,281.776>also applied CVT to image recognition and found 
			<line pos=307.276,255.082,525.545,268.227>it performs competitively with existing methods. 
			<line pos=307.276,241.533,525.545,254.678>Most of the semi-supervised image recognition 
			<line pos=307.276,227.984,525.545,241.129>approaches we compare against rely on the in- 
			<line pos=307.276,214.435,525.545,227.580>puts being continuous, so they would be difﬁ- 
			<line pos=307.276,200.885,525.545,214.030>cult to apply to text. More speciﬁcally, consis- 
			<line pos=307.276,187.336,525.545,200.481>tency regularization methods (Sajjadi et al., 2016; 
			<line pos=307.276,173.787,525.545,186.932>Laine and Aila, 2017; Miyato et al., 2017b) rely 
			<line pos=307.276,160.238,525.545,173.383>on adding continuous noise and applying image- 
			<line pos=307.276,146.689,525.545,159.834>speciﬁc transformations like cropping to inputs, 
			<line pos=307.276,133.139,525.545,146.284>GANs (Salimans et al., 2016; Wei et al., 2018) are 
			<line pos=307.276,119.590,525.545,132.735>very difﬁcult to train on text due to its discrete na- 
			<line pos=307.276,106.041,525.545,119.186>ture, and mixup (Zhang et al., 2018; Verma et al., 
		<box id=7 pos=319.928,87.569,522.469,98.608>
			<line pos=319.928,87.569,522.469,98.608>4 ~ https://github.com/tensorflow/models/ 
		<box id=8 pos=307.276,77.606,506.330,87.003>
			<line pos=307.276,77.606,506.330,87.003>tree/master/research/adversarial_text 
	<page id=14>
		<box id=0 pos=72.000,544.161,290.273,778.912>
			<line pos=72.000,765.767,290.269,778.912>2018) requires a way of smoothly interpolating be- 
			<line pos=72.000,752.217,170.193,765.362>tween different inputs. 
			<line pos=72.000,733.849,290.267,748.118>Approach. Our image recognition models are 
			<line pos=72.000,720.300,290.269,733.445>based on Convolutional Neural Networks, which 
			<line pos=72.000,705.925,290.268,725.918>produce a set of features H(x ~ i ~ ) ∈ R ~ n ~ × ~ n ~ × ~ d from 
			<line pos=72.000,692.375,290.268,706.347>an image x ~ i ~ . The ﬁrst two dimensions of H in- 
			<line pos=72.000,679.653,290.269,692.798>dex into the spatial coordinates of feature vec- 
			<line pos=72.000,666.103,290.273,679.248>tors and d is the size of the feature vectors. For 
			<line pos=72.000,652.554,290.269,665.699>shallower CNNs, a particular feature vector cor- 
			<line pos=72.000,639.005,290.269,652.150>responds to a region of the input image. For ex- 
			<line pos=72.000,624.629,290.268,638.601>ample, H ~ 0,0 would be a d ~ -dimensional vector of 
			<line pos=72.000,611.907,290.269,625.052>features extracted from the upper left corner. For 
			<line pos=72.000,598.357,290.269,611.502>deeper CNNs, a particular feature vector would be 
			<line pos=72.000,584.808,290.269,597.953>extracted from the whole image, but still only use 
			<line pos=72.000,571.259,290.269,584.404>a “region” of the representations from an earlier 
			<line pos=72.000,557.710,290.269,570.855>layer. The CNNs in our experiments are all in the 
			<line pos=72.000,544.161,131.116,557.306>ﬁrst category. 
		<box id=1 pos=72.000,419.190,291.977,543.756>
			<line pos=82.909,530.611,290.269,543.756>The primary prediction layers of our CNNs take 
			<line pos=72.000,517.062,290.269,530.207>as input the mean of H over the ﬁrst two dimen- 
			<line pos=72.000,503.513,290.270,516.658>sions, which results in a d ~ -dimensional vector that 
			<line pos=72.000,489.964,188.651,503.109>is fed into a softmax layer: 
			<line pos=74.261,467.727,291.977,484.418>p ~ θ ~ (y ~ | ~ x ~ i ~ ) = softmax ~ (W global average pool ~ (H) + b) 
			<line pos=82.909,446.288,290.267,459.433>We add n ~ 2 auxiliary prediction layers to the top 
			<line pos=72.000,432.739,290.271,445.884>of the CNN. The j ~ th layer takes a single feature 
			<line pos=72.000,419.190,139.102,432.335>vector as input: 
		<box id=2 pos=84.283,394.639,277.985,416.519>
			<line pos=89.772,394.639,277.985,416.519>θ ~ (y ~ | ~ x ~ i ~ ) = softmax ~ (W j ~ H ~ (cid:98) ~ j/n ~ (cid:99) ~ ,j mod n + b ~ j ~ ) 
			<line pos=84.283,397.592,93.197,411.439>p ~ j 
		<box id=3 pos=72.000,93.848,290.269,384.964>
			<line pos=72.000,370.695,290.269,384.964>Data. We evaluated our models on the CIFAR- 
			<line pos=72.000,357.146,290.269,370.291>10 (Krizhnevsky and Hinton, 2009) dataset. Fol- 
			<line pos=72.000,343.596,290.269,356.741>lowing previous work, we make the datasets semi- 
			<line pos=72.000,330.047,290.269,343.192>supervised by only using the provided labels for a 
			<line pos=72.000,316.498,290.269,329.643>subset of the examples in the training set; the rest 
			<line pos=72.000,302.949,221.182,316.094>are treated as unlabeled examples. 
			<line pos=72.000,284.581,290.265,298.850>Model. We use the convolutional neural network 
			<line pos=72.000,271.032,290.269,284.177>from Miyato et al. (2017b), adapting their Ten- 
			<line pos=72.000,257.482,290.269,271.679>sorFlow implementation ~ 5 ~ . Their model contains 
			<line pos=72.000,243.933,290.269,257.078>9 convolutional layers and 2 max pooling layers. 
			<line pos=72.000,230.384,290.269,243.529>See Appendix D of Miyato et al.’s paper for more 
			<line pos=72.000,216.835,103.211,229.980>details. 
			<line pos=82.909,203.286,290.268,222.453>We add 36 auxiliary softmax layers to the 6 × 
			<line pos=72.000,189.736,290.264,202.881>6 collection of feature vectors produced by the 
			<line pos=72.000,176.187,290.269,189.332>CNN. Each auxiliary layer sees a patch of the im- 
			<line pos=72.000,162.638,290.264,181.805>age ranging in size from 21 × 21 pixels (the cor- 
			<line pos=72.000,149.089,290.267,168.256>ner) to 29 × 29 pixels (the center) of the 32 × 32 
			<line pos=72.000,135.540,290.269,148.685>pixel images. For some experiments, we combine 
			<line pos=72.000,121.990,290.269,135.135>CVT with standard consistency regularization by 
			<line pos=72.000,108.441,290.269,121.586>adding a perturbation (e.g., a small random vec- 
			<line pos=72.000,93.848,290.267,114.059>tor) to the student’s inputs when computing L ~ CVT ~ . 
		<box id=4 pos=84.653,77.606,265.674,88.645>
			<line pos=84.653,77.606,265.674,88.645>5 ~ https://github.com/takerum/vat_tf 
		<box id=5 pos=307.276,589.627,525.545,780.036>
			<line pos=307.276,765.767,525.539,780.036>Results. The results are shown in Table 7. Un- 
			<line pos=307.276,752.217,525.545,765.362>surprisingly, adding continuous noise to the in- 
			<line pos=307.276,738.668,525.545,751.813>puts works much better with images, where the in- 
			<line pos=307.276,725.119,525.545,738.264>puts are naturally continuous, than with language. 
			<line pos=307.276,711.570,525.545,724.715>Therefore we see much better results from VAT 
			<line pos=307.276,698.021,525.545,711.166>on semi-supervised CIFAR-10 compared to on our 
			<line pos=307.276,684.471,525.545,697.616>NLP tasks. However, we still ﬁnd incorporat- 
			<line pos=307.276,670.922,525.545,684.067>ing CVT improves over models without CVT. Our 
			<line pos=307.276,657.373,525.545,670.518>CVT + VAT models are competitive with current 
			<line pos=307.276,643.824,525.545,656.969>start-of-the-art approaches. We found the gains 
			<line pos=307.276,630.275,525.545,643.420>from CVT are larger when no data augmentation 
			<line pos=307.276,616.726,525.545,629.871>is applied, perhaps because random translations of 
			<line pos=307.276,603.176,525.545,616.321>the input expose the model to different “views” in 
			<line pos=307.276,589.627,441.000,602.772>a similar manner as with CVT. 
		<box id=6 pos=307.276,564.192,412.290,579.746>
			<line pos=307.276,564.192,412.290,579.746>D Negative Results 
		<box id=7 pos=307.276,461.035,525.545,555.475>
			<line pos=307.276,542.330,525.545,555.475>We brieﬂy describe a few ideas we implemented 
			<line pos=307.276,528.781,525.545,541.926>that did not seem to be effective in initial experi- 
			<line pos=307.276,515.232,525.545,528.377>ments. Note these ﬁndings are from early one-off 
			<line pos=307.276,501.682,525.545,514.827>experiments. We did not pursue them further after 
			<line pos=307.276,488.133,525.545,501.278>our ﬁrst attempts did not pan out, so it is possible 
			<line pos=307.276,474.584,525.545,487.729>that some of these approaches could be effective 
			<line pos=307.276,461.035,480.600,474.180>with the proper adjustments and tuning. 
		<box id=8 pos=318.658,314.814,525.545,455.923>
			<line pos=318.658,436.756,525.539,455.923>• Hard vs soft targets: Classic self-training 
			<line pos=329.094,423.207,525.545,436.352>algorithms train the student model with 
			<line pos=329.094,409.658,525.545,422.803>one-hot “hard” targets corresponding to the 
			<line pos=329.094,396.109,507.741,409.254>teacher’s highest probability prediction. 
			<line pos=516.458,396.109,525.545,409.254>In 
			<line pos=329.094,382.560,525.545,395.705>our experiments, this decreased performance 
			<line pos=329.094,369.010,525.545,382.155>compared to using soft targets. This ﬁnding is 
			<line pos=329.094,355.461,525.545,368.606>consistent with research on knowledge distil- 
			<line pos=329.094,341.912,525.545,355.057>lation (Hinton et al., 2015; Furlanello et al., 
			<line pos=329.094,328.363,525.545,341.508>2018) where soft targets also work notably 
			<line pos=329.094,314.814,431.901,327.959>better than hard targets. 
		<box id=9 pos=318.658,290.611,451.767,309.702>
			<line pos=318.658,290.611,451.767,309.702>• Conﬁdence thresholding: 
		<box id=10 pos=329.094,155.043,525.545,303.680>
			<line pos=466.327,290.535,525.542,303.680>Classic self- 
			<line pos=329.094,276.986,525.545,290.131>training often only trains the student on a 
			<line pos=329.094,263.437,525.545,276.582>subset of the unlabeled examples on which 
			<line pos=329.094,249.888,525.545,263.033>the teacher has conﬁdent predictions (i.e., the 
			<line pos=329.094,236.339,525.545,249.484>output distribution has low entropy). We 
			<line pos=329.094,222.789,506.640,235.934>tried both “hard” (where the student 
			<line pos=513.425,222.789,525.545,235.934>ig- 
			<line pos=329.094,209.240,525.545,222.385>nores low-conﬁdence examples) and “soft” 
			<line pos=329.094,195.691,525.545,208.836>(where examples are weighted according to 
			<line pos=329.094,182.142,525.545,195.287>the teacher’s conﬁdence) versions of this for 
			<line pos=329.094,168.593,525.545,181.738>training our models, but they did not seem to 
			<line pos=329.094,155.043,425.705,168.188>improve performance. 
		<box id=11 pos=318.658,76.568,525.545,149.932>
			<line pos=318.658,130.765,525.544,149.932>• Mean Teacher: The Mean Teacher method 
			<line pos=329.094,117.216,525.545,130.361>(Tarvainen and Valpola, 2017) tracks an ex- 
			<line pos=329.094,103.667,525.545,116.812>ponential moving average (EMA) of model 
			<line pos=329.094,90.118,525.545,103.263>weights, which are used to produce targets 
			<line pos=329.094,76.568,525.545,89.713>for the students. The idea is that these tar- 
	<page id=15>
		<box id=0 pos=93.818,698.021,290.269,778.912>
			<line pos=93.818,765.767,290.269,778.912>gets may be better quality due to a self- 
			<line pos=93.818,752.217,290.269,765.362>ensembling effect. However, we found this 
			<line pos=93.818,738.668,290.269,751.813>approach to have little to no beneﬁt in our 
			<line pos=93.818,725.119,290.269,738.264>experiments, although using EMA model 
			<line pos=93.818,711.570,142.571,724.715>weights at 
			<line pos=171.600,711.570,290.269,724.715>time did improve results 
			<line pos=93.818,698.021,128.574,711.166>slightly. 
		<box id=1 pos=149.509,711.570,164.662,724.715>
			<line pos=149.509,711.570,164.662,724.715>test 
		<box id=2 pos=83.382,540.013,290.269,694.672>
			<line pos=83.382,675.505,290.265,694.672>• Purely supervised CVT: Lastly, we ex- 
			<line pos=93.818,661.956,290.269,675.101>plored adding cross-view losses to purely su- 
			<line pos=93.818,648.407,290.269,661.552>pervised classiﬁers. We hoped that adding 
			<line pos=93.818,634.858,290.269,648.003>auxiliary softmax layers with different views 
			<line pos=93.818,621.308,290.269,634.453>of the input would act as a regularizer on the 
			<line pos=93.818,607.759,290.269,620.904>model. However, we found little to no ben- 
			<line pos=93.818,594.210,290.269,607.355>eﬁt from this approach. This negative re- 
			<line pos=93.818,580.661,290.269,593.806>sult suggests that the gains from CVT are 
			<line pos=93.818,567.112,290.269,580.257>from the improved semi-supervised learning 
			<line pos=93.818,553.562,290.269,566.707>mechanism, not the additional prediction lay- 
			<line pos=93.818,540.013,210.600,553.158>ers regularizing the model. 
	<page id=16>
		<box id=0 pos=76.698,725.772,97.382,733.784>
			<line pos=76.698,725.772,97.382,733.784>Method 
		<box id=1 pos=76.698,528.926,189.165,718.641>
			<line pos=76.698,710.630,184.631,718.641>LSTM-CNN-CRF (Ma and Hovy, 2016) 
			<line pos=76.698,702.373,179.930,710.384>LSTM-CNN (Chiu and Nichols, 2016) 
			<line pos=76.698,694.115,173.096,702.127>ID-CNN-CRF (Strubell et al., 2017) 
			<line pos=76.698,685.858,180.023,693.869>Tri-Trained LSTM (Lewis et al., 2016) 
			<line pos=76.698,677.600,165.190,685.612>Shortcut LSTM (Wu et al., 2017) 
			<line pos=76.698,669.343,159.060,677.354>JMT* (Hashimoto et al., 2017) 
			<line pos=76.698,661.085,184.179,669.096>LM-LSTM-CNN-CRF (Liu et al., 2017) 
			<line pos=76.698,652.822,152.622,664.151>TagLM ~ † (Peters et al., 2017) 
			<line pos=76.698,644.559,150.202,655.888>ELMo ~ † (Peters et al., 2018) 
			<line pos=76.698,633.545,150.278,641.556>NPM (Ma and Hovy, 2017) 
			<line pos=76.698,625.287,189.165,633.299>Deep Biafﬁne (Dozat and Manning, 2017) 
			<line pos=76.698,617.030,158.502,625.042>Stack Pointer (Ma et al., 2018) 
			<line pos=76.698,606.017,176.599,614.028>Stanford (Luong and Manning, 2015) 
			<line pos=76.698,597.759,150.929,605.771>Google (Luong et al., 2017) 
			<line pos=76.698,586.745,106.244,594.757>Supervised 
			<line pos=76.698,578.488,155.364,586.499>Virtual Adversarial Training* 
			<line pos=76.698,570.230,118.451,578.242>Word Dropout* 
			<line pos=76.698,561.973,97.382,569.984>ELMo* 
			<line pos=76.698,553.709,134.453,565.039>ELMo + Multi-task* ~ † 
			<line pos=76.698,545.452,93.320,553.463>CVT* 
			<line pos=76.698,537.189,132.073,548.518>CVT + Multi-Task* ~ † 
			<line pos=76.698,528.926,154.531,540.255>CVT + Multi-Task + Large* ~ † 
		<box id=2 pos=412.334,729.901,466.972,737.913>
			<line pos=412.334,729.901,466.972,737.913>Dependency Parsing 
		<box id=3 pos=401.527,721.644,414.558,729.655>
			<line pos=401.527,721.644,414.558,729.655>UAS 
		<box id=4 pos=442.688,721.644,455.247,729.655>
			<line pos=442.688,721.644,455.247,729.655>LAS 
		<box id=5 pos=484.093,721.644,514.144,737.913>
			<line pos=484.093,729.901,514.144,737.913>Translation 
			<line pos=484.093,721.644,501.452,729.655>BLEU 
		<box id=6 pos=236.639,721.644,262.868,737.913>
			<line pos=236.639,729.901,262.868,737.913>Chunking 
			<line pos=236.639,721.644,243.660,729.655>F1 
		<box id=7 pos=318.962,721.644,332.259,737.913>
			<line pos=318.962,729.901,332.259,737.913>FGN 
			<line pos=318.962,721.644,325.983,729.655>F1 
		<box id=8 pos=277.801,694.115,354.052,737.913>
			<line pos=277.801,729.901,291.098,737.913>NER 
			<line pos=277.801,721.644,284.821,729.655>F1 
			<line pos=277.801,710.630,292.760,718.641>91.21 
			<line pos=277.801,702.373,354.052,714.054>91.62 ± 0.33 86.28 ± 0.26 
			<line pos=277.801,694.115,354.052,705.797>90.65 ± 0.15 86.84 ± 0.19 
		<box id=9 pos=360.123,710.630,375.083,737.913>
			<line pos=360.123,729.901,372.316,737.913>POS 
			<line pos=360.123,721.644,372.489,729.655>Acc. 
			<line pos=360.123,710.630,375.083,718.641>97.55 
		<box id=10 pos=195.478,721.644,209.148,737.913>
			<line pos=195.478,729.901,209.148,737.913>CCG 
			<line pos=195.478,721.644,207.845,729.655>Acc. 
		<box id=11 pos=195.478,677.600,210.437,693.869>
			<line pos=195.478,685.858,207.113,693.869>94.7 
			<line pos=195.478,677.600,210.437,685.612>95.08 
		<box id=12 pos=236.640,644.559,312.891,677.354>
			<line pos=236.640,669.343,251.599,677.354>95.77 
			<line pos=236.640,661.085,312.891,672.766>95.96 ± 0.08 91.71 ± 0.10 
			<line pos=236.640,652.822,312.891,664.503>96.37 ± 0.05 91.93 ± 0.19 
			<line pos=277.801,644.559,312.891,656.240>92.22 ± 0.10 
		<box id=13 pos=360.122,661.085,395.213,685.612>
			<line pos=360.123,677.600,375.082,685.612>97.53 
			<line pos=360.123,669.343,375.083,677.354>97.55 
			<line pos=360.122,661.085,395.213,672.766>97.53 ± 0.03 
		<box id=14 pos=401.527,669.343,416.486,677.354>
			<line pos=401.527,669.343,416.486,677.354>94.67 
		<box id=15 pos=442.688,669.343,457.648,677.354>
			<line pos=442.688,669.343,457.648,677.354>92.90 
		<box id=16 pos=401.527,617.030,416.486,641.556>
			<line pos=401.527,633.545,413.162,641.556>94.9 
			<line pos=401.527,625.287,416.486,633.299>95.74 
			<line pos=401.527,617.030,416.486,625.042>95.87 
		<box id=17 pos=442.688,617.030,457.648,641.556>
			<line pos=442.688,633.545,454.323,641.556>93.0 
			<line pos=442.688,625.287,457.648,633.299>94.08 
			<line pos=442.688,617.030,457.648,625.042>94.19 
		<box id=18 pos=484.093,597.759,495.728,614.028>
			<line pos=484.093,606.017,495.728,614.028>23.3 
			<line pos=484.093,597.759,495.728,605.771>26.1 
		<box id=19 pos=195.478,528.926,519.182,598.427>
			<line pos=195.478,586.745,519.182,598.427>94.94 ± 0.02 95.10 ± 0.06 91.16 ± 0.09 87.48 ± 0.08 97.60 ± 0.02 95.08 ± 0.03 93.27 ± 0.03 28.88 ± 0.12 
			<line pos=195.478,578.488,487.417,590.169>95.07 ± 0.04 95.06 ± 0.06 91.75 ± 0.10 87.91 ± 0.11 97.64 ± 0.03 95.44 ± 0.06 93.72 ± 0.07 – 
			<line pos=195.478,570.230,519.182,581.912>95.20 ± 0.04 95.79 ± 0.08 92.14 ± 0.11 88.06 ± 0.09 97.66 ± 0.01 95.56 ± 0.05 93.80 ± 0.08 29.33 ± 0.10 
			<line pos=195.478,561.973,519.182,573.654>95.79 ± 0.04 96.50 ± 0.03 92.24 ± 0.09 88.49 ± 0.12 97.72 ± 0.01 96.22 ± 0.05 94.44 ± 0.06 29.34 ± 0.11 
			<line pos=195.478,553.709,487.417,565.391>95.91 ± 0.05 96.83 ± 0.03 92.32 ± 0.12 88.37 ± 0.16 97.79 ± 0.03 96.40 ± 0.04 94.79 ± 0.05 – 
			<line pos=195.478,545.452,519.182,557.133>95.65 ± 0.04 96.58 ± 0.04 92.34 ± 0.06 88.68 ± 0.14 97.70 ± 0.03 95.86 ± 0.03 94.06 ± 0.02 29.58 ± 0.07 
			<line pos=195.478,537.189,487.417,548.871>95.97 ± 0.04 96.85 ± 0.05 92.42 ± 0.08 88.42 ± 0.13 97.76 ± 0.02 96.44 ± 0.04 94.83 ± 0.06 – 
			<line pos=195.478,528.926,487.417,540.607>96.05 ± 0.03 96.98 ± 0.05 92.61 ± 0.09 88.81 ± 0.09 97.74 ± 0.02 96.61 ± 0.04 95.02 ± 0.04 – 
		<box id=20 pos=72.000,428.371,525.547,500.152>
			<line pos=72.000,488.147,525.547,500.152>Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger 
			<line pos=72.000,476.192,525.547,488.197>model has four times as many hidden units as the others, making it similar in size to the models when ELMo is 
			<line pos=72.000,464.237,525.547,476.242>included. For dependency parsing, we omit results from Choe and Charniak (2016), Kuncoro et al. (2017), and Liu 
			<line pos=72.000,452.282,525.547,464.287>and Zhang (2017) because these train constituency parsers and convert the system outputs to dependency parses. 
			<line pos=72.000,440.326,525.547,452.331>They produce higher scores, but have access to more information during training and do not apply to datasets 
			<line pos=72.000,428.371,416.133,444.872>without constituency annotations. * denotes semi-supervised and † denotes multi-task. 
		<box id=21 pos=77.978,143.740,240.469,337.547>
			<line pos=77.978,324.402,122.040,337.547>Parameter 
			<line pos=77.978,306.331,220.734,319.476>Word Embeddings Initializiation 
			<line pos=77.978,292.781,194.924,305.926>Character Embedding Size 
			<line pos=77.978,279.232,206.313,292.377>Character CNN Filter Widths 
			<line pos=77.978,265.683,201.294,278.828>Character CNN Num Filters 
			<line pos=77.978,252.134,170.084,265.279>Encoder LSTM sizes 
			<line pos=77.978,238.585,240.469,251.730>Encoder LSTM sizes, “Large” model 
			<line pos=77.978,225.035,198.262,238.180>LSTM projection layer size 
			<line pos=77.978,211.486,158.564,224.631>Hidden layer sizes 
			<line pos=77.978,197.937,114.338,211.082>Dropout 
			<line pos=77.978,184.388,150.720,197.533>EMA coefﬁcient 
			<line pos=77.978,170.839,136.440,183.984>Learning rate 
			<line pos=77.978,157.290,128.891,170.435>Momentum 
			<line pos=77.978,143.740,123.120,156.885>Batch size 
		<box id=22 pos=252.422,143.740,498.753,337.547>
			<line pos=252.425,324.402,277.265,337.547>Value 
			<line pos=252.425,306.331,320.302,319.476>300d GloVe 6B 
			<line pos=252.425,292.781,263.335,305.926>50 
			<line pos=252.425,279.232,286.964,292.377>[2, 3, 4] 
			<line pos=252.425,265.683,362.716,278.828>300 (100 per ﬁlter width) 
			<line pos=252.425,252.134,454.375,265.279>1024 for the ﬁrst layer, 512 for the second one 
			<line pos=252.425,238.585,459.829,251.730>4096 for the ﬁrst layer, 2048 for the second one 
			<line pos=252.425,225.035,268.789,238.180>512 
			<line pos=252.425,211.486,268.789,224.631>512 
			<line pos=252.425,197.937,482.957,211.082>0.5 for labeled examples, 0.8 for unlabeled examples 
			<line pos=252.425,184.388,276.971,197.533>0.998 
			<line pos=252.422,170.839,498.753,183.984>0.5/(1 + 0.005t ~ 0.5 ~ ) ( ~ t is number of SGD updates so far) 
			<line pos=252.425,157.290,266.062,170.435>0.9 
			<line pos=252.425,143.740,307.865,156.885>64 sentences 
		<box id=23 pos=217.532,120.286,380.012,132.291>
			<line pos=217.532,120.286,380.012,132.291>Table 6: Hyperparameters for the model. 
	<page id=17>
		<box id=0 pos=92.922,581.070,126.860,594.215>
			<line pos=92.922,581.070,126.860,594.215>Method 
		<box id=1 pos=92.922,321.366,303.729,569.369>
			<line pos=92.922,556.224,217.755,569.369>GAN (Salimans et al., 2016) 
			<line pos=92.922,542.675,303.729,555.820>Stochastic Transformations (Sajjadi et al., 2016) 
			<line pos=92.922,529.126,233.202,542.271>Π model (Laine and Aila, 2017) 
			<line pos=92.922,515.576,282.424,528.721>Temporal Ensemble (Laine and Aila, 2017) 
			<line pos=92.922,502.027,289.733,515.172>Mean Teacher (Tarvainen and Valpola, 2017) 
			<line pos=92.922,488.478,252.598,501.623>Complement GAN (Dai et al., 2017) 
			<line pos=92.922,474.929,210.828,488.074>VAT (Miyato et al., 2017b) 
			<line pos=92.922,461.380,201.566,474.525>VAdD (Park et al., 2017) 
			<line pos=92.922,447.831,232.908,460.976>VAdD + VAT (Park et al., 2017) 
			<line pos=92.922,434.281,260.877,447.426>SNGT + Π model (Luong et al., 2017) 
			<line pos=92.922,420.732,242.431,433.877>SNGT + VAT (Luong et al., 2017) 
			<line pos=92.922,407.183,270.129,420.328>Consistency + WGAN (Wei et al., 2018) 
			<line pos=92.922,393.634,255.020,406.779>Manifold Mixup (Verma et al., 2018) 
			<line pos=92.922,375.563,141.402,388.708>Supervised 
			<line pos=92.922,362.013,141.435,375.158>VAT (ours) 
			<line pos=92.922,348.464,211.504,361.609>CVT, no input perturbation 
			<line pos=92.922,334.915,233.922,348.060>CVT, random input perturbation 
			<line pos=92.922,321.366,248.289,334.511>CVT, adversarial input perturbation 
		<box id=2 pos=345.569,587.844,391.431,600.989>
			<line pos=345.569,587.844,391.431,600.989>CIFAR-10 
		<box id=3 pos=444.984,587.844,496.998,600.989>
			<line pos=444.984,587.844,496.998,600.989>CIFAR-10+ 
		<box id=4 pos=400.432,574.295,450.428,587.440>
			<line pos=400.432,574.295,450.428,587.440>4000 labels 
		<box id=5 pos=345.565,321.442,403.145,569.369>
			<line pos=345.566,556.224,351.020,569.369>– 
			<line pos=345.566,542.675,351.020,555.820>– 
			<line pos=345.566,529.126,403.145,548.293>16.55 ± 0.29 
			<line pos=345.566,515.576,351.020,528.721>– 
			<line pos=345.566,502.027,351.020,515.172>– 
			<line pos=345.566,488.478,403.145,507.645>14.41 ± 0.30 
			<line pos=345.566,474.929,370.111,488.074>13.15 
			<line pos=345.566,461.380,351.020,474.525>– 
			<line pos=345.566,447.831,351.020,460.976>– 
			<line pos=345.565,434.281,403.145,453.448>13.62 ± 0.17 
			<line pos=345.569,420.808,403.145,439.899>12.49 ± 0.36 
			<line pos=345.566,407.183,351.020,420.328>– 
			<line pos=345.566,393.634,351.020,406.779>– 
			<line pos=345.566,375.563,403.145,394.730>23.61 ± 0.60 
			<line pos=345.566,362.013,403.145,381.180>13.29 ± 0.33 
			<line pos=345.566,348.464,403.145,367.631>14.63 ± 0.20 
			<line pos=345.566,334.915,403.145,354.082>13.80 ± 0.30 
			<line pos=345.569,321.442,403.145,340.533>12.01 ± 0.11 
		<box id=6 pos=444.988,321.442,502.564,575.391>
			<line pos=444.991,556.224,502.564,575.391>18.63 ± 2.32 
			<line pos=444.991,542.675,502.564,561.842>11.29 ± 0.24 
			<line pos=444.992,529.126,502.564,548.293>12.36 ± 0.31 
			<line pos=444.991,515.576,502.564,534.743>12.16 ± 0.24 
			<line pos=444.991,502.027,502.564,521.194>12.31 ± 0.28 
			<line pos=444.992,488.478,450.447,501.623>– 
			<line pos=444.991,474.929,469.537,488.074>10.55 
			<line pos=444.991,461.380,502.564,480.547>11.68 ± 0.19 
			<line pos=444.988,447.907,502.564,466.998>10.07 ± 0.11 
			<line pos=444.992,434.281,502.564,453.448>11.00 ± 0.36 
			<line pos=444.992,420.808,497.109,439.899>9.89 ± 0.34 
			<line pos=444.988,407.259,497.109,426.350>9.98 ± 0.21 
			<line pos=444.988,393.710,502.564,412.801>10.26 ± 0.32 
			<line pos=444.992,375.563,502.564,394.730>19.61 ± 0.56 
			<line pos=444.992,362.013,502.564,381.180>10.90 ± 0.31 
			<line pos=444.992,348.464,502.564,367.631>12.44 ± 0.27 
			<line pos=444.992,334.915,502.564,354.082>11.10 ± 0.26 
			<line pos=444.992,321.442,502.564,340.533>10.11 ± 0.15 
		<box id=7 pos=72.000,254.050,525.547,301.921>
			<line pos=72.000,289.916,525.547,301.921>Table 7: Error rates on semi-supervised CIFAR-10. We report means and standard deviations from 5 runs. CIFAR- 
			<line pos=72.000,277.961,525.547,289.966>10+ refers to results where data augmentation (random translations of the input image) was applied. For some of 
			<line pos=72.000,266.006,525.547,278.011>our models we add a random or adversarially chosen perturbation to the student model’s inputs, which is done in 
			<line pos=72.000,254.050,236.622,266.055>most consistency regularization methods. 